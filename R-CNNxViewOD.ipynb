{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T10:08:54.445658Z","iopub.execute_input":"2024-12-06T10:08:54.446055Z","iopub.status.idle":"2024-12-06T10:09:08.221911Z","shell.execute_reply.started":"2024-12-06T10:08:54.446021Z","shell.execute_reply":"2024-12-06T10:09:08.220486Z"}},"outputs":[{"name":"stdout","text":"Collecting selectivesearch\n  Downloading selectivesearch-0.4.tar.gz (3.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (1.26.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (0.23.2)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (1.14.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->selectivesearch) (3.1.2)\nBuilding wheels for collected packages: selectivesearch\n  Building wheel for selectivesearch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for selectivesearch: filename=selectivesearch-0.4-py3-none-any.whl size=4335 sha256=4294cbd37e8f8425cf832d1f6e22c4900f4d836f63a1b3daf6e78e68b118a435\n  Stored in directory: /root/.cache/pip/wheels/0e/49/95/01447a4e0f48a135ac91fbdb1dd2a1c0523e40e29957b383a3\nSuccessfully built selectivesearch\nInstalling collected packages: selectivesearch\nSuccessfully installed selectivesearch-0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport imageio.v3 as imageio\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nimport cv2\nimport shutil\nimport json\nimport yaml\nimport random\nimport time\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\nimport concurrent.futures\nimport multiprocessing as mp\nfrom PIL import Image, ImageOps\nfrom collections import defaultdict, Counter\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TF\nimport torch.optim as optim\nimport re\nimport selectivesearch\nimport torch.optim as optim\nfrom torchvision import models\nfrom torchvision.models import AlexNet_Weights\nimport matplotlib.patches as mpatches\nimport torch.nn.functional as F\nfrom sklearn.svm import SVC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T10:09:25.730043Z","iopub.execute_input":"2024-12-06T10:09:25.730519Z","iopub.status.idle":"2024-12-06T10:09:26.922207Z","shell.execute_reply.started":"2024-12-06T10:09:25.730479Z","shell.execute_reply":"2024-12-06T10:09:26.921068Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Data sources\nIN_DATASET_NM = 'our-xview-dataset'\n\n#Output folders and file names\nOUT_COCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nOUT_DATAFRAME_NM = 'xview_labels.parquet'\nYAML_NM = 'xview_yolo.yaml'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/our-input/xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nfuture_ds_img_fldr = Path(f'/kaggle/working/{OUT_IMAGE_FLDR_NM}')\nfuture_ds_cfg_fldr = Path(f'/kaggle/working/{OUT_CFG_FLDR_NM}')\n\nout_data_parquet_pth = out_dataset_pth / OUT_DATAFRAME_NM\nout_data_parquet_pth = out_dataset_pth / OUT_DATAFRAME_NM\ncoco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nyolo_yaml_pth = cfg_fldr_pth / YAML_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n\ndef make_empty_dir(directory):\n    if directory.is_dir():\n        shutil.rmtree(directory)\n    os.makedirs(directory)\n\nmake_empty_dir(cfg_fldr_pth)\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T10:11:01.688467Z","iopub.execute_input":"2024-12-06T10:11:01.688924Z","iopub.status.idle":"2024-12-06T10:11:01.697826Z","shell.execute_reply.started":"2024-12-06T10:11:01.688883Z","shell.execute_reply":"2024-12-06T10:11:01.696680Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"markdown","source":"## Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# le immagini devono essere aperte con Image.open\nimage = Image.open(img_path).convert('RGB') #apro l'immagine e ne ricavo le dimensioni\noriginal_width, original_height = image.size\n\ndir_name = #directory in cui salvare le region_proposal\n\ndef generate_dataset_proposals(txt_file, dir_name):\n  # prendo i path delle immagini e li memorizzo in una lista\n   with open(txt_file, 'r') as f:\n            image_paths = [line.strip() for line in f.readlines()]\n\n  img_dir = img_dir\n  os.makedirs(img_dir, exist_ok=True)\n\n  for index in range(len(image_paths)):\n    img_name = os.path.basename(image_paths[index]) #prendo il path dell'immagine da self.image_paths in base all'indice fornito\n    img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', '')) #ricavo l'id dell'immagine -> non l'ho già fatto nell'init?\n\n    dir_image = os.path.join(dir_name, img_id) #nome della directory che conterrà le region proposals relative all'immagine\n\n    if img_id not in self.image_info: #vedo dal dizionario self.image_info se l'immagine è contenuta nel file COCO\n              raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n\n    img_path = os.path.join(self.img_dir, img_name) #prendo il path completo dell'immagine unendo il path della cartella con il nome.jpg dell'immagine\n    if not os.path.exists(img_path): #se il path non esiste allora lo segnalo\n              raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    image = Image.open(img_path).convert('RGB') #apro l'immagine e ne ricavo le dimensioni\n    original_width, original_height = image.size\n\n     # GESTIONE DELLE REGION PROPOSALS\n     proposals_tensor = generate_region_proposals(image) # image = immagine aperta in formato RGB con la libreria PIL\n     #    produce una lista di proposals nel formato (x_min, y_min, x_max, y_max)\n\n     processed_proposals = process_proposals(image_tensor, proposals_tensor) # image_tensor = image dopo la data agumentation\n     #    produce le immagine = region proposals relative all'immagine di input\n\n     # salvo le region proposals come immagini in una cartella relativa all'immagine di input\n     # - salvataggio in dir_image\n     os.makedirs(dir_image, exist_ok=True)\n\n    # Iterare sulle region proposals e salvarle come immagini\n    for i, proposal_tensor in enumerate(processed_proposals):\n        # Convertire il tensore in immagine PIL (assumendo valori nel range [0, 1])\n        proposal_image = Image.fromarray((proposal_tensor.numpy() * 255).astype('uint8'))\n\n        # Generare un nome file unico\n        proposal_filename = os.path.join(dir_image, f'proposal_{i:04d}.jpg')\n\n        # Salvare l'immagine\n        proposal_image.save(proposal_filename)\n\n        print(f\"Salvata proposal {i+1}/{len(processed_proposals)}: {proposal_filename}\")\n\n        # Aggiungi il path relativo alla lista\n        relative_path = os.path.relpath(proposal_filename, dir_name)  # Path relativo rispetto a dir_name\n        all_proposal_paths.append(relative_path)\n\n    # Scrittura di tutti i path relativi in un unico file .txt\n    with open(output_txt, 'w') as txt_file:\n        for path in all_proposal_paths:\n            txt_file.write(f\"{path}\\n\")\n\n    print(f\"Creato file TXT con i path relativi di tutte le region proposals: {output_txt}\")\n\ndef generate_region_proposals(image): #funzione per la generazione delle region proposals per singola immagine\n        img_np = np.array(image) #trasformo l'immagine in un array numpy\n\n        if len(img_np.shape) == 3 and img_np.shape[0] == 3: #porto l'immagine nel formato corretto\n            img_np = np.transpose(img_np, (1, 2, 0))  # Da [C, H, W] a [H, W, C]\n\n        _, regions = selectivesearch.selective_search(img_np, scale=500, sigma=0.9, min_size=10) #richiamo la funzione di selective search\n        #scale: granularità della ricerca (più alto, meno dettagliato) ; sigma: Standard deviation per il filtro gaussiano usato per la segmentazione ;\n        #min_size: Dimensione minima di un segmento nell'algoritmo\n        #regions: lista di regioni candidate (proposals).\n        # - regione = dizionario che contiene info. -> incluse le coordinate di un rettangolo delimitante (region['rect'])\n\n        #CHECK SULLA PRODUZIONE DELLE REGION PROPOSALS\n        if len(regions) == 0:\n            print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n        candidate_proposals = []\n        for region in regions: #per ogni regione nella lista delle regioni candidate\n            x, y, w, h = region['rect'] # prendo le coordinate del rettangolo delimitante\n            if w > 0 and h > 0 and w >= 10 and h >= 10: # prendo solo le regioni con altezza e larghezza >= 10 per evitare che siano molto rumorose\n                area = w * h\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0]) # limito la regione alle dimensioni dell'immagine\n                candidate_proposals.append([x, y, x_max, y_max, area]) #inserisco la nuova regione nella lista delle region proposals -> aggiungo un valore in più (area) per facilitare il filtraggio dopo\n\n        unique_proposals = list(set(tuple(p) for p in candidate_proposals)) # converto le proposals in tuple in modo da eliminare i duplicati\n\n        #in questo modo non viene preservata la corrispondenza tra region proposals e labels\n\n        #FILTRO LE PROPOSALS PER PRENDERE SOLO QUELLE UTILI/NECESSARIE\n        min_area = 10\n        max_area_ratio = 0.8\n        proposals = []\n\n        for x_min, y_min, x_max, y_max, area in unique_proposals: #per ogni proposal\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\n                proposals.append((x_min, y_min, x_max, y_max))\n\n        return proposals # restituisce le region proposal valide\n\ndef process_proposals(image_tensor, proposals, output_size=(227, 227)): # la funzione trasforma le proposals trovate in immagini ottenute ritagliando l'imagine originale\n        processed_proposals = []\n        for proposal in proposals: #per ogni proposal\n            try:\n                _, H, W = image_tensor.shape # vedo le dimensioni dell'immagine\n                x_min, y_min, x_max, y_max = map(int, proposal)\n                x_min, y_min = max(0, x_min), max(0, y_min)\n                x_max, y_max = min(W, x_max), min(H, y_max)\n\n                # Controlla se la proposal ha dimensioni valide per l'immagine di partenza -> tecnicamente non si potrebbe eliminare l'if ?\n                if x_min < x_max and y_min < y_max:\n                    cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]  # Ritaglio\n\n                    # Controlla che il ritaglio non sia vuoto\n                    if cropped_region.numel() == 0:\n                        print(f\"Ritaglio vuoto per proposal: {proposal}. Salto.\")\n                        continue\n\n                    # Controlla che il tensor sia 3D (C, H, W)\n                    if cropped_region.ndim != 3:\n                        print(f\"Proposal non valida per il ridimensionamento: {proposal}. Salto.\")\n                        continue\n\n                    resized_region = torch.nn.functional.interpolate(\n                        cropped_region.unsqueeze(0), size=output_size, mode='bilinear', align_corners=False\n                    ).squeeze(0)  # Ridimensiona\n\n                    processed_proposals.append(resized_region)\n            except Exception as e:\n                print(f\"Errore durante il processamento della proposal: {proposal}. Errore: {e}\")\n\n        return processed_proposals  # Lista di tensori delle region proposals\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name): #prende il nome.jpg di una immagine e restituisce solo l'identificativo senza prefissi e suffissi\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f: #salva le region proposals in un file txt con direttamente le info della cartella\n            self.image_paths = [line.strip() for line in f.readlines()] #memorizzo i path delle immagini in una lista\n\n        with open(coco_json_file, 'r') as f: #leggo il file .json - contenente (...) - con coco\n            coco_data = json.load(f)\n\n        self.image_annotations = {} #dizionario contenente per ogni immagine una lista di categorie di oggetti presebti\n\n        for annotation in coco_data['annotations']: #uso la sezione annotazioni del file .json per ricavare delle info. sulle immagini del dataset\n            image_id = annotation['image_id']\n            category_id = annotation['category_id'] # lista di category_id = categorie degli oggetti nell'immagine)\n\n            if image_id not in self.image_annotations: #verifico se l'id dell'immagine è già presente nel dizionario\n                self.image_annotations[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        } #dizionario in cui per ogni nome dell'immagine ottenuta da generate_id(file_name) associa il nome.jpg dell'imagine\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) # trasformazione di base da applicare a tutte le immagini\n\n        # lasciare momentaneamente in caso di aggiornamenti futuri\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) # strasformazione per la data agumentation\n\n        self.aug = aug\n\n    def __len__(self): # ritorna il numero di elementi in self.image_paths -> chi è?\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index]) #prendo il path dell'immagine da self.image_paths in base all'indice fornito\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', '')) #ricavo l'id dell'immagine -> non l'ho già fatto nell'init?\n\n        if img_id not in self.image_info: #vedo dal dizionario self.image_info se l'immagine è contenuta nel file COCO\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n\n        img_path = os.path.join(self.img_dir, img_name) #prendo il path completo dell'immagine unendo il path della cartella con il nome.jpg dell'immagine\n        if not os.path.exists(img_path): #se il path non esiste allora lo segnalo\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n        image = Image.open(img_path).convert('RGB') #apro l'immagine e ne ricavo le dimensioni\n        #original_width, original_height = image.size\n\n        if self.aug: #se la variabile self.aug è alta allora applico la self.aug_transform altrimenti la self.base_transform _> HA SENSO? LE DUE FUNZIONI SONO = !!\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n\n        #POTREBBE ESSERCI UN PROBLEMA NELLA CORRISPONDENZA TRA LABLES E REGION PROPOSALS -> perchè in proposals_tensor non ci sono tutte le region proposals perchè alcune vengono scartate\n        # -> se la corrispondenza è 1 a 1 allora potrebbe convenire dare a _generate_region_proposals(image) sia le labes che l'immagine? -> non è certo perchè\n        # in lables c'è una lista di lable ma non viene specificato dove sono localizzate\n\n        # restituisce un dizionario\n        return {\n            \"regions\": image_tensor  # region proposals elaborate, una lista di tensori che rappresentano regioni candidate per il rilevamento\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataLoader Legacy","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n\n        self.img_dir = img_dir\n\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n\n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox_str = annotation['bbox']\n            bbox = list(map(float, bbox_str.strip('[]').split(', ')))\n\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug = aug    \n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n    \n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        \n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Ridimensiona i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        categories = [c for c in categories if isinstance(c, int)]\n        if not categories:\n            categories = [-1]  # Etichetta speciale per immagini senza annotazioni\n        \n        scale_x = 320 / original_width\n        scale_y = 320 / original_height\n        scaled_bboxes = [\n            torch.tensor([\n                bbox[0] * scale_x,  # x_min\n                bbox[1] * scale_y,  # y_min\n                bbox[2] * scale_x,  # x_max\n                bbox[3] * scale_y   # y_max\n            ], dtype=torch.float32)\n            for bbox in bboxes\n        ] if bboxes else [torch.zeros(4, dtype=torch.float32)]\n        \n        labels = torch.tensor(categories, dtype=torch.int64)\n        proposals_tensor = self._generate_region_proposals(image)\n        \n        # Chiama match_proposals_with_labels per associare le regioni con le etichette\n        matched_proposals, matched_labels = self.match_proposals_with_labels(proposals_tensor, scaled_bboxes, labels)\n        \n        # Elabora le proposte abbinate\n        processed_proposals = self._process_proposals(image_tensor, matched_proposals)\n        \n        return {\n            \"image\": image_tensor,\n            \"labels\": matched_labels,  # Etichette abbinate alle regioni\n            \"bboxes\": scaled_bboxes,\n            \"regions\": processed_proposals  # Lista di tensori delle region proposals\n        }\n\n    def match_proposals_with_labels(self, regions, bboxes, labels, iou_threshold=0.5):\n        matched_features = []\n        matched_labels = []\n\n        for region in regions:\n            # Calcola l'IoU per ciascun bbox\n            ious = self.calculate_iou(region, bboxes)\n            max_iou = ious.max()\n            # MANTENERE PROPORZIONE 1/3 CON \n            if max_iou > iou_threshold:\n                # Se IoU > threshold, associa l'etichetta del bbox con massimo IoU\n                matched_features.append(region)  # Region proposal\n                matched_labels.append(labels[ious.argmax()])  # Etichetta corrispondente\n            else:\n                # Se nessun bbox corrisponde, classifica come background\n                matched_features.append(region)\n                matched_labels.append(0)  # 0 indica \"background\"\n\n        return matched_features, matched_labels\n\n    def calculate_iou(self, region, bboxes):\n        # Calcola l'Intersection over Union (IoU) tra la regione proposta e i bounding boxes\n        x_min, y_min, x_max, y_max = region\n        proposal_area = (x_max - x_min) * (y_max - y_min)\n\n        ious = []\n        for bbox in bboxes:\n            bx_min, by_min, bx_max, by_max = bbox\n            inter_x_min = max(x_min, bx_min)\n            inter_y_min = max(y_min, by_min)\n            inter_x_max = min(x_max, bx_max)\n            inter_y_max = min(y_max, by_max)\n            \n            # Calcola l'area dell'intersezione\n            inter_width = max(0, inter_x_max - inter_x_min)\n            inter_height = max(0, inter_y_max - inter_y_min)\n            inter_area = inter_width * inter_height\n            \n            union_area = proposal_area + (bx_max - bx_min) * (by_max - by_min) - inter_area\n            ious.append(inter_area / union_area if union_area > 0 else 0)\n        \n        return torch.tensor(ious)\n\n    def _generate_region_proposals(self, image):\n        img_np = np.array(image)\n        \n        if len(img_np.shape) == 3 and img_np.shape[0] == 3:\n            img_np = np.transpose(img_np, (1, 2, 0))  # Da [C, H, W] a [H, W, C]\n        elif len(img_np.shape) == 2:\n            img_np = np.stack([img_np] * 3, axis=-1)  # Da [H, W] a [H, W, 3]\n        elif img_np.shape[2] < 3:\n            img_np = np.repeat(img_np, 3, axis=2)  # Da [H, W, 1] a [H, W, 3]\n        elif img_np.shape[2] != 3:\n            raise ValueError(f\"L'immagine ha una forma non valida: {img_np.shape}\")\n        \n        _, regions = selectivesearch.selective_search(img_np, scale=500, sigma=0.9, min_size=10)\n        if len(regions) == 0:\n            print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n        \n        proposals = []\n        for region in regions:\n            x, y, w, h = region['rect']\n            if w > 0 and h > 0 and w >= 10 and h >= 10:\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0])\n                proposals.append([x, y, x_max, y_max])\n        \n        filtered_proposals = self._filter_proposals(proposals, img_np.shape[1], img_np.shape[0])\n        \n        return filtered_proposals  # Restituisce una lista di coordinate (non ancora tensorizzate)\n\n    def _filter_proposals(self, proposals, img_width, img_height, min_area=100, max_area_ratio=0.8):\n        unique_proposals = set(tuple(p) for p in proposals)\n        filtered = []\n        for x_min, y_min, x_max, y_max in unique_proposals:\n            width = x_max - x_min\n            height = y_max - y_min\n            area = width * height\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\n                filtered.append((x_min, y_min, x_max, y_max))\n        return filtered\n\n    def _process_proposals(self, image_tensor, proposals, output_size=(227, 227)):\n        processed_proposals = []\n        for proposal in proposals:\n            try:\n                _, H, W = image_tensor.shape\n                x_min, y_min, x_max, y_max = map(int, proposal)\n                x_min, y_min = max(0, x_min), max(0, y_min)\n                x_max, y_max = min(W, x_max), min(H, y_max)\n    \n                # Controlla se la proposal è valida\n                if x_min < x_max and y_min < y_max:\n                    cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]  # Ritaglio\n                    \n                    # Controlla che il ritaglio non sia vuoto\n                    if cropped_region.numel() == 0:\n                        print(f\"Ritaglio vuoto per proposal: {proposal}. Salto.\")\n                        continue\n                    \n                    # Controlla che il tensor sia 3D (C, H, W)\n                    if cropped_region.ndim != 3:\n                        print(f\"Proposal non valida per il ridimensionamento: {proposal}. Salto.\")\n                        continue\n                    \n                    resized_region = torch.nn.functional.interpolate(\n                        cropped_region.unsqueeze(0), size=output_size, mode='bilinear', align_corners=False\n                    ).squeeze(0)  # Ridimensiona\n                    \n                    processed_proposals.append(resized_region)\n            except Exception as e:\n                print(f\"Errore durante il processamento della proposal: {proposal}. Errore: {e}\")\n    \n        return processed_proposals  # Lista di tensori delle region proposals","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    images = []\n    labels = []\n    bboxes = []\n    regions = []\n\n    for sample in batch:\n        images.append(sample['image'])  # Le immagini sono già tensori\n        labels.append(torch.tensor(sample['labels'], dtype=torch.int64))  # Convertiamo in tensore\n        bboxes.append(sample['bboxes'])  # Bboxes potrebbero essere già tensori\n        regions.append(sample['regions'])  # Le region proposals potrebbero essere già tensori\n\n    # Stacking delle immagini e concatenamento delle etichette\n    images = torch.stack(images, dim=0)\n    labels = torch.cat(labels, dim=0)  # Ora labels sono tensori, possiamo concatenarle\n    bboxes = [torch.stack(b, dim=0) if len(b) > 0 else torch.zeros(1, 4) for b in bboxes]  # Gestione delle bounding boxes\n    regions = [torch.stack(r, dim=0) if len(r) > 0 else torch.zeros(1, 4) for r in regions]  # Gestione delle region proposals\n\n    return {\n        'image': images,\n        'labels': labels,\n        'bboxes': bboxes,\n        'regions': regions\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=True)\nvalid_dataset = CustomDataset(val_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \ntest_dataset = CustomDataset(test_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_txt_vs_json(txt_paths, coco_data):\n    \"\"\"\n    Controlla se le immagini del JSON sono presenti in almeno uno dei file TXT.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n        coco_data (dict): Dati in formato COCO.\n    \"\"\"\n    # Estrai i nomi delle immagini dal JSON\n    image_names = [image['file_name'] for image in coco_data['images']]\n    \n    # Inizializza un set per contenere tutte le immagini presenti nei TXT\n    txt_image_names = set()\n    \n    # Leggi i nomi delle immagini da ciascun file TXT\n    for txt_path in txt_paths:\n        with open(txt_path, 'r') as f:\n            txt_image_names.update(os.path.basename(line.strip()) for line in f.readlines())\n    \n    # Trova le immagini presenti nel JSON ma non in nessuno dei TXT\n    missing_in_txts = [name for name in image_names if name not in txt_image_names]\n    \n    # Verifica e stampa i risultati\n    print(\"\\nControllo completato:\")\n    if missing_in_txts:\n        print(f\"Errore: le seguenti immagini non sono presenti in nessuno dei file TXT forniti:\\n{missing_in_txts}\")\n    else:\n        print(\"Tutte le immagini del JSON sono presenti in almeno uno dei file TXT.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta il numero di immagini nel JSON\nnum_images = len(coco_data['images'])\nprint(f\"Numero di immagini nel JSON: {num_images}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estrai i nomi delle immagini dal JSON\nimage_names = [image['file_name'] for image in coco_data['images']]\n\n# Stampa i primi 5 nomi delle immagini nel JSON\nprint(f\"Primi 5 nomi delle immagini nel JSON: {image_names[:5]}\")   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_lines_in_txt(txt_paths):\n    \"\"\"\n    Conta il numero di righe in ciascun file TXT dato il percorso e calcola la somma totale delle righe.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n    \"\"\"\n    total_lines = 0  # Variabile per accumulare il numero totale di righe\n    \n    for txt_path in txt_paths:\n        try:\n            # Apri il file e conta le righe\n            with open(txt_path, 'r') as f:\n                num_lines = sum(1 for line in f)\n            total_lines += num_lines  # Aggiungi il numero di righe del file al totale\n            print(f\"Numero di righe nel file {txt_path}: {num_lines}\")\n        except FileNotFoundError:\n            print(f\"Errore: il file {txt_path} non è stato trovato.\")\n        except Exception as e:\n            print(f\"Errore nel leggere il file {txt_path}: {e}\")\n    \n    # Stampa la somma totale delle righe\n    print(f\"\\nSomma totale delle righe in tutti i file: {total_lines}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_paths = [train_txt_pth, val_txt_pth, test_txt_pth]\ncount_lines_in_txt(txt_paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_txt_vs_json(txt_paths, coco_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_dataloader(loader):\n    \"\"\"\n    Funzione per controllare il comportamento di un DataLoader.\n    Visualizza alcune immagini insieme alle loro annotazioni per verificare il corretto funzionamento.\n\n    Args:\n        loader (DataLoader): Il DataLoader da verificare.\n    \"\"\"\n    for batch_idx, batch in enumerate(loader):\n        print(f\"Batch {batch_idx + 1}:\")\n        \n        # Estrai i dati dal dizionario\n        images = batch[\"image\"]\n        labels = batch[\"labels\"]\n        bboxes = batch[\"bboxes\"]\n        regions = batch[\"regions\"]\n\n        # Stampa le shape e le dimensioni dei dati\n        print(f\"  Shape delle immagini (batch): {images.shape}\")\n        print(f\"  Numero di etichette nel batch: {len(labels)}\")\n\n        # Controlla se il batch è vuoto\n        if images.size(0) == 0:\n            print(\"Batch vuoto. Procedo con il batch successivo.\")\n            continue\n\n        # Scegli un'immagine casuale dal batch\n        random_index = random.randint(0, images.size(0) - 1)\n        image = images[random_index].permute(1, 2, 0).numpy()  # Da [C, H, W] a [H, W, C]\n\n        # Visualizza l'immagine\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.imshow(image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Immagine nel batch {batch_idx + 1}, indice {random_index}\")\n\n        # Etichette\n        label = labels[random_index]\n        label_info = label.tolist() if isinstance(label, torch.Tensor) else label\n        print(f\"  Etichette: {label_info}\")\n\n        # Bounding boxes\n        bbox = bboxes[random_index]\n        for box in bbox:\n            if isinstance(box, torch.Tensor):\n                box = box.tolist()  # Converte il bounding box in lista\n    \n            # Converti da [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            x_min, y_min, width, height = box\n            x_max = x_min + width\n            y_max = y_min + height\n    \n            # Opzionale: Verifica dei limiti dell'immagine\n            H, W, _ = image.shape\n            x_min, y_min = max(0, x_min), max(0, y_min)\n            x_max, y_max = min(W, x_max), min(H, y_max)\n    \n            # Disegna il bounding box\n            rect = plt.Rectangle((x_min, y_min), width, height,\n                                  edgecolor='green', facecolor='none', linewidth=1.5)\n            ax.add_patch(rect)\n\n        # Visualizza alcune region proposals come immagini separate\n        region_proposals = regions[random_index]\n        print(f\"  Numero di region proposals: {len(region_proposals)}\")\n\n        fig, axs = plt.subplots(1, min(5, len(region_proposals)), figsize=(15, 5))\n        for i, proposal in enumerate(region_proposals[:5]):\n            proposal_image = proposal.permute(1, 2, 0).numpy()  # Da [C, H, W] a [H, W, C]\n            axs[i].imshow(proposal_image)\n            axs[i].axis(\"off\")\n            axs[i].set_title(f\"Region Proposal {i + 1}\")\n        plt.show()\n\n        break  # Mostra solo il primo batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui il controllo per i dataloader\ncheck_dataloader(train_loader)\ncheck_dataloader(val_loader)\ncheck_dataloader(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}