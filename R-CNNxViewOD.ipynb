{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":10161721,"sourceType":"datasetVersion","datasetId":6274890}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:14:59.186157Z","iopub.execute_input":"2024-12-12T12:14:59.187542Z","iopub.status.idle":"2024-12-12T12:15:08.924227Z","shell.execute_reply.started":"2024-12-12T12:14:59.187475Z","shell.execute_reply":"2024-12-12T12:15:08.922631Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: selectivesearch in /opt/conda/lib/python3.10/site-packages (0.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (1.26.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (0.23.2)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (1.14.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->selectivesearch) (3.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per l'ottimizzazione e la gestione delle dipendenze\nimport selectivesearch\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:08.927419Z","iopub.execute_input":"2024-12-12T12:15:08.927961Z","iopub.status.idle":"2024-12-12T12:15:08.937834Z","shell.execute_reply.started":"2024-12-12T12:15:08.927906Z","shell.execute_reply":"2024-12-12T12:15:08.936490Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nCOCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_COCO_JSON_NM = 'mod_COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n# PROPOSALS\nOUT_PROPOSALS_FLDR_NM = 'proposals'\nprop_fldr = Path(f'/kaggle/working/{OUT_PROPOSALS_FLDR_NM}')\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\nACTPROP_WEIG_COCO_JSON_NM ='active_regions_weights.json'\nactproposalsweights_json = out_dataset_pth / ACTPROP_WEIG_COCO_JSON_NM\n\n# ACTIVE REGIONS\nact_reg_path = Path('/kaggle/input/activeregion-xviewdataset')\nact_reg_folder = Path('/kaggle/input/activeregion-xviewdataset/activeregion-xview-dataset/proposals')\n\n#DATASET\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:08.939509Z","iopub.execute_input":"2024-12-12T12:15:08.940377Z","iopub.status.idle":"2024-12-12T12:15:08.956933Z","shell.execute_reply.started":"2024-12-12T12:15:08.940314Z","shell.execute_reply":"2024-12-12T12:15:08.955760Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)\nclean_output(prop_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:08.958378Z","iopub.execute_input":"2024-12-12T12:15:08.958855Z","iopub.status.idle":"2024-12-12T12:15:09.010637Z","shell.execute_reply.started":"2024-12-12T12:15:08.958806Z","shell.execute_reply":"2024-12-12T12:15:09.009468Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\nCartella /kaggle/working/proposals non trovata. Nessuna azione necessaria.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:09.014162Z","iopub.execute_input":"2024-12-12T12:15:09.014498Z","iopub.status.idle":"2024-12-12T12:15:09.019534Z","shell.execute_reply.started":"2024-12-12T12:15:09.014467Z","shell.execute_reply":"2024-12-12T12:15:09.018409Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON dal percorso specificato.\n\n    :param file_path: Percorso al file JSON da caricare.\n    :return: Dati contenuti nel file JSON (come dizionario o lista).\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    data = load_json(input_path)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Lista di nuove annotazioni da aggiungere per immagini senza bbox\n    new_annotations = []\n \n    # Elenco di annotazioni da rimuovere\n    annotations_to_remove = []\n \n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        x, y, width, height = annotation['bbox']\n        xmin = x\n        xmax = x + width\n        ymin = y\n        ymax = y + height\n        # Verifica che xmin < xmax e ymin < ymax\n        if xmin >= xmax or ymin >= ymax:\n            annotations_to_remove.append(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, xmax, ymin, ymax]\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = [ann for ann in data['annotations'] if ann['id'] not in annotations_to_remove]\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, image['width'], 0.0, image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n \n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n \n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:09.021332Z","iopub.execute_input":"2024-12-12T12:15:09.022182Z","iopub.status.idle":"2024-12-12T12:15:09.038329Z","shell.execute_reply.started":"2024-12-12T12:15:09.022131Z","shell.execute_reply":"2024-12-12T12:15:09.037229Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:09.039602Z","iopub.execute_input":"2024-12-12T12:15:09.039942Z","iopub.status.idle":"2024-12-12T12:15:25.393956Z","shell.execute_reply.started":"2024-12-12T12:15:09.039912Z","shell.execute_reply":"2024-12-12T12:15:25.392935Z"}},"outputs":[{"name":"stderr","text":"Processing Categories: 100%|██████████| 11/11 [00:00<00:00, 82241.25it/s]\nBuilding Image Annotations Dictionary: 100%|██████████| 670213/670213 [00:00<00:00, 2056317.91it/s]\nProcessing Annotations: 100%|██████████| 670213/670213 [00:03<00:00, 203461.95it/s]\nProcessing Images: 100%|██████████| 45891/45891 [00:00<00:00, 560428.37it/s]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"## Category Check","metadata":{}},{"cell_type":"code","source":"def count_bounding_boxes(json_path):\n    \"\"\"\n    Conta il numero di bounding box per ogni categoria in un file COCO JSON.\n\n    Args:\n        json_path (str): Percorso del file JSON.\n\n    Returns:\n        list: Elenco di tuple con ID categoria, nome categoria e numero di bounding box.\n    \"\"\"\n    # Carica il file JSON\n    coco_data = load_json(json_path)\n\n    # Estrarre i dati principali\n    annotations = coco_data.get(\"annotations\", [])\n    categories = coco_data.get(\"categories\", [])\n\n    # Mappare id di categoria ai nomi delle categorie\n    category_id_to_name = {category[\"id\"]: category[\"name\"] for category in categories}\n\n    # Contare i bounding box per categoria\n    bbox_counts = defaultdict(int)\n    for annotation in annotations:\n        category_id = annotation[\"category_id\"]\n        bbox_counts[category_id] += 1\n\n    # Creare un elenco dei risultati\n    results = [\n        (cat_id, category_id_to_name.get(cat_id, \"Unknown\"), count)\n        for cat_id, count in bbox_counts.items()\n    ]\n    \n    # Ordinare i risultati in ordine decrescente per numero di bounding box\n    results.sort(key=lambda x: x[2], reverse=True)\n    \n    # Stampare i risultati\n    for cat_id, category_name, count in results:\n        print(f\"Categoria ID {cat_id} ('{category_name}'): {count} bounding box\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:25.395160Z","iopub.execute_input":"2024-12-12T12:15:25.395480Z","iopub.status.idle":"2024-12-12T12:15:25.403072Z","shell.execute_reply.started":"2024-12-12T12:15:25.395448Z","shell.execute_reply":"2024-12-12T12:15:25.402000Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"count_bounding_boxes(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:25.405033Z","iopub.execute_input":"2024-12-12T12:15:25.405466Z","iopub.status.idle":"2024-12-12T12:15:28.732504Z","shell.execute_reply.started":"2024-12-12T12:15:25.405418Z","shell.execute_reply":"2024-12-12T12:15:28.731286Z"}},"outputs":[{"name":"stdout","text":"Categoria ID 6 ('Building'): 384942 bounding box\nCategoria ID 1 ('Passenger Vehicle'): 225097 bounding box\nCategoria ID 2 ('Truck'): 34377 bounding box\nCategoria ID 0 ('background'): 13692 bounding box\nCategoria ID 4 ('Maritime Vessel'): 6329 bounding box\nCategoria ID 5 ('Engineering Vehicle'): 5473 bounding box\nCategoria ID 9 ('Shipping Container'): 5391 bounding box\nCategoria ID 3 ('Railway Vehicle'): 4233 bounding box\nCategoria ID 8 ('Storage Tank'): 2033 bounding box\nCategoria ID 11 ('Aircraft'): 1708 bounding box\nCategoria ID 10 ('Pylon'): 470 bounding box\nCategoria ID 7 ('Helipad'): 152 bounding box\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalità RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:28.733749Z","iopub.execute_input":"2024-12-12T12:15:28.734062Z","iopub.status.idle":"2024-12-12T12:15:28.742508Z","shell.execute_reply.started":"2024-12-12T12:15:28.734030Z","shell.execute_reply":"2024-12-12T12:15:28.741475Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.8, min_size=20)\n\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:28.744059Z","iopub.execute_input":"2024-12-12T12:15:28.744400Z","iopub.status.idle":"2024-12-12T12:15:28.762888Z","shell.execute_reply.started":"2024-12-12T12:15:28.744368Z","shell.execute_reply":"2024-12-12T12:15:28.761764Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:28.764158Z","iopub.execute_input":"2024-12-12T12:15:28.764486Z","iopub.status.idle":"2024-12-12T12:15:28.779671Z","shell.execute_reply.started":"2024-12-12T12:15:28.764455Z","shell.execute_reply":"2024-12-12T12:15:28.778635Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    coco_data = load_json(coco_json)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:28.780922Z","iopub.execute_input":"2024-12-12T12:15:28.781246Z","iopub.status.idle":"2024-12-12T12:15:28.792953Z","shell.execute_reply.started":"2024-12-12T12:15:28.781216Z","shell.execute_reply":"2024-12-12T12:15:28.791998Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"generate_dataset_proposals(new_coco_json_pth, img_fldr, prop_fldr, proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:15:28.795956Z","iopub.execute_input":"2024-12-12T12:15:28.796729Z","iopub.status.idle":"2024-12-12T12:16:07.944616Z","shell.execute_reply.started":"2024-12-12T12:15:28.796684Z","shell.execute_reply":"2024-12-12T12:16:07.943155Z"}},"outputs":[{"name":"stderr","text":"Processing batches:   1%|          | 1/92 [00:34<52:28, 34.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"Creato file JSON con le region proposals: /kaggle/working/proposals.json\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"## Positive Region Proposals","metadata":{}},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:16:07.946309Z","iopub.execute_input":"2024-12-12T12:16:07.946707Z","iopub.status.idle":"2024-12-12T12:16:07.956942Z","shell.execute_reply.started":"2024-12-12T12:16:07.946644Z","shell.execute_reply":"2024-12-12T12:16:07.955456Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def calculate_bbox_areas(json_file_path, category_id):\n    \"\"\"\n    Calcola la media delle aree di tutti i bounding box con category_id specificato.\n\n    :param json_file_path: Percorso al file JSON contenente le annotazioni.\n    :param category_id: ID della categoria per cui calcolare le aree.\n    :return: Media delle aree dei bounding box della categoria specificata.\n    \"\"\"\n    # Carica il file JSON\n    data = load_json(json_file_path)\n    \n    areas = []\n\n    # Itera sulle annotazioni\n    for annotation in data.get(\"annotations\", []):\n        if annotation[\"category_id\"] == category_id:\n            bbox = annotation[\"bbox\"]  # bbox formato [xmin, xmax, ymin, ymax]\n            xmin, xmax, ymin, ymax = map(float, bbox)\n            width = xmax - xmin\n            height = ymax - ymin\n            area = width * height\n            areas.append(area)\n\n    # Calcola e restituisci la media delle aree\n    if areas:\n        return sum(areas) / len(areas)\n    else:\n        return 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:16:07.958319Z","iopub.execute_input":"2024-12-12T12:16:07.959520Z","iopub.status.idle":"2024-12-12T12:16:07.975712Z","shell.execute_reply.started":"2024-12-12T12:16:07.959471Z","shell.execute_reply":"2024-12-12T12:16:07.974414Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"buildings_area = calculate_bbox_areas(new_coco_json_pth, 6)\ncars_area = calculate_bbox_areas(new_coco_json_pth, 1)\n\nprint(\"Area media buildings:\", buildings_area)\nprint(\"Area media cars:\", cars_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:16:07.977179Z","iopub.execute_input":"2024-12-12T12:16:07.977537Z","iopub.status.idle":"2024-12-12T12:16:15.785368Z","shell.execute_reply.started":"2024-12-12T12:16:07.977503Z","shell.execute_reply":"2024-12-12T12:16:15.784205Z"}},"outputs":[{"name":"stdout","text":"Area media buildings: 2670.4960123862816\nArea media cars: 387.1186732830735\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"def get_adaptive_threshold(bbox):\n    \"\"\"Calcola un threshold IoU adattivo in base alla dimensione del bounding box.\"\"\"\n    # bbox è un tensore o una lista con [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]  # Calcolo larghezza\n    height = bbox[3] - bbox[1]  # Calcolo altezza\n    \n    # Adattiamo la soglia in base alle dimensioni del bbox\n    area = width * height\n    if area < 500:  # Bbox piccoli (ad esempio, cars)\n        return 0.25  # Soglia più bassa per bbox piccoli\n    else:  # Bbox grandi\n        return 0.5  # Soglia più alta per bbox grandi (grandi edifici)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:25:58.306733Z","iopub.execute_input":"2024-12-12T12:25:58.307187Z","iopub.status.idle":"2024-12-12T12:25:58.313153Z","shell.execute_reply.started":"2024-12-12T12:25:58.307145Z","shell.execute_reply":"2024-12-12T12:25:58.312071Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path):\n    \"\"\"Associa le regioni proposte ai bounding boxes secondo le specifiche e salva un nuovo JSON in formato COCO.\"\"\"\n\n    # Carica i file JSON\n    regions = load_json(region_json_path)\n\n    bboxes = load_json(bbox_json_path)\n\n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        bbox = annot[\"bbox\"]\n        annotations_by_image[img_id].append((torch.tensor(bbox, dtype=torch.float32), annot[\"category_id\"]))\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    active_region_data = []  # Lista per i dati delle regioni attive\n    iou_values = []\n\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            continue\n\n        gt_bboxes = [item[0] for item in gt_data]\n        gt_categories = [item[1] for item in gt_data]\n\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1],\n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        if not iou_matrix:\n            continue\n\n        iou_matrix = torch.tensor(iou_matrix)\n        max_ious, indices = torch.max(iou_matrix, dim=1)\n\n        iou_values.extend(max_ious.tolist())\n\n        for idx, iou in enumerate(max_ious):\n            # Calcolare il threshold IoU adattivo in base alla dimensione del bounding box\n            adaptive_threshold = get_adaptive_threshold(gt_bboxes[indices[idx].item()])\n        \n            if iou >= adaptive_threshold:\n                # La regione appartiene alla categoria corrispondente al massimo IoU\n                category_id = gt_categories[indices[idx].item()]\n            elif iou < 0.01:\n                # Classifica come sfondo\n                category_id = 11\n            else:\n                # Ignora la regione\n                continue\n\n            x_min, y_min, x_max, y_max = proposal_coords[idx].values()\n            width = x_max - x_min\n            height = y_max - y_min\n\n            # Salva la regione selezionata\n            active_region_data.append({\n                \"image_id\": image_id,\n                \"file_name\": file_name,\n                \"category_id\": category_id,\n                \"proposal_id\": idx,\n                \"region_bbox\": [x_min, y_min, width, height]\n            })\n\n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)\n\n    return iou_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:30:05.217078Z","iopub.execute_input":"2024-12-12T12:30:05.217510Z","iopub.status.idle":"2024-12-12T12:30:05.232590Z","shell.execute_reply.started":"2024-12-12T12:30:05.217471Z","shell.execute_reply":"2024-12-12T12:30:05.231437Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# Esegui l'assegnazione e ottieni i valori IoU\niou_values = assign_and_save_regions(proposals_json, new_coco_json_pth, img_fldr, prop_fldr, actproposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:30:08.009302Z","iopub.execute_input":"2024-12-12T12:30:08.009691Z","iopub.status.idle":"2024-12-12T12:30:29.576171Z","shell.execute_reply.started":"2024-12-12T12:30:08.009644Z","shell.execute_reply":"2024-12-12T12:30:29.574717Z"}},"outputs":[{"name":"stderr","text":"Elaborazione immagini: 100%|██████████| 500/500 [00:11<00:00, 42.79it/s] \n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"def create_iou_histogram(iou_values, bins=50, range=(0, 1)):\n    \"\"\"Crea un istogramma dei valori IoU.\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.hist(iou_values, bins=bins, range=range, color='blue', alpha=0.7)\n    plt.title('Distribuzione dei valori IoU')\n    plt.xlabel('IoU')\n    plt.ylabel('Frequenza')\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:31:32.861497Z","iopub.execute_input":"2024-12-12T12:31:32.861901Z","iopub.status.idle":"2024-12-12T12:31:32.869143Z","shell.execute_reply.started":"2024-12-12T12:31:32.861862Z","shell.execute_reply":"2024-12-12T12:31:32.867888Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Crea l'istogramma dei valori IoU\ncreate_iou_histogram(iou_values, bins=20, range=(0, 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:31:35.787370Z","iopub.execute_input":"2024-12-12T12:31:35.788309Z","iopub.status.idle":"2024-12-12T12:31:36.089532Z","shell.execute_reply.started":"2024-12-12T12:31:35.788268Z","shell.execute_reply":"2024-12-12T12:31:36.088425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"print(ignored_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:31:40.407208Z","iopub.execute_input":"2024-12-12T12:31:40.407982Z","iopub.status.idle":"2024-12-12T12:31:40.413393Z","shell.execute_reply.started":"2024-12-12T12:31:40.407941Z","shell.execute_reply":"2024-12-12T12:31:40.412189Z"}},"outputs":[{"name":"stdout","text":"467911\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"def analyze_regions(file_path):\n    \"\"\"\n    Analizza un file JSON per ottenere il numero di regioni e le occorrenze dei category_id.\n\n    :param file_path: Percorso al file JSON contenente le annotazioni.\n    :return: Tupla contenente il numero di regioni e un dizionario con le occorrenze dei category_id.\n    \"\"\"\n    # Carica il file JSON\n    data = load_json(file_path)\n\n    # Conta il numero di regioni\n    num_regioni = len(data)\n    \n    # Ottieni le occorrenze dei category_id\n    category_ids = [entry['category_id'] for entry in data]\n    category_counts = Counter(category_ids)\n\n    # Ordina le occorrenze per ID di categoria\n    sorted_category_counts = dict(sorted(category_counts.items()))\n\n    return num_regioni, sorted_category_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:38:22.511532Z","iopub.execute_input":"2024-12-12T12:38:22.512519Z","iopub.status.idle":"2024-12-12T12:38:22.518996Z","shell.execute_reply.started":"2024-12-12T12:38:22.512471Z","shell.execute_reply":"2024-12-12T12:38:22.517632Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"num_regioni, category_counts = analyze_regions(actproposals_json)\nprint(f\"Numero di regioni: {num_regioni}\")\nprint(\"Occorrenze dei category_id:\", category_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:38:23.888434Z","iopub.execute_input":"2024-12-12T12:38:23.888868Z","iopub.status.idle":"2024-12-12T12:38:23.913769Z","shell.execute_reply.started":"2024-12-12T12:38:23.888829Z","shell.execute_reply":"2024-12-12T12:38:23.912693Z"}},"outputs":[{"name":"stdout","text":"Numero di regioni: 6873\nOccorrenze dei category_id: {1: 971, 2: 277, 5: 14, 6: 2241, 7: 15, 8: 8, 9: 6, 11: 3341}\n","output_type":"stream"}],"execution_count":72},{"cell_type":"markdown","source":"# Positive Region Proposals with mAP","metadata":{}},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef calculate_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:14:50.407686Z","iopub.execute_input":"2024-12-12T09:14:50.408119Z","iopub.status.idle":"2024-12-12T09:14:50.417889Z","shell.execute_reply.started":"2024-12-12T09:14:50.408081Z","shell.execute_reply":"2024-12-12T09:14:50.416550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_labels_with_map(proposals, annotations, map_threshold, categories):\n    \"\"\"\n    Assegna etichette alle regioni proposte basandosi sulle annotations utilizzando mAP.\n\n    Parameters:\n        proposals (list): Lista delle regioni proposte per un'immagine.\n        annotations (list): Lista dei bounding box annotati con etichette.\n        map_threshold (float): Soglia mAP per considerare una corrispondenza positiva.\n        categories (dict): Dizionario di mapping category_id -> nome categoria.\n\n    Returns:\n        list: Lista delle proposte con le etichette assegnate.\n    \"\"\"\n    labeled_proposals = []\n\n    for proposal in proposals:\n        best_iou = 0\n        best_annotation = None\n\n        for annotation in annotations:\n            bbox = annotation['bbox']\n            annotation_bbox_x = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]  # Conversione in [x, y, x+w, y+h]\n            annotation_bbox = [float(value) for value in annotation_bbox_x]\n            \n            # Calcola l'IoU\n            iou = calculate_iou(proposal['coordinates'], annotation_bbox)\n            if iou > best_iou:\n                best_iou = iou\n                best_annotation = annotation\n\n        if best_iou >= map_threshold and best_annotation:\n            labeled_proposals.append({\n                'proposal_id': proposal['proposal_id'],\n                'coordinates': proposal['coordinates'],\n                'label': categories.get(best_annotation['category_id'], 'unknown'),\n                'iou': best_iou\n            })\n        else:\n            labeled_proposals.append({\n                'proposal_id': proposal['proposal_id'],\n                'coordinates': proposal['coordinates'],\n                'label': 'background',  # Classe \"sfondo\"\n                'iou': best_iou\n            })\n\n    return labeled_proposals\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:29:58.471889Z","iopub.execute_input":"2024-12-12T09:29:58.472883Z","iopub.status.idle":"2024-12-12T09:29:58.481267Z","shell.execute_reply.started":"2024-12-12T09:29:58.472838Z","shell.execute_reply":"2024-12-12T09:29:58.479985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_map(results):\n    \"\"\"\n    Calcola la Mean Average Precision (mAP) sui risultati etichettati.\n\n    Parameters:\n        results (list): Lista di tutte le proposte etichettate.\n\n    Returns:\n        float: Valore mAP.\n    \"\"\"\n    ap_per_class = {}\n\n    for result in results:\n        for proposal in result['labeled_proposals']:\n            label = proposal['label']\n            if label == 'background':\n                continue\n\n            if label not in ap_per_class:\n                ap_per_class[label] = {'precision': [], 'recall': []}\n\n            # Placeholder per precisione e richiamo (da calcolare separatamente)\n            ap_per_class[label]['precision'].append(proposal.get('precision', 1))\n            ap_per_class[label]['recall'].append(proposal.get('recall', 1))\n\n    mean_ap = 0\n    num_classes = len(ap_per_class)\n\n    for label, metrics in ap_per_class.items():\n        sorted_indices = sorted(range(len(metrics['recall'])), key=lambda i: metrics['recall'][i])\n        precisions = [metrics['precision'][i] for i in sorted_indices]\n        recalls = [metrics['recall'][i] for i in sorted_indices]\n\n        ap = 0\n        for i in range(1, len(recalls)):\n            ap += (recalls[i] - recalls[i - 1]) * precisions[i]\n\n        mean_ap += ap\n\n    return mean_ap / num_classes if num_classes > 0 else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:14:57.107530Z","iopub.execute_input":"2024-12-12T09:14:57.107966Z","iopub.status.idle":"2024-12-12T09:14:57.117417Z","shell.execute_reply.started":"2024-12-12T09:14:57.107925Z","shell.execute_reply":"2024-12-12T09:14:57.116160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_images_with_map(region_file, annotation_file, output_file):\n    \"\"\"\n    Processa tutte le immagini, collega le regioni proposte con le annotations e calcola la mAP.\n\n    Parameters:\n        region_file (str): Path al file JSON con le regioni proposte.\n        annotation_file (str): Path al file JSON con le annotations.\n        output_file (str): Path al file JSON di output.\n    \"\"\"\n    # Carica i file JSON\n    regions_data = load_json(region_file)\n    coco_data = load_json(annotation_file)\n\n    # Prepara le categorie\n    categories = {}\n    for cat in coco_data['categories']:\n        cat_id, cat_name = list(cat.items())[0]\n        categories[int(cat_id)] = cat_name\n\n    # Prepara le annotations\n    annotations_by_image = {}\n    for annotation in coco_data['annotations']:\n        img_id = annotation['image_id']\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        # Mantenere il formato bbox originale, senza conversione qui\n        annotations_by_image[img_id].append({\n            \"bbox\": annotation[\"bbox\"],  # Mantieni il bbox nel formato [x, y, w, h]\n            \"category_id\": annotation['category_id']\n        })\n\n    results = []\n\n    for image_data in tqdm(regions_data, desc=\"Processing Images\"):\n        image_id = image_data['image_id']\n        proposals = image_data['proposals']\n\n        # Trova le annotations corrispondenti all'immagine corrente\n        annotations = annotations_by_image.get(image_id, [])\n\n        # Calcola la soglia mAP dinamica basata sul numero di annotations\n        map_threshold = 0.5 if len(annotations) == 0 else min(0.5 + 0.05 * (len(annotations) - 1), 0.95)\n\n        # Associa le etichette alle proposte\n        labeled_proposals = assign_labels_with_map(proposals, annotations, map_threshold, categories)\n\n        # Aggiungi al risultato\n        results.append({\n            'image_id': image_id,\n            'file_name': image_data['file_name'],\n            'labeled_proposals': labeled_proposals\n        })\n\n    # Calcola la mAP complessiva\n    mean_ap = calculate_map(results)\n\n    # Salva i risultati in un file JSON\n    with open(output_file, 'w') as f:\n        json.dump({\"results\": results, \"mean_ap\": mean_ap}, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:30:02.725403Z","iopub.execute_input":"2024-12-12T09:30:02.725816Z","iopub.status.idle":"2024-12-12T09:30:02.736543Z","shell.execute_reply.started":"2024-12-12T09:30:02.725778Z","shell.execute_reply":"2024-12-12T09:30:02.735276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_images_with_map('/kaggle/input/regions/proposals.json', coco_json_pth, actproposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:30:06.500259Z","iopub.execute_input":"2024-12-12T09:30:06.501305Z","iopub.status.idle":"2024-12-12T09:30:15.319683Z","shell.execute_reply.started":"2024-12-12T09:30:06.501247Z","shell.execute_reply":"2024-12-12T09:30:15.318090Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"# Percorso del file JSON e del percorso base\ninput_json_path = \"/kaggle/input/activeregion-xviewdataset/activeregion-xview-dataset/active_regions.json\"\n\n# Carica il dataset dal JSON\ndata = load_json(input_json_path)\n\n# Converti in DataFrame per una gestione più comoda\ndf = pd.DataFrame(data)\n\n# Estrai il nome del file dal campo 'saved_path'\ndf[\"file_name\"] = df[\"saved_path\"].apply(lambda x: os.path.basename(x))\n\n# Aggiungi il percorso base al campo 'saved_path'\ndf[\"saved_path\"] = df[\"file_name\"].apply(lambda x: str(act_reg_folder / x))\n\n# Estrai i dati e le etichette\nX = df.index  # Indici delle righe\ny = df[\"category_id\"]  # Etichetta per stratificazione\n\n# Step 1: Train + Val/Test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Step 2: Val/Test split\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\n# Creazione dei dataset finali\ntrain_data = df.loc[X_train]\nval_data = df.loc[X_val]\ntest_data = df.loc[X_test]\n\n# Salva i dataset in nuovi file JSON\ntrain_data.to_json(\"train.json\", orient=\"records\", lines=False)\nval_data.to_json(\"val.json\", orient=\"records\", lines=False)\ntest_data.to_json(\"test.json\", orient=\"records\", lines=False)\n\nprint(\"Splitting completato. File salvati: train.json, val.json, test.json.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:09.455287Z","iopub.execute_input":"2024-12-09T15:55:09.456141Z","iopub.status.idle":"2024-12-09T15:55:12.584134Z","shell.execute_reply.started":"2024-12-09T15:55:09.456087Z","shell.execute_reply":"2024-12-09T15:55:12.583216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        # Carica il file JSON\n        self.data = load_json(json_file)\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor()                 # Converte l'immagine in un tensore\n            #transforms.Normalize(                  # Normalizzazione con valori di ImageNet\n            #    mean=[0.485, 0.456, 0.406], \n            #    std=[0.229, 0.224, 0.225]\n            #)\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:17.533632Z","iopub.execute_input":"2024-12-09T15:55:17.534111Z","iopub.status.idle":"2024-12-09T15:55:17.541627Z","shell.execute_reply.started":"2024-12-09T15:55:17.534070Z","shell.execute_reply":"2024-12-09T15:55:17.540448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds = CustomDataset(test_path)\ntrain_ds = CustomDataset(train_path)\nval_ds = CustomDataset(val_path)\n\nTrainLoader = DataLoader(train_ds, batch_size=64, shuffle=True)\nValLoader = DataLoader(val_ds, batch_size=64, shuffle=False)\nTestLoader = DataLoader(test_ds, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:19.887131Z","iopub.execute_input":"2024-12-09T15:55:19.887466Z","iopub.status.idle":"2024-12-09T15:55:20.956676Z","shell.execute_reply.started":"2024-12-09T15:55:19.887437Z","shell.execute_reply":"2024-12-09T15:55:20.955687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)  \n        #final = func.sigmoid(self.fn10(feature))\n        final = self.fn10(feature)\n\n        return feature, final","metadata":{"execution":{"iopub.status.busy":"2024-12-09T16:57:42.091423Z","iopub.execute_input":"2024-12-09T16:57:42.092167Z","iopub.status.idle":"2024-12-09T16:57:42.101018Z","shell.execute_reply.started":"2024-12-09T16:57:42.092099Z","shell.execute_reply":"2024-12-09T16:57:42.099977Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12 #11 classi + sfondo\nnet = AlexNet(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:28:06.806844Z","iopub.execute_input":"2024-12-09T17:28:06.807183Z","iopub.status.idle":"2024-12-09T17:28:07.304442Z","shell.execute_reply.started":"2024-12-09T17:28:06.807152Z","shell.execute_reply":"2024-12-09T17:28:07.303636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:28:08.636863Z","iopub.execute_input":"2024-12-09T17:28:08.637481Z","iopub.status.idle":"2024-12-09T17:28:08.642476Z","shell.execute_reply.started":"2024-12-09T17:28:08.637448Z","shell.execute_reply":"2024-12-09T17:28:08.641470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_loss(train_losses, val_losses):\n    \"\"\"\n    Funzione per fare il plot della funzione di loss durante il training e la validazione.\n\n    :param train_losses: Lista delle perdite durante il training.\n    :param val_losses: Lista delle perdite durante la validazione.\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle='-', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\", linestyle='-', marker='x')\n    \n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:10:14.961027Z","iopub.execute_input":"2024-12-09T17:10:14.961731Z","iopub.status.idle":"2024-12-09T17:10:14.967187Z","shell.execute_reply.started":"2024-12-09T17:10:14.961693Z","shell.execute_reply":"2024-12-09T17:10:14.966232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_class_weights(train_loader, num_classes=12, background_weight=2.0):\n    \"\"\"\n    Calcola i pesi per ciascuna classe nel dataset in base alla frequenza delle etichette,\n    includendo un peso per la classe \"sfondo\" (ID=11) che potrebbe non essere presente nei dati.\n\n    :param train_loader: DataLoader per il training set.\n    :param num_classes: Numero totale di classi nel dataset (incluso lo sfondo).\n    :param background_weight: Peso assegnato alla classe \"sfondo\" (ID=11).\n    :return: Tensor dei pesi per ciascuna classe.\n    \"\"\"\n    # Estrai tutte le etichette dal dataset nel train_loader\n    all_labels = []\n    for _, labels in tqdm(train_loader, desc=\"Estrazione etichette\", leave=False):\n        all_labels.extend(labels.numpy())  # Estrae le etichette dalle immagini\n\n    # Conta le occorrenze di ciascuna classe\n    class_counts = Counter(all_labels)\n    \n    # Calcola il peso per ogni classe (inversamente proporzionale alla frequenza)\n    total_samples = len(all_labels)\n    class_weights = {class_id: total_samples / count for class_id, count in class_counts.items()}\n    \n    # Normalizza i pesi (in modo che il peso massimo sia 1)\n    max_weight = max(class_weights.values())\n    normalized_weights = {class_id: weight / max_weight for class_id, weight in class_weights.items()}\n    \n    # Aggiungi un peso elevato per la classe sfondo (ID=11)\n    normalized_weights[11] = background_weight  # Assegna un peso maggiore alla classe sfondo\n    \n    # Crea un tensor dei pesi, dove ogni peso corrisponde alla classe\n    weights = torch.tensor([normalized_weights.get(i, 1.0) for i in range(num_classes)], dtype=torch.float32)\n    \n    return weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:22:47.953332Z","iopub.execute_input":"2024-12-09T18:22:47.953714Z","iopub.status.idle":"2024-12-09T18:22:47.960758Z","shell.execute_reply.started":"2024-12-09T18:22:47.953681Z","shell.execute_reply":"2024-12-09T18:22:47.959857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss, class_weights):\n    \"\"\"\n    Funzione per addestrare il modello con pesi per classi sbilanciate.\n\n    :param net: Modello da addestrare.\n    :param train_loader: DataLoader per il training set.\n    :param val_loader: DataLoader per il validation set.\n    :param criterion: Funzione di loss.\n    :param optimizer: Ottimizzatore.\n    :param device: Dispositivo (CPU o GPU).\n    :param epochs: Numero di epoche di training.\n    :param path_min_loss: Percorso per salvare il modello con la minore loss di validazione.\n    \"\"\"\n    min_val_loss = float('inf')\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n\n    # Liste per registrare le perdite durante il training e la validazione\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        net.train()  # Modalità training\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        # Barra di avanzamento per il training\n        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n\n        for images, labels in train_progress:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss pesata\n            loss = criterion(outputs, labels)\n\n            # Backward pass e aggiornamento pesi\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Statistiche\n            train_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_train += labels.size(0)\n            correct_train += predicted.eq(labels).sum().item()\n\n            # Aggiorna la barra di avanzamento con la loss corrente\n            train_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_train / total_train)\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n        train_accuracy = 100. * correct_train / total_train\n        train_losses.append(avg_train_loss)  # Aggiungi il valore di train_loss\n\n        # Validazione\n        net.eval()  # Modalità validazione\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        # Barra di avanzamento per la validazione\n        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", leave=False)\n\n        with torch.no_grad():\n            for images, labels in val_progress:\n                images, labels = images.to(device), labels.to(device)\n\n                # Forward pass\n                _, outputs = net(images)\n\n                # Calcolo della loss\n                loss = criterion(outputs, labels)\n\n                # Statistiche\n                val_loss += loss.item() * images.size(0)\n                _, predicted = outputs.max(1)\n                total_val += labels.size(0)\n                correct_val += predicted.eq(labels).sum().item()\n\n                # Aggiorna la barra di avanzamento con la loss e accuracy\n                val_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_val / total_val)\n\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = 100. * correct_val / total_val\n        val_losses.append(avg_val_loss)  # Aggiungi il valore di val_loss\n\n        # Salva il modello con la loss di validazione più bassa\n        if avg_val_loss < min_val_loss:\n            print(f\"Salvataggio del miglior modello: Val Loss migliorata da {min_val_loss:.4f} a {avg_val_loss:.4f}\")\n            min_val_loss = avg_val_loss\n            torch.save(net.state_dict(), path_min_loss)\n\n        # Stampa statistiche per epoca\n        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    print(\"Training completato!\")\n\n    # Chiamata alla funzione per tracciare il grafico\n    plot_loss(train_losses, val_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:37:08.760299Z","iopub.execute_input":"2024-12-09T17:37:08.761075Z","iopub.status.idle":"2024-12-09T17:37:08.772940Z","shell.execute_reply.started":"2024-12-09T17:37:08.761036Z","shell.execute_reply":"2024-12-09T17:37:08.771964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss):\n    \"\"\"\n    Funzione per testare il modello.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    \"\"\"\n    # Carica il miglior modello salvato\n    net.load_state_dict(torch.load(path_min_loss))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:35.602023Z","iopub.execute_input":"2024-12-09T15:55:35.602918Z","iopub.status.idle":"2024-12-09T15:55:35.609314Z","shell.execute_reply.started":"2024-12-09T15:55:35.602863Z","shell.execute_reply":"2024-12-09T15:55:35.608417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calcola i pesi per ogni classe dal train_loader\nclass_weights = compute_class_weights(TrainLoader)\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:43:26.664026Z","iopub.execute_input":"2024-12-09T17:43:26.664672Z","iopub.status.idle":"2024-12-09T17:50:17.385314Z","shell.execute_reply.started":"2024-12-09T17:43:26.664629Z","shell.execute_reply":"2024-12-09T17:50:17.384330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\nepochs = 10\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\ndevice = torch.device(\"cuda\")\n\nnet = net.to(device)\ncriterion = criterion.to(device)\n\npath_min_loss = '/kaggle/working/AlexNet.pth'\n\ntrain_model(\n    net=net,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    class_weights=class_weights\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:50:35.762331Z","iopub.execute_input":"2024-12-09T17:50:35.763129Z","iopub.status.idle":"2024-12-09T18:22:21.341423Z","shell.execute_reply.started":"2024-12-09T17:50:35.763092Z","shell.execute_reply":"2024-12-09T18:22:21.339728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=net,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:23:05.370465Z","iopub.execute_input":"2024-12-09T18:23:05.370839Z","iopub.status.idle":"2024-12-09T18:25:40.418621Z","shell.execute_reply.started":"2024-12-09T18:23:05.370807Z","shell.execute_reply":"2024-12-09T18:25:40.417655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    \"\"\"\n    Funzione per calcolare e visualizzare la matrice di confusione.\n\n    :param y_true: Etichette reali\n    :param y_pred: Etichette predette dal modello\n    :param class_names: Nomi delle classi\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted labels')\n    plt.ylabel('True labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n# Durante la fase di validazione o test, calcola la matrice di confusione\ndef validate_model(net, val_loader, device, class_names):\n    net.eval()  # Modalità di valutazione\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Forward pass\n            _, outputs = net(images)\n            \n            # Predizioni\n            _, predicted = outputs.max(1)\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n    \n    # Visualizza la matrice di confusione\n    plot_confusion_matrix(all_labels, all_preds, class_names)\n\n# Esempio di come chiamare la funzione\nclass_names = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3', 'Classe 4', 'Classe 5', 'Classe 6', 'Classe 7', 'Classe 8', 'Classe 9', 'Classe 10']\nvalidate_model(net, ValLoader, device, class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:27:40.856352Z","iopub.execute_input":"2024-12-09T18:27:40.857110Z","iopub.status.idle":"2024-12-09T18:28:34.851618Z","shell.execute_reply.started":"2024-12-09T18:27:40.857061Z","shell.execute_reply":"2024-12-09T18:28:34.850651Z"}},"outputs":[],"execution_count":null}]}