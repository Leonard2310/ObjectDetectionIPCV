{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:22:08.885911Z","iopub.execute_input":"2024-12-08T09:22:08.886313Z","iopub.status.idle":"2024-12-08T09:22:22.348839Z","shell.execute_reply.started":"2024-12-08T09:22:08.886278Z","shell.execute_reply":"2024-12-08T09:22:22.347567Z"}},"outputs":[{"name":"stdout","text":"Collecting selectivesearch\n  Downloading selectivesearch-0.4.tar.gz (3.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (1.26.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (0.23.2)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (1.14.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->selectivesearch) (3.1.2)\nBuilding wheels for collected packages: selectivesearch\n  Building wheel for selectivesearch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for selectivesearch: filename=selectivesearch-0.4-py3-none-any.whl size=4335 sha256=ed72ee750317e59d83e2c6af84e23752b71a3c26da4952f7c911055f4fc021aa\n  Stored in directory: /root/.cache/pip/wheels/0e/49/95/01447a4e0f48a135ac91fbdb1dd2a1c0523e40e29957b383a3\nSuccessfully built selectivesearch\nInstalling collected packages: selectivesearch\nSuccessfully installed selectivesearch-0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per l'ottimizzazione e la gestione delle dipendenze\nimport selectivesearch\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:06:48.469337Z","iopub.execute_input":"2024-12-08T11:06:48.469730Z","iopub.status.idle":"2024-12-08T11:06:48.478129Z","shell.execute_reply.started":"2024-12-08T11:06:48.469697Z","shell.execute_reply":"2024-12-08T11:06:48.476882Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nOUT_COCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n# PROPOSALS\nOUT_PROPOSALS_FLDR_NM = 'proposals'\nprop_fldr = Path(f'/kaggle/working/{OUT_PROPOSALS_FLDR_NM}')\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:23:06.921903Z","iopub.execute_input":"2024-12-08T09:23:06.922828Z","iopub.status.idle":"2024-12-08T09:23:06.929468Z","shell.execute_reply.started":"2024-12-08T09:23:06.922781Z","shell.execute_reply":"2024-12-08T09:23:06.928417Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)\nclean_output(prop_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:24:13.839760Z","iopub.execute_input":"2024-12-08T09:24:13.840731Z","iopub.status.idle":"2024-12-08T09:24:13.847707Z","shell.execute_reply.started":"2024-12-08T09:24:13.840689Z","shell.execute_reply":"2024-12-08T09:24:13.846712Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\nCartella /kaggle/working/proposals non trovata. Nessuna azione necessaria.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:27:26.172471Z","iopub.execute_input":"2024-12-08T09:27:26.173048Z","iopub.status.idle":"2024-12-08T09:27:26.180321Z","shell.execute_reply.started":"2024-12-08T09:27:26.172974Z","shell.execute_reply":"2024-12-08T09:27:26.178758Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"markdown","source":"## Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalit√† RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:24:39.680958Z","iopub.execute_input":"2024-12-08T09:24:39.681339Z","iopub.status.idle":"2024-12-08T09:24:39.689761Z","shell.execute_reply.started":"2024-12-08T09:24:39.681308Z","shell.execute_reply":"2024-12-08T09:24:39.688662Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.8, min_size=20)\n\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:25:11.957070Z","iopub.execute_input":"2024-12-08T09:25:11.958014Z","iopub.status.idle":"2024-12-08T09:25:11.964580Z","shell.execute_reply.started":"2024-12-08T09:25:11.957953Z","shell.execute_reply":"2024-12-08T09:25:11.963556Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:25:15.209958Z","iopub.execute_input":"2024-12-08T09:25:15.210917Z","iopub.status.idle":"2024-12-08T09:25:15.216819Z","shell.execute_reply.started":"2024-12-08T09:25:15.210864Z","shell.execute_reply":"2024-12-08T09:25:15.215632Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    with open(coco_json, 'r') as f:\n        coco_data = json.load(f)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:25:18.282423Z","iopub.execute_input":"2024-12-08T09:25:18.282803Z","iopub.status.idle":"2024-12-08T09:25:18.292097Z","shell.execute_reply.started":"2024-12-08T09:25:18.282763Z","shell.execute_reply":"2024-12-08T09:25:18.291068Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"'''\ndef generate_dataset_proposals(txt_file, dir_name):\n  # prendo i path delle immagini e li memorizzo in una lista\n   with open(txt_file, 'r') as f:\n            image_paths = [line.strip() for line in f.readlines()]\n\n  img_dir = img_dir\n  os.makedirs(img_dir, exist_ok=True)\n\n  for index in range(len(image_paths)):\n    img_name = os.path.basename(image_paths[index]) #prendo il path dell'immagine da self.image_paths in base all'indice fornito\n    img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', '')) #ricavo l'id dell'immagine -> non l'ho gi√† fatto nell'init?\n\n    dir_image = os.path.join(dir_name, img_id) #nome della directory che conterr√† le region proposals relative all'immagine\n\n    if img_id not in self.image_info: #vedo dal dizionario self.image_info se l'immagine √® contenuta nel file COCO\n              raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n\n    img_path = os.path.join(self.img_dir, img_name) #prendo il path completo dell'immagine unendo il path della cartella con il nome.jpg dell'immagine\n    if not os.path.exists(img_path): #se il path non esiste allora lo segnalo\n              raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    image = Image.open(img_path).convert('RGB') #apro l'immagine e ne ricavo le dimensioni\n    original_width, original_height = image.size\n\n     # GESTIONE DELLE REGION PROPOSALS\n     proposals_tensor = generate_region_proposals(image) # image = immagine aperta in formato RGB con la libreria PIL\n     #    produce una lista di proposals nel formato (x_min, y_min, x_max, y_max)\n\n     processed_proposals = process_proposals(image_tensor, proposals_tensor) # image_tensor = image dopo la data agumentation\n     #    produce le immagine = region proposals relative all'immagine di input\n\n     # salvo le region proposals come immagini in una cartella relativa all'immagine di input\n     # - salvataggio in dir_image\n     os.makedirs(dir_image, exist_ok=True)\n\n    # Iterare sulle region proposals e salvarle come immagini\n    for i, proposal_tensor in enumerate(processed_proposals):\n        # Convertire il tensore in immagine PIL (assumendo valori nel range [0, 1])\n        proposal_image = Image.fromarray((proposal_tensor.numpy() * 255).astype('uint8'))\n\n        # Generare un nome file unico\n        proposal_filename = os.path.join(dir_image, f'proposal_{i:04d}.jpg')\n\n        # Salvare l'immagine\n        proposal_image.save(proposal_filename)\n\n        print(f\"Salvata proposal {i+1}/{len(processed_proposals)}: {proposal_filename}\")\n\n        # Aggiungi il path relativo alla lista\n        relative_path = os.path.relpath(proposal_filename, dir_name)  # Path relativo rispetto a dir_name\n        all_proposal_paths.append(relative_path)\n\n    # Scrittura di tutti i path relativi in un unico file .txt\n    with open(output_txt, 'w') as txt_file:\n        for path in all_proposal_paths:\n            txt_file.write(f\"{path}\\n\")\n\n    print(f\"Creato file TXT con i path relativi di tutte le region proposals: {output_txt}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:51:02.971971Z","iopub.execute_input":"2024-12-06T13:51:02.972444Z","iopub.status.idle":"2024-12-06T13:51:02.990379Z","shell.execute_reply.started":"2024-12-06T13:51:02.972394Z","shell.execute_reply":"2024-12-06T13:51:02.989291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ndef generate_region_proposals(image, img_width, img_height): #funzione per la generazione delle region proposals per singola immagine\n        img_np = np.array(image) #trasformo l'immagine in un array numpy\n\n        if len(img_np.shape) == 3 and img_np.shape[0] == 3: #porto l'immagine nel formato corretto\n            img_np = np.transpose(img_np, (1, 2, 0))  # Da [C, H, W] a [H, W, C]\n            \n        _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.9, min_size=10) #richiamo la funzione di selective search\n        #scale: granularit√† della ricerca (pi√π alto, meno dettagliato) ; \n        #sigma: Standard deviation per il filtro gaussiano usato per la segmentazione ;\n        #min_size: Dimensione minima di un segmento nell'algoritmo\n        #regions: lista di regioni candidate (proposals).\n        # - regione = dizionario che contiene info. -> incluse le coordinate di un rettangolo delimitante (region['rect'])\n\n        #CHECK SULLA PRODUZIONE DELLE REGION PROPOSALS\n        if len(regions) == 0:\n            print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n        candidate_proposals = []\n        for region in regions: #per ogni regione nella lista delle regioni candidate\n            x, y, w, h = region['rect'] # prendo le coordinate del rettangolo delimitante\n            if w > 0 and h > 0 and w >= 10 and h >= 10: # prendo solo le regioni con altezza e larghezza >= 10 per evitare che siano molto rumorose\n                area = w * h\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0]) # limito la regione alle dimensioni dell'immagine\n                candidate_proposals.append([x, y, x_max, y_max, area]) #inserisco la nuova regione nella lista delle region proposals -> aggiungo un valore in pi√π (area) per facilitare il filtraggio dopo\n\n        unique_proposals = list(set(tuple(p) for p in candidate_proposals)) # converto le proposals in tuple in modo da eliminare i duplicati\n\n        #in questo modo non viene preservata la corrispondenza tra region proposals e labels\n\n        #FILTRO LE PROPOSALS PER PRENDERE SOLO QUELLE UTILI/NECESSARIE\n        min_area = 10\n        max_area_ratio = 0.8\n        proposals = []\n\n        for x_min, y_min, x_max, y_max, area in unique_proposals: #per ogni proposal\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\n                proposals.append((x_min, y_min, x_max, y_max))\n\n        return proposals # restituisce le region proposal valide\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:51:02.991823Z","iopub.execute_input":"2024-12-06T13:51:02.992216Z","iopub.status.idle":"2024-12-06T13:51:03.007558Z","shell.execute_reply.started":"2024-12-06T13:51:02.992145Z","shell.execute_reply":"2024-12-06T13:51:03.006406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ndef process_proposals(image_tensor, proposals, output_size=(227, 227)):  # la funzione trasforma le proposals trovate in immagini ottenute ritagliando l'immagine originale\n    processed_proposals = []\n    for proposal in proposals:  # per ogni proposal\n        try:\n            _, H, W = image_tensor.shape  # vedo le dimensioni dell'immagine\n            x_min, y_min, x_max, y_max = map(int, proposal)\n            x_min, y_min = max(0, x_min), max(0, y_min)\n            x_max, y_max = min(W, x_max), min(H, y_max)\n\n            # Controlla se la proposal ha dimensioni valide per l'immagine di partenza -> tecnicamente non si potrebbe eliminare l'if?\n            if x_min < x_max and y_min < y_max:\n                cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]  # Ritaglio\n\n                # Controlla che il ritaglio non sia vuoto\n                if cropped_region.size == 0:\n                    print(f\"Ritaglio vuoto per proposal: {proposal}. Salto.\")\n                    continue\n\n                # Controlla che il tensor sia 3D (C, H, W)\n                if cropped_region.ndim != 3:\n                    print(f\"Proposal non valida per il ridimensionamento: {proposal}. Salto.\")\n                    continue\n\n                # Converti cropped_region in un tensore PyTorch\n                cropped_region = torch.tensor(cropped_region).permute(2, 0, 1)  # Cambia il formato da HWC a CHW\n\n                # Ridimensiona la regione proposta\n                resized_region = torch.nn.functional.interpolate(\n                    cropped_region.unsqueeze(0), size=output_size, mode='bilinear', align_corners=False\n                ).squeeze(0)  # Ridimensiona\n\n                processed_proposals.append(resized_region)\n        except Exception as e:\n            print(f\"Errore durante il processamento della proposal: {proposal}. Errore: {e}\")\n\n    return processed_proposals  # Lista di tensori delle region proposals\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:51:03.012108Z","iopub.execute_input":"2024-12-06T13:51:03.012545Z","iopub.status.idle":"2024-12-06T13:51:03.030127Z","shell.execute_reply.started":"2024-12-06T13:51:03.012506Z","shell.execute_reply":"2024-12-06T13:51:03.028914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image)\n\n    # Esegui la selective search per trovare le regioni di interesse (proposals)\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.9, min_size=10)\n\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []  # Lista per le proposte elaborate\n\n    # Pre-filtraggio delle regioni e raccolta delle coordinate delle proposte\n    for region in regions:\n        x, y, w, h = region['rect']\n        \n        # Controlla se la regione √® abbastanza grande senza calcoli inutili\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            # Aggiungi le coordinate alla lista delle proposte\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals  # Restituisce solo le coordinate delle proposte\n\n# Funzione per elaborare un batch di immagini in parallelo\ndef process_images_in_parallel(images, img_width, img_height):\n    # Utilizza concurrent.futures per elaborare pi√π immagini in parallelo\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        # La funzione generate_and_process_proposals verr√† applicata su ogni immagine\n        results = list(executor.map(lambda image: generate_and_process_proposals(image, img_width, img_height), images))\n    return results\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:03:08.73042Z","iopub.status.idle":"2024-12-06T14:03:08.730813Z","shell.execute_reply.started":"2024-12-06T14:03:08.730627Z","shell.execute_reply":"2024-12-06T14:03:08.730647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_dataset_proposals(coco_json_pth, img_fldr, prop_fldr, proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:27:34.674150Z","iopub.execute_input":"2024-12-08T09:27:34.674549Z","iopub.status.idle":"2024-12-08T10:05:49.494358Z","shell.execute_reply.started":"2024-12-08T09:27:34.674515Z","shell.execute_reply":"2024-12-08T10:05:49.493070Z"}},"outputs":[{"name":"stderr","text":"Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [38:11<00:00, 35.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Creato file JSON con le region proposals: /kaggle/working/proposals.json\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Positive Region Proposals","metadata":{}},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'√® sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:30:47.550690Z","iopub.execute_input":"2024-12-08T10:30:47.551083Z","iopub.status.idle":"2024-12-08T10:30:47.559719Z","shell.execute_reply.started":"2024-12-08T10:30:47.551050Z","shell.execute_reply":"2024-12-08T10:30:47.558419Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path, iou_threshold=0.5):\n    \"\"\"Associa le regioni proposte ai bounding boxes, salva le regioni positive come immagini e crea un nuovo JSON con informazioni attivate.\"\"\"\n    \n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n    \n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        # Converte il campo bbox da stringa a lista di float e poi in formato [x1, y1, x2, y2]\n        bbox = json.loads(annot[\"bbox\"])  # Converte la stringa in una lista di numeri\n        x, y, w, h = bbox\n        bbox_converted = [x, y, x + w, y + h]  # Converti in [x1, y1, x2, y2]\n        annotations_by_image[img_id].append((torch.tensor(bbox_converted, dtype=torch.float32), annot[\"category_id\"]))\n    \n    # Crea un dizionario per mappare category_id ai nomi delle categorie\n    category_mapping = {cat_id: name for cat_id, name in enumerate(bboxes[\"categories\"])}\n    \n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n\n    counter = 0  # Contatore delle immagini salvate\n    \n    active_region_data = []  # Lista per i dati delle regioni attive\n\n    # Avvolgi il ciclo principale per ogni immagine con tqdm (una barra di progresso generale)\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n        \n        # Ottieni bounding boxes ground-truth e categorie per l'immagine corrente\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Se non ci sono bounding boxes ground-truth, salta l'immagine\n            continue\n        \n        gt_bboxes = [item[0] for item in gt_data]  # Bounding box ground truth\n        gt_categories = [item[1] for item in gt_data]  # Categorie ground truth\n        \n        # Trasforma proposals in una lista di dizionari compatibili con get_iou\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1], \n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n        \n        # Calcola la matrice IoU usando la funzione get_iou\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                # Ogni gt_bbox deve essere un dizionario simile a proposal\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        # Verifica se la matrice IoU √® vuota (ad esempio, se non ci sono bounding box ground-truth per un'immagine, allora gt_bboxes √® vuoto)\n        if not iou_matrix:\n            continue\n        \n        iou_matrix = torch.tensor(iou_matrix)\n\n        # Identifica le regioni positive (IoU >= soglia)\n        max_ious, indices = torch.max(iou_matrix, dim=1)\n        positive_indices = torch.nonzero(max_ious >= iou_threshold).squeeze(1)\n        \n        # Carica l'immagine originale\n        image_path = os.path.join(image_dir, file_name)\n        original_image = cv2.imread(image_path)\n        if original_image is None:\n            print(f\"Immagine non trovata: {image_path}\")\n            continue\n        \n        # Avvolgi il ciclo per ogni proposta positiva senza tqdm (non serve pi√π una barra per ogni proposta)\n        for idx in positive_indices:\n            x_min, y_min, x_max, y_max = proposal_coords[idx].values()\n\n            # Calcola la larghezza e l'altezza per il formato COCO\n            width = x_max - x_min\n            height = y_max - y_min\n            \n            cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n            \n            # Ridimensiona a 224x224\n            resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n            \n            # Ottieni l'etichetta della categoria dal bounding box assegnato\n            category_id = gt_categories[indices[idx].item()]\n            \n            # Salva l'immagine\n            output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n            cv2.imwrite(output_path, resized)\n            \n            # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n            active_region_data.append({\n                \"image_id\": image_id,\n                \"file_name\": file_name,\n                \"category_id\": category_id,\n                \"proposal_id\": idx.item(),\n                \"region_bbox\": [x_min, y_min, width, height],  \n                \"saved_path\": output_path\n            })\n            \n            counter += 1\n    \n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:11:50.486645Z","iopub.execute_input":"2024-12-08T10:11:50.487090Z","iopub.status.idle":"2024-12-08T10:11:50.505978Z","shell.execute_reply.started":"2024-12-08T10:11:50.487051Z","shell.execute_reply":"2024-12-08T10:11:50.504803Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"assign_and_save_regions(proposals_json, coco_json_pth, img_fldr, prop_fldr, actproposals_json, iou_threshold=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:30:53.933043Z","iopub.execute_input":"2024-12-08T10:30:53.933416Z","iopub.status.idle":"2024-12-08T10:42:50.070702Z","shell.execute_reply.started":"2024-12-08T10:30:53.933385Z","shell.execute_reply":"2024-12-08T10:42:50.069610Z"}},"outputs":[{"name":"stderr","text":"Elaborazione immagini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32199/32199 [11:37<00:00, 46.13it/s] \n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"print(ignored_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:43:09.519841Z","iopub.execute_input":"2024-12-08T10:43:09.520300Z","iopub.status.idle":"2024-12-08T10:43:09.539596Z","shell.execute_reply.started":"2024-12-08T10:43:09.520266Z","shell.execute_reply":"2024-12-08T10:43:09.538467Z"}},"outputs":[{"name":"stdout","text":"412\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# controllo sulle regioni\nfrom collections import Counter\n\nfile_path = '/kaggle/working/active_regions.json'\n\n#carica il file JSON\nwith open(file_path, 'r') as f:\n    data = json.load(f)\n\n#conta numero di regioni\nnum_regioni = len(data)\nprint(f\"Numero di regioni: {num_regioni}\")\n\n# occorrenze dei category_id\ncategory_ids = [entry['category_id'] for entry in data]\ncategory_counts = Counter(category_ids)\nprint(\"Occorrenze dei category_id:\", category_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:07:38.889794Z","iopub.execute_input":"2024-12-08T11:07:38.890510Z","iopub.status.idle":"2024-12-08T11:07:39.654913Z","shell.execute_reply.started":"2024-12-08T11:07:38.890472Z","shell.execute_reply":"2024-12-08T11:07:39.653826Z"}},"outputs":[{"name":"stdout","text":"Numero di regioni: 180600\nOccorrenze dei category_id: Counter({6: 159041, 1: 6132, 9: 3563, 4: 3488, 2: 2765, 0: 2731, 5: 1226, 8: 1159, 3: 329, 10: 98, 7: 68})\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"'''\nclass CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name): #prende il nome.jpg di una immagine e restituisce solo l'identificativo senza prefissi e suffissi\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f: #salva le region proposals in un file txt con direttamente le info della cartella\n            self.image_paths = [line.strip() for line in f.readlines()] #memorizzo i path delle immagini in una lista\n\n        with open(coco_json_file, 'r') as f: #leggo il file .json - contenente (...) - con coco\n            coco_data = json.load(f)\n\n        self.image_annotations = {} #dizionario contenente per ogni immagine una lista di categorie di oggetti presebti\n\n        for annotation in coco_data['annotations']: #uso la sezione annotazioni del file .json per ricavare delle info. sulle immagini del dataset\n            image_id = annotation['image_id']\n            category_id = annotation['category_id'] # lista di category_id = categorie degli oggetti nell'immagine)\n\n            if image_id not in self.image_annotations: #verifico se l'id dell'immagine √® gi√† presente nel dizionario\n                self.image_annotations[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        } #dizionario in cui per ogni nome dell'immagine ottenuta da generate_id(file_name) associa il nome.jpg dell'imagine\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) # trasformazione di base da applicare a tutte le immagini\n\n        # lasciare momentaneamente in caso di aggiornamenti futuri\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) # strasformazione per la data agumentation\n\n        self.aug = aug\n\n    def __len__(self): # ritorna il numero di elementi in self.image_paths -> chi √®?\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index]) #prendo il path dell'immagine da self.image_paths in base all'indice fornito\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', '')) #ricavo l'id dell'immagine -> non l'ho gi√† fatto nell'init?\n\n        if img_id not in self.image_info: #vedo dal dizionario self.image_info se l'immagine √® contenuta nel file COCO\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n\n        img_path = os.path.join(self.img_dir, img_name) #prendo il path completo dell'immagine unendo il path della cartella con il nome.jpg dell'immagine\n        if not os.path.exists(img_path): #se il path non esiste allora lo segnalo\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n        image = Image.open(img_path).convert('RGB') #apro l'immagine e ne ricavo le dimensioni\n        #original_width, original_height = image.size\n\n        if self.aug: #se la variabile self.aug √® alta allora applico la self.aug_transform altrimenti la self.base_transform _> HA SENSO? LE DUE FUNZIONI SONO = !!\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n\n        #POTREBBE ESSERCI UN PROBLEMA NELLA CORRISPONDENZA TRA LABLES E REGION PROPOSALS -> perch√® in proposals_tensor non ci sono tutte le region proposals perch√® alcune vengono scartate\n        # -> se la corrispondenza √® 1 a 1 allora potrebbe convenire dare a _generate_region_proposals(image) sia le labes che l'immagine? -> non √® certo perch√®\n        # in lables c'√® una lista di lable ma non viene specificato dove sono localizzate\n\n        # restituisce un dizionario\n        return {\n            \"regions\": image_tensor  # region proposals elaborate, una lista di tensori che rappresentano regioni candidate per il rilevamento\n        }\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:13:17.78606Z","iopub.status.idle":"2024-12-06T13:13:17.786622Z","shell.execute_reply.started":"2024-12-06T13:13:17.786288Z","shell.execute_reply":"2024-12-06T13:13:17.786307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        with open(json_file, 'r') as f:\n            self.data = json.load(f)  # Carica il file JSON\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor(),                 # Converte l'immagine in un tensore\n            transforms.Normalize(                  # Normalizzazione con valori di ImageNet\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225]\n            )\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni (ad esempio, ridimensionamento, normalizzazione)\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)\n        final = func.sigmoid(self.fn10(feature))\n\n        return feature, final","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}