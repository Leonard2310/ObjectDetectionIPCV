{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:11:50.738105Z","iopub.execute_input":"2024-12-06T13:11:50.738539Z","iopub.status.idle":"2024-12-06T13:12:07.387103Z","shell.execute_reply.started":"2024-12-06T13:11:50.738504Z","shell.execute_reply":"2024-12-06T13:12:07.385540Z"}},"outputs":[{"name":"stdout","text":"Collecting selectivesearch\n  Downloading selectivesearch-0.4.tar.gz (3.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (1.26.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (0.23.2)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (1.14.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->selectivesearch) (3.1.2)\nBuilding wheels for collected packages: selectivesearch\n  Building wheel for selectivesearch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for selectivesearch: filename=selectivesearch-0.4-py3-none-any.whl size=4335 sha256=0dd10aeed2eae7d1b58e3b4e3be2f1d9da631dbb99ddb8c2ad62b17c2cf5c3b3\n  Stored in directory: /root/.cache/pip/wheels/0e/49/95/01447a4e0f48a135ac91fbdb1dd2a1c0523e40e29957b383a3\nSuccessfully built selectivesearch\nInstalling collected packages: selectivesearch\nSuccessfully installed selectivesearch-0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall opencv-contrib-python opencv-python --yes\n\n!pip install opencv-contrib-python ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:12:07.390223Z","iopub.execute_input":"2024-12-06T13:12:07.390784Z","iopub.status.idle":"2024-12-06T13:12:25.955227Z","shell.execute_reply.started":"2024-12-06T13:12:07.390725Z","shell.execute_reply":"2024-12-06T13:12:25.953974Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: opencv-contrib-python 4.10.0.84\nUninstalling opencv-contrib-python-4.10.0.84:\n  Successfully uninstalled opencv-contrib-python-4.10.0.84\nFound existing installation: opencv-python 4.10.0.84\nUninstalling opencv-python-4.10.0.84:\n  Successfully uninstalled opencv-python-4.10.0.84\nCollecting opencv-contrib-python\n  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-contrib-python) (1.26.4)\nDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: opencv-contrib-python\nSuccessfully installed opencv-contrib-python-4.10.0.84\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport imageio.v3 as imageio\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nimport cv2\nimport shutil\nimport json\nimport yaml\nimport random\nimport time\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\nimport concurrent.futures\nimport multiprocessing as mp\nfrom PIL import Image, ImageOps\nfrom collections import defaultdict, Counter\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TF\nimport torch.optim as optim\nimport re\nimport selectivesearch\nimport torch.optim as optim\nfrom torchvision import models\nfrom torchvision.models import AlexNet_Weights\nimport matplotlib.patches as mpatches\nimport torch.nn.functional as F\nfrom sklearn.svm import SVC\nfrom concurrent.futures import ProcessPoolExecutor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:12:25.957183Z","iopub.execute_input":"2024-12-06T13:12:25.957608Z","iopub.status.idle":"2024-12-06T13:12:33.060039Z","shell.execute_reply.started":"2024-12-06T13:12:25.957568Z","shell.execute_reply":"2024-12-06T13:12:33.058806Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nOUT_COCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nOUT_DATAFRAME_NM = 'xview_labels.parquet'\nYAML_NM = 'xview_yolo.yaml'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\nout_data_parquet_pth = in_dataset_pth / OUT_DATAFRAME_NM\ncoco_json_pth = in_dataset_pth / OUT_COCO_JSON_NM\nyolo_yaml_pth = cfg_fldr_pth / YAML_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n# PROPOSALS\nOUT_NUMPY_FLDR_NM = 'proposals'\nnp_fldr = Path(f'/kaggle/working/{OUT_NUMPY_FLDR_NM}')\nOUT_PROPOSALS_FLDR_NM = 'proposals'\nprop_fldr = Path(f'/kaggle/working/{OUT_PROPOSALS_FLDR_NM}')\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = out_dataset_pth / PROP_COCO_JSON_NM\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:18:09.002097Z","iopub.execute_input":"2024-12-06T13:18:09.002477Z","iopub.status.idle":"2024-12-06T13:18:09.010126Z","shell.execute_reply.started":"2024-12-06T13:18:09.002442Z","shell.execute_reply":"2024-12-06T13:18:09.008810Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)\nclean_output(prop_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:12:33.074303Z","iopub.execute_input":"2024-12-06T13:12:33.074797Z","iopub.status.idle":"2024-12-06T13:12:33.096736Z","shell.execute_reply.started":"2024-12-06T13:12:33.074744Z","shell.execute_reply":"2024-12-06T13:12:33.095495Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\nCartella /kaggle/working/proposals non trovata. Nessuna azione necessaria.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"markdown","source":"## Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"def convert_images_to_npy(img_folder, npy_folder):\n    os.makedirs(npy_folder, exist_ok=True)\n    img_files = [img for img in os.listdir(img_folder) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n\n    for img_name in tqdm(img_files, desc=\"Converting images to .npy format\"):\n        img_path = os.path.join(img_folder, img_name)\n        # Leggi l'immagine\n        image = cv2.imread(img_path)\n        # Salva in formato .npy\n        npy_path = os.path.join(npy_folder, img_name.split('.')[0] + '.npy')\n        np.save(npy_path, image)\n\n# Converti le immagini\nconvert_images_to_npy(img_fldr, np_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:19:40.829882Z","iopub.execute_input":"2024-12-06T13:19:40.831418Z"}},"outputs":[{"name":"stderr","text":"Converting images to .npy format:  42%|████▏     | 19421/45891 [03:07<05:35, 78.83it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"## Region Proposals Generation\n\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine dal file .npy\n    image = np.load(img_path)\n    original_height, original_width, _ = image.shape\n\n    # Chiamata alla funzione unificata per generare le proposte\n    processed_proposals = generate_and_process_proposals(image, original_width, original_height)\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    # Prepara le proposte per l'output\n    for i, proposal in enumerate(processed_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data\n\ndef generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    with open(coco_json, 'r') as f:\n        coco_data = json.load(f)\n\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    images_with_annotations = [\n        image_data for image_data in coco_data['images'] \n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n    ]\n\n    # Usa ProcessPoolExecutor per elaborare le immagini in parallelo\n    max_workers = 4  # Regola il numero di processi in base al tuo ambiente\n    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Parallelizza l'elaborazione delle immagini e mostra il progresso con tqdm\n        results = list(tqdm(executor.map(process_single_image, images_with_annotations, [img_fldr]*len(images_with_annotations)), \n                           total=len(images_with_annotations), \n                           desc=\"Processing images\"))\n        \n    with open(output_json, 'w') as json_file:\n        json.dump(results, json_file, indent=4)\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:13:25.376018Z","iopub.execute_input":"2024-12-06T13:13:25.376457Z","iopub.status.idle":"2024-12-06T13:13:25.388965Z","shell.execute_reply.started":"2024-12-06T13:13:25.376382Z","shell.execute_reply":"2024-12-06T13:13:25.387703Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"'''\ndef generate_dataset_proposals(txt_file, dir_name):\n  # prendo i path delle immagini e li memorizzo in una lista\n   with open(txt_file, 'r') as f:\n            image_paths = [line.strip() for line in f.readlines()]\n\n  img_dir = img_dir\n  os.makedirs(img_dir, exist_ok=True)\n\n  for index in range(len(image_paths)):\n    img_name = os.path.basename(image_paths[index]) #prendo il path dell'immagine da self.image_paths in base all'indice fornito\n    img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', '')) #ricavo l'id dell'immagine -> non l'ho già fatto nell'init?\n\n    dir_image = os.path.join(dir_name, img_id) #nome della directory che conterrà le region proposals relative all'immagine\n\n    if img_id not in self.image_info: #vedo dal dizionario self.image_info se l'immagine è contenuta nel file COCO\n              raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n\n    img_path = os.path.join(self.img_dir, img_name) #prendo il path completo dell'immagine unendo il path della cartella con il nome.jpg dell'immagine\n    if not os.path.exists(img_path): #se il path non esiste allora lo segnalo\n              raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    image = Image.open(img_path).convert('RGB') #apro l'immagine e ne ricavo le dimensioni\n    original_width, original_height = image.size\n\n     # GESTIONE DELLE REGION PROPOSALS\n     proposals_tensor = generate_region_proposals(image) # image = immagine aperta in formato RGB con la libreria PIL\n     #    produce una lista di proposals nel formato (x_min, y_min, x_max, y_max)\n\n     processed_proposals = process_proposals(image_tensor, proposals_tensor) # image_tensor = image dopo la data agumentation\n     #    produce le immagine = region proposals relative all'immagine di input\n\n     # salvo le region proposals come immagini in una cartella relativa all'immagine di input\n     # - salvataggio in dir_image\n     os.makedirs(dir_image, exist_ok=True)\n\n    # Iterare sulle region proposals e salvarle come immagini\n    for i, proposal_tensor in enumerate(processed_proposals):\n        # Convertire il tensore in immagine PIL (assumendo valori nel range [0, 1])\n        proposal_image = Image.fromarray((proposal_tensor.numpy() * 255).astype('uint8'))\n\n        # Generare un nome file unico\n        proposal_filename = os.path.join(dir_image, f'proposal_{i:04d}.jpg')\n\n        # Salvare l'immagine\n        proposal_image.save(proposal_filename)\n\n        print(f\"Salvata proposal {i+1}/{len(processed_proposals)}: {proposal_filename}\")\n\n        # Aggiungi il path relativo alla lista\n        relative_path = os.path.relpath(proposal_filename, dir_name)  # Path relativo rispetto a dir_name\n        all_proposal_paths.append(relative_path)\n\n    # Scrittura di tutti i path relativi in un unico file .txt\n    with open(output_txt, 'w') as txt_file:\n        for path in all_proposal_paths:\n            txt_file.write(f\"{path}\\n\")\n\n    print(f\"Creato file TXT con i path relativi di tutte le region proposals: {output_txt}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:12:33.115274Z","iopub.execute_input":"2024-12-06T13:12:33.115827Z","iopub.status.idle":"2024-12-06T13:12:33.137226Z","shell.execute_reply.started":"2024-12-06T13:12:33.115787Z","shell.execute_reply":"2024-12-06T13:12:33.136082Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'\\ndef generate_dataset_proposals(txt_file, dir_name):\\n  # prendo i path delle immagini e li memorizzo in una lista\\n   with open(txt_file, \\'r\\') as f:\\n            image_paths = [line.strip() for line in f.readlines()]\\n\\n  img_dir = img_dir\\n  os.makedirs(img_dir, exist_ok=True)\\n\\n  for index in range(len(image_paths)):\\n    img_name = os.path.basename(image_paths[index]) #prendo il path dell\\'immagine da self.image_paths in base all\\'indice fornito\\n    img_id = int(img_name.replace(\\'_\\', \\'\\').replace(\\'.jpg\\', \\'\\').replace(\\'img\\', \\'\\')) #ricavo l\\'id dell\\'immagine -> non l\\'ho già fatto nell\\'init?\\n\\n    dir_image = os.path.join(dir_name, img_id) #nome della directory che conterrà le region proposals relative all\\'immagine\\n\\n    if img_id not in self.image_info: #vedo dal dizionario self.image_info se l\\'immagine è contenuta nel file COCO\\n              raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\\n\\n    img_path = os.path.join(self.img_dir, img_name) #prendo il path completo dell\\'immagine unendo il path della cartella con il nome.jpg dell\\'immagine\\n    if not os.path.exists(img_path): #se il path non esiste allora lo segnalo\\n              raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\\n\\n    image = Image.open(img_path).convert(\\'RGB\\') #apro l\\'immagine e ne ricavo le dimensioni\\n    original_width, original_height = image.size\\n\\n     # GESTIONE DELLE REGION PROPOSALS\\n     proposals_tensor = generate_region_proposals(image) # image = immagine aperta in formato RGB con la libreria PIL\\n     #    produce una lista di proposals nel formato (x_min, y_min, x_max, y_max)\\n\\n     processed_proposals = process_proposals(image_tensor, proposals_tensor) # image_tensor = image dopo la data agumentation\\n     #    produce le immagine = region proposals relative all\\'immagine di input\\n\\n     # salvo le region proposals come immagini in una cartella relativa all\\'immagine di input\\n     # - salvataggio in dir_image\\n     os.makedirs(dir_image, exist_ok=True)\\n\\n    # Iterare sulle region proposals e salvarle come immagini\\n    for i, proposal_tensor in enumerate(processed_proposals):\\n        # Convertire il tensore in immagine PIL (assumendo valori nel range [0, 1])\\n        proposal_image = Image.fromarray((proposal_tensor.numpy() * 255).astype(\\'uint8\\'))\\n\\n        # Generare un nome file unico\\n        proposal_filename = os.path.join(dir_image, f\\'proposal_{i:04d}.jpg\\')\\n\\n        # Salvare l\\'immagine\\n        proposal_image.save(proposal_filename)\\n\\n        print(f\"Salvata proposal {i+1}/{len(processed_proposals)}: {proposal_filename}\")\\n\\n        # Aggiungi il path relativo alla lista\\n        relative_path = os.path.relpath(proposal_filename, dir_name)  # Path relativo rispetto a dir_name\\n        all_proposal_paths.append(relative_path)\\n\\n    # Scrittura di tutti i path relativi in un unico file .txt\\n    with open(output_txt, \\'w\\') as txt_file:\\n        for path in all_proposal_paths:\\n            txt_file.write(f\"{path}\\n\")\\n\\n    print(f\"Creato file TXT con i path relativi di tutte le region proposals: {output_txt}\")\\n'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"'''\ndef generate_region_proposals(image, img_width, img_height): #funzione per la generazione delle region proposals per singola immagine\n        img_np = np.array(image) #trasformo l'immagine in un array numpy\n\n        if len(img_np.shape) == 3 and img_np.shape[0] == 3: #porto l'immagine nel formato corretto\n            img_np = np.transpose(img_np, (1, 2, 0))  # Da [C, H, W] a [H, W, C]\n            \n        _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.9, min_size=10) #richiamo la funzione di selective search\n        #scale: granularità della ricerca (più alto, meno dettagliato) ; \n        #sigma: Standard deviation per il filtro gaussiano usato per la segmentazione ;\n        #min_size: Dimensione minima di un segmento nell'algoritmo\n        #regions: lista di regioni candidate (proposals).\n        # - regione = dizionario che contiene info. -> incluse le coordinate di un rettangolo delimitante (region['rect'])\n\n        #CHECK SULLA PRODUZIONE DELLE REGION PROPOSALS\n        if len(regions) == 0:\n            print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n        candidate_proposals = []\n        for region in regions: #per ogni regione nella lista delle regioni candidate\n            x, y, w, h = region['rect'] # prendo le coordinate del rettangolo delimitante\n            if w > 0 and h > 0 and w >= 10 and h >= 10: # prendo solo le regioni con altezza e larghezza >= 10 per evitare che siano molto rumorose\n                area = w * h\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0]) # limito la regione alle dimensioni dell'immagine\n                candidate_proposals.append([x, y, x_max, y_max, area]) #inserisco la nuova regione nella lista delle region proposals -> aggiungo un valore in più (area) per facilitare il filtraggio dopo\n\n        unique_proposals = list(set(tuple(p) for p in candidate_proposals)) # converto le proposals in tuple in modo da eliminare i duplicati\n\n        #in questo modo non viene preservata la corrispondenza tra region proposals e labels\n\n        #FILTRO LE PROPOSALS PER PRENDERE SOLO QUELLE UTILI/NECESSARIE\n        min_area = 10\n        max_area_ratio = 0.8\n        proposals = []\n\n        for x_min, y_min, x_max, y_max, area in unique_proposals: #per ogni proposal\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\n                proposals.append((x_min, y_min, x_max, y_max))\n\n        return proposals # restituisce le region proposal valide\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:12:33.138327Z","iopub.execute_input":"2024-12-06T13:12:33.138754Z","iopub.status.idle":"2024-12-06T13:12:33.160780Z","shell.execute_reply.started":"2024-12-06T13:12:33.138719Z","shell.execute_reply":"2024-12-06T13:12:33.159572Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\ndef generate_region_proposals(image, img_width, img_height): #funzione per la generazione delle region proposals per singola immagine\\n        img_np = np.array(image) #trasformo l\\'immagine in un array numpy\\n\\n        if len(img_np.shape) == 3 and img_np.shape[0] == 3: #porto l\\'immagine nel formato corretto\\n            img_np = np.transpose(img_np, (1, 2, 0))  # Da [C, H, W] a [H, W, C]\\n            \\n        _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.9, min_size=10) #richiamo la funzione di selective search\\n        #scale: granularità della ricerca (più alto, meno dettagliato) ; \\n        #sigma: Standard deviation per il filtro gaussiano usato per la segmentazione ;\\n        #min_size: Dimensione minima di un segmento nell\\'algoritmo\\n        #regions: lista di regioni candidate (proposals).\\n        # - regione = dizionario che contiene info. -> incluse le coordinate di un rettangolo delimitante (region[\\'rect\\'])\\n\\n        #CHECK SULLA PRODUZIONE DELLE REGION PROPOSALS\\n        if len(regions) == 0:\\n            print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\\n\\n        candidate_proposals = []\\n        for region in regions: #per ogni regione nella lista delle regioni candidate\\n            x, y, w, h = region[\\'rect\\'] # prendo le coordinate del rettangolo delimitante\\n            if w > 0 and h > 0 and w >= 10 and h >= 10: # prendo solo le regioni con altezza e larghezza >= 10 per evitare che siano molto rumorose\\n                area = w * h\\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0]) # limito la regione alle dimensioni dell\\'immagine\\n                candidate_proposals.append([x, y, x_max, y_max, area]) #inserisco la nuova regione nella lista delle region proposals -> aggiungo un valore in più (area) per facilitare il filtraggio dopo\\n\\n        unique_proposals = list(set(tuple(p) for p in candidate_proposals)) # converto le proposals in tuple in modo da eliminare i duplicati\\n\\n        #in questo modo non viene preservata la corrispondenza tra region proposals e labels\\n\\n        #FILTRO LE PROPOSALS PER PRENDERE SOLO QUELLE UTILI/NECESSARIE\\n        min_area = 10\\n        max_area_ratio = 0.8\\n        proposals = []\\n\\n        for x_min, y_min, x_max, y_max, area in unique_proposals: #per ogni proposal\\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\\n                proposals.append((x_min, y_min, x_max, y_max))\\n\\n        return proposals # restituisce le region proposal valide\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"'''\ndef process_proposals(image_tensor, proposals, output_size=(227, 227)):  # la funzione trasforma le proposals trovate in immagini ottenute ritagliando l'immagine originale\n    processed_proposals = []\n    for proposal in proposals:  # per ogni proposal\n        try:\n            _, H, W = image_tensor.shape  # vedo le dimensioni dell'immagine\n            x_min, y_min, x_max, y_max = map(int, proposal)\n            x_min, y_min = max(0, x_min), max(0, y_min)\n            x_max, y_max = min(W, x_max), min(H, y_max)\n\n            # Controlla se la proposal ha dimensioni valide per l'immagine di partenza -> tecnicamente non si potrebbe eliminare l'if?\n            if x_min < x_max and y_min < y_max:\n                cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]  # Ritaglio\n\n                # Controlla che il ritaglio non sia vuoto\n                if cropped_region.size == 0:\n                    print(f\"Ritaglio vuoto per proposal: {proposal}. Salto.\")\n                    continue\n\n                # Controlla che il tensor sia 3D (C, H, W)\n                if cropped_region.ndim != 3:\n                    print(f\"Proposal non valida per il ridimensionamento: {proposal}. Salto.\")\n                    continue\n\n                # Converti cropped_region in un tensore PyTorch\n                cropped_region = torch.tensor(cropped_region).permute(2, 0, 1)  # Cambia il formato da HWC a CHW\n\n                # Ridimensiona la regione proposta\n                resized_region = torch.nn.functional.interpolate(\n                    cropped_region.unsqueeze(0), size=output_size, mode='bilinear', align_corners=False\n                ).squeeze(0)  # Ridimensiona\n\n                processed_proposals.append(resized_region)\n        except Exception as e:\n            print(f\"Errore durante il processamento della proposal: {proposal}. Errore: {e}\")\n\n    return processed_proposals  # Lista di tensori delle region proposals\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:12:33.162461Z","iopub.execute_input":"2024-12-06T13:12:33.162837Z","iopub.status.idle":"2024-12-06T13:12:33.184079Z","shell.execute_reply.started":"2024-12-06T13:12:33.162803Z","shell.execute_reply":"2024-12-06T13:12:33.182852Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'\\ndef process_proposals(image_tensor, proposals, output_size=(227, 227)):  # la funzione trasforma le proposals trovate in immagini ottenute ritagliando l\\'immagine originale\\n    processed_proposals = []\\n    for proposal in proposals:  # per ogni proposal\\n        try:\\n            _, H, W = image_tensor.shape  # vedo le dimensioni dell\\'immagine\\n            x_min, y_min, x_max, y_max = map(int, proposal)\\n            x_min, y_min = max(0, x_min), max(0, y_min)\\n            x_max, y_max = min(W, x_max), min(H, y_max)\\n\\n            # Controlla se la proposal ha dimensioni valide per l\\'immagine di partenza -> tecnicamente non si potrebbe eliminare l\\'if?\\n            if x_min < x_max and y_min < y_max:\\n                cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]  # Ritaglio\\n\\n                # Controlla che il ritaglio non sia vuoto\\n                if cropped_region.size == 0:\\n                    print(f\"Ritaglio vuoto per proposal: {proposal}. Salto.\")\\n                    continue\\n\\n                # Controlla che il tensor sia 3D (C, H, W)\\n                if cropped_region.ndim != 3:\\n                    print(f\"Proposal non valida per il ridimensionamento: {proposal}. Salto.\")\\n                    continue\\n\\n                # Converti cropped_region in un tensore PyTorch\\n                cropped_region = torch.tensor(cropped_region).permute(2, 0, 1)  # Cambia il formato da HWC a CHW\\n\\n                # Ridimensiona la regione proposta\\n                resized_region = torch.nn.functional.interpolate(\\n                    cropped_region.unsqueeze(0), size=output_size, mode=\\'bilinear\\', align_corners=False\\n                ).squeeze(0)  # Ridimensiona\\n\\n                processed_proposals.append(resized_region)\\n        except Exception as e:\\n            print(f\"Errore durante il processamento della proposal: {proposal}. Errore: {e}\")\\n\\n    return processed_proposals  # Lista di tensori delle region proposals\\n'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import concurrent.futures\n\ndef generate_and_process_proposals(image, img_width, img_height):\n\n    # Esegui la selective search per trovare le regioni di interesse (proposals)\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.9, min_size=10)\n\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []  # Lista per le proposte elaborate\n\n    # Pre-filtraggio delle regioni e raccolta delle coordinate delle proposte\n    for region in regions:\n        x, y, w, h = region['rect']\n        \n        # Controlla se la regione è abbastanza grande senza calcoli inutili\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            # Aggiungi le coordinate alla lista delle proposte\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals  # Restituisce solo le coordinate delle proposte\n\n# Funzione per elaborare un batch di immagini in parallelo\ndef process_images_in_parallel(images, img_width, img_height):\n    # Utilizza concurrent.futures per elaborare più immagini in parallelo\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        # La funzione generate_and_process_proposals verrà applicata su ogni immagine\n        results = list(executor.map(lambda image: generate_and_process_proposals(image, img_width, img_height), images))\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:12:33.187882Z","iopub.execute_input":"2024-12-06T13:12:33.188536Z","iopub.status.idle":"2024-12-06T13:12:33.204549Z","shell.execute_reply.started":"2024-12-06T13:12:33.188483Z","shell.execute_reply":"2024-12-06T13:12:33.203419Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"generate_dataset_proposals(coco_json_pth, img_fldr, prop_fldr, proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:13:30.012884Z","iopub.execute_input":"2024-12-06T13:13:30.013293Z","iopub.status.idle":"2024-12-06T13:16:49.126404Z","shell.execute_reply.started":"2024-12-06T13:13:30.013256Z","shell.execute_reply":"2024-12-06T13:16:49.124801Z"}},"outputs":[{"name":"stderr","text":"Processing images:   1%|▏         | 473/32199 [03:10<3:33:12,  2.48it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_dataset_proposals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_json_pth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_fldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop_fldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals_json\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 60\u001b[0m, in \u001b[0;36mgenerate_dataset_proposals\u001b[0;34m(coco_json, img_fldr, output_dir, output_json)\u001b[0m\n\u001b[1;32m     57\u001b[0m max_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Regola il numero di processi in base al tuo ambiente\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Parallelizza l'elaborazione delle immagini e mostra il progresso con tqdm\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_single_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_with_annotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_fldr\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages_with_annotations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages_with_annotations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_json, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m     65\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(results, json_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"## Positive Region Proposals","metadata":{}},{"cell_type":"code","source":"# Funzione IoU per calcolare la sovrapposizione\ndef box_iou(boxes1, boxes2):\n    \"\"\"Calcola la IoU tra due set di bounding boxes.\"\"\"\n    # Espande le dimensioni per il broadcasting\n    boxes1 = boxes1.unsqueeze(1)  # (N, 1, 4)\n    boxes2 = boxes2.unsqueeze(0)  # (1, M, 4)\n    \n    # Calcola gli estremi delle intersezioni\n    inter_min = torch.max(boxes1[:, :, :2], boxes2[:, :, :2])  # (N, M, 2)\n    inter_max = torch.min(boxes1[:, :, 2:], boxes2[:, :, 2:])  # (N, M, 2)\n    inter_sizes = (inter_max - inter_min).clamp(min=0)  # Nessuna area negativa\n    inter_area = inter_sizes[:, :, 0] * inter_sizes[:, :, 1]  # Area dell'intersezione\n    \n    # Calcola le aree delle bounding boxes\n    boxes1_area = (boxes1[:, :, 2] - boxes1[:, :, 0]) * (boxes1[:, :, 3] - boxes1[:, :, 1])\n    boxes2_area = (boxes2[:, :, 2] - boxes2[:, :, 0]) * (boxes2[:, :, 3] - boxes2[:, :, 1])\n    \n    # Calcola l'area dell'unione\n    union_area = boxes1_area + boxes2_area - inter_area\n    return inter_area / union_area  # IoU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:13:17.781596Z","iopub.status.idle":"2024-12-06T13:13:17.782031Z","shell.execute_reply.started":"2024-12-06T13:13:17.781822Z","shell.execute_reply":"2024-12-06T13:13:17.781841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, iou_threshold=0.5):\n    \"\"\"Associa le regioni proposte ai bounding boxes e salva le regioni positive come immagini.\"\"\"\n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n    \n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        annotations_by_image[img_id].append((annot[\"bbox\"], annot[\"category_id\"]))\n    \n    # Crea un dizionario per mappare category_id ai nomi delle categorie\n    category_mapping = {cat_id: name for cat_id, name in enumerate(bboxes[\"categories\"])}\n    \n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n    \n    train_images = []  # Per tracciare i percorsi delle immagini salvate\n    train_labels = []  # Etichette corrispondenti (category_id)\n    counter = 0  # Contatore delle immagini salvate\n    \n    # Per ogni immagine nelle regioni\n    for image in regions:\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n        \n        # Ottieni bounding boxes ground-truth e categorie per l'immagine corrente\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Se non ci sono bounding boxes ground-truth, salta l'immagine\n            continue\n        \n        gt_bboxes = torch.tensor([item[0] for item in gt_data], dtype=torch.float32)\n        gt_categories = [item[1] for item in gt_data]\n        \n        # Trasforma proposals in tensori\n        proposal_coords = torch.tensor([p[\"coordinates\"] for p in proposals], dtype=torch.float32)\n        \n        # Calcola la matrice IoU\n        iou_matrix = box_iou(proposal_coords, gt_bboxes)\n        \n        # Identifica le regioni positive (IoU >= soglia)\n        max_ious, indices = torch.max(iou_matrix, dim=1)\n        positive_indices = torch.nonzero(max_ious >= iou_threshold).squeeze(1)\n        \n        # Carica l'immagine originale\n        image_path = os.path.join(image_dir, file_name)\n        original_image = cv2.imread(image_path)\n        if original_image is None:\n            print(f\"Immagine non trovata: {image_path}\")\n            continue\n        \n        # Per ogni regione positiva, ritaglia e salva l'immagine\n        for idx in positive_indices:\n            x_min, y_min, x_max, y_max = proposal_coords[idx].int().tolist()\n            cropped = original_image[y_min:y_max, x_min:x_max]\n            \n            # Ridimensiona a 224x224\n            resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n            \n            # Ottieni l'etichetta della categoria dal bounding box assegnato\n            category_id = gt_categories[indices[idx].item()]\n            \n            # Salva l'immagine\n            output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n            cv2.imwrite(output_path, resized)\n            \n            # Aggiorna train_images e train_labels\n            train_images.append(output_path)\n            train_labels.append(category_id)\n            \n            counter += 1\n    \n    return train_images, train_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:13:17.783046Z","iopub.status.idle":"2024-12-06T13:13:17.783510Z","shell.execute_reply.started":"2024-12-06T13:13:17.783288Z","shell.execute_reply":"2024-12-06T13:13:17.783306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name): #prende il nome.jpg di una immagine e restituisce solo l'identificativo senza prefissi e suffissi\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f: #salva le region proposals in un file txt con direttamente le info della cartella\n            self.image_paths = [line.strip() for line in f.readlines()] #memorizzo i path delle immagini in una lista\n\n        with open(coco_json_file, 'r') as f: #leggo il file .json - contenente (...) - con coco\n            coco_data = json.load(f)\n\n        self.image_annotations = {} #dizionario contenente per ogni immagine una lista di categorie di oggetti presebti\n\n        for annotation in coco_data['annotations']: #uso la sezione annotazioni del file .json per ricavare delle info. sulle immagini del dataset\n            image_id = annotation['image_id']\n            category_id = annotation['category_id'] # lista di category_id = categorie degli oggetti nell'immagine)\n\n            if image_id not in self.image_annotations: #verifico se l'id dell'immagine è già presente nel dizionario\n                self.image_annotations[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        } #dizionario in cui per ogni nome dell'immagine ottenuta da generate_id(file_name) associa il nome.jpg dell'imagine\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) # trasformazione di base da applicare a tutte le immagini\n\n        # lasciare momentaneamente in caso di aggiornamenti futuri\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) # strasformazione per la data agumentation\n\n        self.aug = aug\n\n    def __len__(self): # ritorna il numero di elementi in self.image_paths -> chi è?\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index]) #prendo il path dell'immagine da self.image_paths in base all'indice fornito\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', '')) #ricavo l'id dell'immagine -> non l'ho già fatto nell'init?\n\n        if img_id not in self.image_info: #vedo dal dizionario self.image_info se l'immagine è contenuta nel file COCO\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n\n        img_path = os.path.join(self.img_dir, img_name) #prendo il path completo dell'immagine unendo il path della cartella con il nome.jpg dell'immagine\n        if not os.path.exists(img_path): #se il path non esiste allora lo segnalo\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n        image = Image.open(img_path).convert('RGB') #apro l'immagine e ne ricavo le dimensioni\n        #original_width, original_height = image.size\n\n        if self.aug: #se la variabile self.aug è alta allora applico la self.aug_transform altrimenti la self.base_transform _> HA SENSO? LE DUE FUNZIONI SONO = !!\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n\n        #POTREBBE ESSERCI UN PROBLEMA NELLA CORRISPONDENZA TRA LABLES E REGION PROPOSALS -> perchè in proposals_tensor non ci sono tutte le region proposals perchè alcune vengono scartate\n        # -> se la corrispondenza è 1 a 1 allora potrebbe convenire dare a _generate_region_proposals(image) sia le labes che l'immagine? -> non è certo perchè\n        # in lables c'è una lista di lable ma non viene specificato dove sono localizzate\n\n        # restituisce un dizionario\n        return {\n            \"regions\": image_tensor  # region proposals elaborate, una lista di tensori che rappresentano regioni candidate per il rilevamento\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:13:17.786060Z","iopub.status.idle":"2024-12-06T13:13:17.786622Z","shell.execute_reply.started":"2024-12-06T13:13:17.786288Z","shell.execute_reply":"2024-12-06T13:13:17.786307Z"}},"outputs":[],"execution_count":null}]}