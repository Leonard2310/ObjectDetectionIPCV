{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":10189581,"sourceType":"datasetVersion","datasetId":6295408},{"sourceId":10197950,"sourceType":"datasetVersion","datasetId":6258380}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:24.077710Z","iopub.execute_input":"2024-12-19T12:38:24.077943Z","iopub.status.idle":"2024-12-19T12:38:35.875964Z","shell.execute_reply.started":"2024-12-19T12:38:24.077918Z","shell.execute_reply":"2024-12-19T12:38:35.875048Z"}},"outputs":[{"name":"stdout","text":"Collecting selectivesearch\n  Downloading selectivesearch-0.4.tar.gz (3.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (1.26.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (0.23.2)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (1.14.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->selectivesearch) (3.1.2)\nBuilding wheels for collected packages: selectivesearch\n  Building wheel for selectivesearch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for selectivesearch: filename=selectivesearch-0.4-py3-none-any.whl size=4335 sha256=53d83dcb6da09dfc43418e60bd6f3c110fb5626c293c7749cc2204985a44bd61\n  Stored in directory: /root/.cache/pip/wheels/0e/49/95/01447a4e0f48a135ac91fbdb1dd2a1c0523e40e29957b383a3\nSuccessfully built selectivesearch\nInstalling collected packages: selectivesearch\nSuccessfully installed selectivesearch-0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Librerie per l'ottimizzazione e la gestione delle dipendenze\nimport selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:35.877203Z","iopub.execute_input":"2024-12-19T12:38:35.877585Z","iopub.status.idle":"2024-12-19T12:38:36.526813Z","shell.execute_reply.started":"2024-12-19T12:38:35.877556Z","shell.execute_reply":"2024-12-19T12:38:36.526144Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport zipfile\nimport torchvision\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:36.527898Z","iopub.execute_input":"2024-12-19T12:38:36.528354Z","iopub.status.idle":"2024-12-19T12:38:42.511799Z","shell.execute_reply.started":"2024-12-19T12:38:36.528317Z","shell.execute_reply":"2024-12-19T12:38:42.511140Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nCOCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_COCO_JSON_NM = 'mod_COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nprop_dataset_pth = Path('/kaggle/input/proposals-xview-dataset-r-cnn')\nactreg_dataset_pth = Path('/kaggle/input/activeregion-xviewdataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nin_new_coco_json_pth = prop_dataset_pth / OUT_COCO_JSON_NM\n\n# PROPOSALS\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = prop_dataset_pth / PROP_COCO_JSON_NM\nout_proposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nPROPOSALS = 'proposals'\nprop_fldr = out_dataset_pth / PROPOSALS\n\n\n# ACTIVE REGIONS\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\nin_actproposals_json = actreg_dataset_pth / ACTPROP_COCO_JSON_NM\nout_act_reg_folder = out_dataset_pth / PROPOSALS\nact_reg_folder = actreg_dataset_pth / PROPOSALS\n\n#DATASET\ntrain_path = out_dataset_pth / 'train.json'\ntest_path = out_dataset_pth / 'test.json'\nval_path = out_dataset_pth / 'val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:35:12.416769Z","iopub.execute_input":"2024-12-19T13:35:12.417608Z","iopub.status.idle":"2024-12-19T13:35:12.424327Z","shell.execute_reply.started":"2024-12-19T13:35:12.417573Z","shell.execute_reply":"2024-12-19T13:35:12.423506Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:42.520382Z","iopub.execute_input":"2024-12-19T12:38:42.520937Z","iopub.status.idle":"2024-12-19T12:38:42.535909Z","shell.execute_reply.started":"2024-12-19T12:38:42.520911Z","shell.execute_reply":"2024-12-19T12:38:42.535033Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:42.537995Z","iopub.execute_input":"2024-12-19T12:38:42.538247Z","iopub.status.idle":"2024-12-19T12:38:42.545526Z","shell.execute_reply.started":"2024-12-19T12:38:42.538224Z","shell.execute_reply":"2024-12-19T12:38:42.544962Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON dal percorso specificato.\n\n    :param file_path: Percorso al file JSON da caricare.\n    :return: Dati contenuti nel file JSON (come dizionario o lista).\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:42.546308Z","iopub.execute_input":"2024-12-19T12:38:42.546645Z","iopub.status.idle":"2024-12-19T12:38:42.556181Z","shell.execute_reply.started":"2024-12-19T12:38:42.546615Z","shell.execute_reply":"2024-12-19T12:38:42.555328Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    data = load_json(input_path)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Lista di nuove annotazioni da aggiungere per immagini senza bbox\n    new_annotations = []\n \n    # Elenco di annotazioni da rimuovere\n    annotations_to_remove = []\n \n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        x, y, width, height = annotation['bbox']\n        xmin = x\n        xmax = x + width\n        ymin = y\n        ymax = y + height\n        # Verifica che xmin < xmax e ymin < ymax\n        if xmin >= xmax or ymin >= ymax:\n            annotations_to_remove.append(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, ymin, xmax, ymax]\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = [ann for ann in data['annotations'] if ann['id'] not in annotations_to_remove]\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, 0.0, image['width'], image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n \n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n \n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:42.557518Z","iopub.execute_input":"2024-12-19T12:38:42.557750Z","iopub.status.idle":"2024-12-19T12:38:42.571687Z","shell.execute_reply.started":"2024-12-19T12:38:42.557728Z","shell.execute_reply":"2024-12-19T12:38:42.571001Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:42.572534Z","iopub.execute_input":"2024-12-19T12:38:42.572811Z","iopub.status.idle":"2024-12-19T12:38:57.696248Z","shell.execute_reply.started":"2024-12-19T12:38:42.572788Z","shell.execute_reply":"2024-12-19T12:38:57.695574Z"}},"outputs":[{"name":"stderr","text":"Processing Categories: 100%|██████████| 11/11 [00:00<00:00, 104857.60it/s]\nBuilding Image Annotations Dictionary: 100%|██████████| 669983/669983 [00:00<00:00, 2456763.46it/s]\nProcessing Annotations: 100%|██████████| 669983/669983 [00:03<00:00, 199137.43it/s]\nProcessing Images: 100%|██████████| 45891/45891 [00:00<00:00, 712071.67it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Category Check","metadata":{}},{"cell_type":"code","source":"def count_bounding_boxes(json_path):\n    \"\"\"\n    Conta il numero di bounding box per ogni categoria in un file COCO JSON.\n\n    Args:\n        json_path (str): Percorso del file JSON.\n\n    Returns:\n        list: Elenco di tuple con ID categoria, nome categoria e numero di bounding box.\n    \"\"\"\n    # Carica il file JSON\n    coco_data = load_json(json_path)\n\n    # Estrarre i dati principali\n    annotations = coco_data.get(\"annotations\", [])\n    categories = coco_data.get(\"categories\", [])\n\n    # Mappare id di categoria ai nomi delle categorie\n    category_id_to_name = {category[\"id\"]: category[\"name\"] for category in categories}\n\n    # Contare i bounding box per categoria\n    bbox_counts = defaultdict(int)\n    for annotation in annotations:\n        category_id = annotation[\"category_id\"]\n        bbox_counts[category_id] += 1\n\n    # Creare un elenco dei risultati\n    results = [\n        (cat_id, category_id_to_name.get(cat_id, \"Unknown\"), count)\n        for cat_id, count in bbox_counts.items()\n    ]\n    \n    # Ordinare i risultati in ordine decrescente per numero di bounding box\n    results.sort(key=lambda x: x[2], reverse=True)\n    \n    # Stampare i risultati\n    for cat_id, category_name, count in results:\n        print(f\"Categoria ID {cat_id} ('{category_name}'): {count} bounding box\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:57.697353Z","iopub.execute_input":"2024-12-19T12:38:57.697693Z","iopub.status.idle":"2024-12-19T12:38:57.704503Z","shell.execute_reply.started":"2024-12-19T12:38:57.697656Z","shell.execute_reply":"2024-12-19T12:38:57.703621Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"count_bounding_boxes(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:57.705428Z","iopub.execute_input":"2024-12-19T12:38:57.705656Z","iopub.status.idle":"2024-12-19T12:39:00.927745Z","shell.execute_reply.started":"2024-12-19T12:38:57.705633Z","shell.execute_reply":"2024-12-19T12:39:00.926898Z"}},"outputs":[{"name":"stdout","text":"Categoria ID 6 ('Building'): 384929 bounding box\nCategoria ID 1 ('Passenger Vehicle'): 224911 bounding box\nCategoria ID 2 ('Truck'): 34345 bounding box\nCategoria ID 0 ('background'): 13691 bounding box\nCategoria ID 4 ('Maritime Vessel'): 6329 bounding box\nCategoria ID 5 ('Engineering Vehicle'): 5477 bounding box\nCategoria ID 9 ('Shipping Container'): 5388 bounding box\nCategoria ID 3 ('Railway Vehicle'): 4233 bounding box\nCategoria ID 8 ('Storage Tank'): 2033 bounding box\nCategoria ID 11 ('Aircraft'): 1708 bounding box\nCategoria ID 10 ('Pylon'): 470 bounding box\nCategoria ID 7 ('Helipad'): 152 bounding box\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalità RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:39:00.928738Z","iopub.execute_input":"2024-12-19T12:39:00.929005Z","iopub.status.idle":"2024-12-19T12:39:00.935898Z","shell.execute_reply.started":"2024-12-19T12:39:00.928980Z","shell.execute_reply":"2024-12-19T12:39:00.935117Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    '''\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.8, min_size=20)\n    '''\n    _, regions = selectivesearch.selective_search(img_np, scale=200, sigma=0.5, min_size=10)\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:39:00.936991Z","iopub.execute_input":"2024-12-19T12:39:00.937403Z","iopub.status.idle":"2024-12-19T12:39:00.952744Z","shell.execute_reply.started":"2024-12-19T12:39:00.937367Z","shell.execute_reply":"2024-12-19T12:39:00.952051Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:39:00.953520Z","iopub.execute_input":"2024-12-19T12:39:00.953833Z","iopub.status.idle":"2024-12-19T12:39:00.966821Z","shell.execute_reply.started":"2024-12-19T12:39:00.953792Z","shell.execute_reply":"2024-12-19T12:39:00.966043Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    coco_data = load_json(coco_json)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    # Filtra le immagini che contengono annotazioni con category_id == 0 (sfondo)\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n        and not any(annot['category_id'] == 0 for annot in image_annotations_map[image_data['id']])  # Escludi sfondo\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:39:00.967926Z","iopub.execute_input":"2024-12-19T12:39:00.968657Z","iopub.status.idle":"2024-12-19T12:39:00.977768Z","shell.execute_reply.started":"2024-12-19T12:39:00.968600Z","shell.execute_reply":"2024-12-19T12:39:00.976945Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"generate_dataset_proposals(new_coco_json_pth, img_fldr, prop_fldr, out_proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:35:20.190418Z","iopub.execute_input":"2024-12-19T13:35:20.190754Z","iopub.status.idle":"2024-12-19T14:26:45.442256Z","shell.execute_reply.started":"2024-12-19T13:35:20.190727Z","shell.execute_reply":"2024-12-19T14:26:45.440955Z"}},"outputs":[{"name":"stderr","text":"Processing batches: 100%|██████████| 65/65 [51:18<00:00, 47.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"Creato file JSON con le region proposals: /kaggle/working/proposals.json\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"proposals-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"mod_COCO_annotations.json\",\n    \"proposals.json\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:31:22.144108Z","iopub.execute_input":"2024-12-19T14:31:22.144492Z","iopub.status.idle":"2024-12-19T14:31:29.545757Z","shell.execute_reply.started":"2024-12-19T14:31:22.144462Z","shell.execute_reply":"2024-12-19T14:31:29.544813Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Positive Region Proposals with Undersampling","metadata":{}},{"cell_type":"code","source":"def calculate_bbox_areas_for_all_categories(json_file_path):\n    \"\"\"\n    Calcola la media delle aree di tutti i bounding box per ogni category_id presente nel dataset.\n    Stampa il risultato per ogni category_id.\n\n    :param json_file_path: Percorso al file JSON contenente le annotazioni.\n    :return: Un dizionario con il category_id come chiave e la media delle aree come valore.\n    \"\"\"\n    # Carica il file JSON\n    data = load_json(json_file_path)\n    \n    # Crea un dizionario per raccogliere le aree per ogni category_id\n    areas_per_category = defaultdict(list)\n    \n    # Crea un dizionario per mappare category_id a category_name\n    category_map = {category['id']: category['name'] for category in data['categories']}\n\n    # Itera sulle annotazioni per raccogliere le aree\n    for annotation in data.get(\"annotations\", []):\n        category_id = annotation[\"category_id\"]\n        area = annotation[\"area\"]\n        areas_per_category[category_id].append(area)\n\n    # Calcola e stampa la media delle aree per ogni category_id\n    for category_id, areas in areas_per_category.items():\n        if areas:\n            average_area = sum(areas) / len(areas)\n            category_name = category_map.get(category_id, \"Unknown\")\n            print(f\"Categoria: {category_name} (ID: {category_id}) - Area media: {average_area:.2f}\")\n        else:\n            category_name = category_map.get(category_id, \"Unknown\")\n            print(f\"Categoria: {category_name} (ID: {category_id}) - Nessuna area disponibile\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:30:38.343828Z","iopub.status.idle":"2024-12-19T13:30:38.344158Z","shell.execute_reply.started":"2024-12-19T13:30:38.344006Z","shell.execute_reply":"2024-12-19T13:30:38.344023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esempio di utilizzo\ncalculate_bbox_areas_for_all_categories(in_new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:30:38.345642Z","iopub.status.idle":"2024-12-19T13:30:38.345939Z","shell.execute_reply.started":"2024-12-19T13:30:38.345798Z","shell.execute_reply":"2024-12-19T13:30:38.345813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:42:19.503127Z","iopub.execute_input":"2024-12-19T14:42:19.503835Z","iopub.status.idle":"2024-12-19T14:42:19.511238Z","shell.execute_reply.started":"2024-12-19T14:42:19.503801Z","shell.execute_reply":"2024-12-19T14:42:19.510423Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def get_adaptive_threshold(bbox):\n    \"\"\"Calcola un threshold IoU adattivo in base all'area del bounding box.\"\"\"\n    # bbox è un tensore o una lista con [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]  # Calcolo larghezza\n    height = bbox[3] - bbox[1]  # Calcolo altezza\n    \n    # Calcolo dell'area\n    area = width * height\n    \n    # Definizione delle aree media per categoria\n    min_area = 200  \n    max_area = 8000  \n    \n    # Normalizziamo l'area rispetto all'intervallo di area definito\n    normalized_area = (area - min_area) / (max_area - min_area)  # Valore tra 0 e 1\n    \n    # Clipping per evitare valori fuori intervallo\n    normalized_area = max(0, min(1, normalized_area))\n    \n    # Mappiamo il valore normalizzato su un intervallo di soglie\n    # Threshold tra 0.3 e 0.6 (ad esempio)\n    threshold = 0.3 + 0.3 * normalized_area  \n\n    return threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:42:21.467063Z","iopub.execute_input":"2024-12-19T14:42:21.467473Z","iopub.status.idle":"2024-12-19T14:42:21.473216Z","shell.execute_reply.started":"2024-12-19T14:42:21.467428Z","shell.execute_reply":"2024-12-19T14:42:21.472333Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path):\n    \"\"\"Associa le regioni proposte ai bounding boxes, salva le regioni positive come immagini e crea un nuovo JSON con informazioni attivate.\"\"\"\n    \n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n    \n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        bbox = annot[\"bbox\"]\n        annotations_by_image[img_id].append((torch.tensor(bbox, dtype=torch.float32), annot[\"category_id\"]))\n    \n    # Crea un dizionario per mappare category_id ai nomi delle categorie\n    category_mapping = {cat_id: name for cat_id, name in enumerate(bboxes[\"categories\"])}\n    \n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n\n    counter = 0  # Contatore delle immagini salvate\n    \n    active_region_data = []  # Lista per i dati delle regioni attive\n\n    # Avvolgi il ciclo principale per ogni immagine con tqdm (una barra di progresso generale)\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n        \n        # Ottieni bounding boxes ground-truth e categorie per l'immagine corrente\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Se non ci sono bounding boxes ground-truth, salta l'immagine\n            continue\n        \n        gt_bboxes = [item[0] for item in gt_data]  # Bounding box ground truth\n        gt_categories = [item[1] for item in gt_data]  # Categorie ground truth\n        \n        # Trasforma proposals in una lista di dizionari compatibili con get_iou\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1], \n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n        \n        # Esegui undersampling per le categorie 0 e 6 prima di procedere\n        filtered_proposals = []\n        filtered_gt_bboxes = []\n        filtered_gt_categories = []\n        \n        for i, (prop, gt_bbox, gt_category) in enumerate(zip(proposal_coords, gt_bboxes, gt_categories)):\n            filtered_proposals.append(prop)\n            filtered_gt_bboxes.append(gt_bbox)\n            filtered_gt_categories.append(gt_category)\n\n        # Trasformiamo di nuovo in coordinate per IoU\n        proposal_coords = filtered_proposals\n        gt_bboxes = filtered_gt_bboxes\n        gt_categories = filtered_gt_categories\n\n        # Calcola la matrice IoU usando la funzione get_iou\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        # Verifica se la matrice IoU è vuota\n        if not iou_matrix:\n            continue\n        \n        iou_matrix = torch.tensor(iou_matrix)\n\n        # Identifica le regioni positive utilizzando la soglia adattiva\n        positive_indices = []\n        for row_idx, row in enumerate(iou_matrix):\n            for col_idx, iou in enumerate(row):\n                adaptive_threshold = get_adaptive_threshold(gt_bboxes[col_idx])\n                \n                if iou >= adaptive_threshold:\n                    positive_indices.append((row_idx, col_idx))\n\n        # Carica l'immagine originale\n        image_path = os.path.join(image_dir, file_name)\n        original_image = cv2.imread(image_path)\n        if original_image is None:\n            print(f\"Immagine non trovata: {image_path}\")\n            continue\n\n        # Avvolgi il ciclo per ogni proposta positiva\n        for row_idx, col_idx in positive_indices:\n            category_id = gt_categories[col_idx]\n            \n            # Aggiungi la condizione per includere solo il 10% delle categorie 0 e 6\n            if category_id in [6]:\n                if random.random() > 0.9:  # Mantieni il 10% delle categorie 0 e 6\n                    # Calcola le coordinate del bounding box\n                    x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                    \n                    cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                    \n                    # Ridimensiona a 224x224\n                    resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                    \n                    # Salva l'immagine\n                    output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                    cv2.imwrite(output_path, resized)\n                    \n                    # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                    active_region_data.append({\n                        \"image_id\": image_id,\n                        \"file_name\": file_name,\n                        \"category_id\": category_id,\n                        \"proposal_id\": row_idx,\n                        \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                        \"original_bbox\": gt_bboxes[col_idx].tolist(),  # Aggiunge il bbox originale\n                        \"saved_path\": output_path\n                    })\n                    \n                    counter += 1\n            else:\n                # Se la categoria non è 0 o 6, includi sempre la proposta\n                # Calcola le coordinate del bounding box\n                x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                \n                cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                \n                # Ridimensiona a 224x224\n                resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                \n                # Salva l'immagine\n                output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                cv2.imwrite(output_path, resized)\n                \n                # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                active_region_data.append({\n                    \"image_id\": image_id,\n                    \"file_name\": file_name,\n                    \"category_id\": category_id,\n                    \"proposal_id\": row_idx,\n                    \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                    \"original_bbox\": gt_bboxes[col_idx].tolist(),  # Aggiunge il bbox originale\n                    \"saved_path\": output_path\n                })\n                \n                counter += 1\n\n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:42:23.143529Z","iopub.execute_input":"2024-12-19T14:42:23.143838Z","iopub.status.idle":"2024-12-19T14:42:23.162614Z","shell.execute_reply.started":"2024-12-19T14:42:23.143812Z","shell.execute_reply":"2024-12-19T14:42:23.161652Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Esegui l'assegnazione e ottieni i valori IoU\n\nassign_and_save_regions(out_proposals_json, new_coco_json_pth, img_fldr, prop_fldr, actproposals_json)\n'''\nassign_and_save_regions(proposals_json, in_new_coco_json_pth, img_fldr, prop_fldr, actproposals_json)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:44:20.868365Z","iopub.execute_input":"2024-12-19T14:44:20.869121Z","iopub.status.idle":"2024-12-19T15:21:59.177161Z","shell.execute_reply.started":"2024-12-19T14:44:20.869088Z","shell.execute_reply":"2024-12-19T15:21:59.176257Z"}},"outputs":[{"name":"stderr","text":"Elaborazione immagini: 100%|██████████| 32200/32200 [37:18<00:00, 14.39it/s]  \n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'\\nassign_and_save_regions(proposals_json, in_new_coco_json_pth, img_fldr, prop_fldr, actproposals_json)\\n'"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"def analyze_regions(file_path):\n    \"\"\"\n    Analizza un file JSON per ottenere il numero di regioni e le occorrenze dei category_id.\n\n    :param file_path: Percorso al file JSON contenente le annotazioni.\n    :return: Tupla contenente il numero di regioni e un dizionario con le occorrenze dei category_id.\n    \"\"\"\n    # Carica il file JSON\n    data = load_json(file_path)\n\n    # Conta il numero di regioni\n    num_regioni = len(data)\n    \n    # Ottieni le occorrenze dei category_id\n    category_ids = [entry['category_id'] for entry in data]\n    category_counts = Counter(category_ids)\n\n    # Ordina le occorrenze per ID di categoria\n    sorted_category_counts = dict(sorted(category_counts.items()))\n\n    return num_regioni, sorted_category_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:21:59.178620Z","iopub.execute_input":"2024-12-19T15:21:59.178904Z","iopub.status.idle":"2024-12-19T15:21:59.184207Z","shell.execute_reply.started":"2024-12-19T15:21:59.178877Z","shell.execute_reply":"2024-12-19T15:21:59.183226Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"num_regioni, category_counts = analyze_regions(actproposals_json)\nprint(f\"Numero di regioni: {num_regioni}\")\nprint(\"Occorrenze dei category_id:\", category_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:21:59.185382Z","iopub.execute_input":"2024-12-19T15:21:59.185749Z","iopub.status.idle":"2024-12-19T15:21:59.315552Z","shell.execute_reply.started":"2024-12-19T15:21:59.185713Z","shell.execute_reply":"2024-12-19T15:21:59.314593Z"}},"outputs":[{"name":"stdout","text":"Numero di regioni: 31148\nOccorrenze dei category_id: {1: 7622, 2: 4787, 3: 803, 4: 1607, 5: 713, 6: 13797, 7: 27, 8: 310, 9: 1215, 10: 54, 11: 213}\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"activeregion-xview-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"active_regions.json\",\n    \"proposals\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:21:59.317419Z","iopub.execute_input":"2024-12-19T15:21:59.317784Z","iopub.status.idle":"2024-12-19T15:22:13.129883Z","shell.execute_reply.started":"2024-12-19T15:21:59.317745Z","shell.execute_reply":"2024-12-19T15:22:13.128953Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"# Carica il dataset dal JSON\n\ndata = load_json(actproposals_json)\n'''\ndata = load_json(in_actproposals_json)\n'''\n\n# Converti in DataFrame per una gestione più comoda\ndf = pd.DataFrame(data)\n\n# Estrai il nome del file dal campo 'saved_path'\ndf[\"file_name\"] = df[\"saved_path\"].apply(lambda x: os.path.basename(x))\n\n# Aggiungi il percorso base al campo 'saved_path'\ndf[\"saved_path\"] = df[\"file_name\"].apply(lambda x: str(act_reg_folder / x))\n\n# Estrai i dati e le etichette\nX = df.index  # Indici delle righe\ny = df[\"category_id\"]  # Etichetta per stratificazione\n\n# Step 1: Train + Val/Test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Step 2: Val/Test split\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\n# Creazione dei dataset finali\ntrain_data = df.loc[X_train]\nval_data = df.loc[X_val]\ntest_data = df.loc[X_test]\n\n# Salva i dataset in nuovi file JSON\ntrain_data.to_json(\"train.json\", orient=\"records\", lines=False)\nval_data.to_json(\"val.json\", orient=\"records\", lines=False)\ntest_data.to_json(\"test.json\", orient=\"records\", lines=False)\n\nprint(\"Splitting completato. File salvati: train.json, val.json, test.json.\")\n\n# Percentuale dei dati per ciascun set\ntotal_data = len(df)\nprint(\"\\nPercentuale dei dati per ciascun set:\")\nprint(f\"Train: {len(train_data) / total_data:.2%}\")\nprint(f\"Validation: {len(val_data) / total_data:.2%}\")\nprint(f\"Test: {len(test_data) / total_data:.2%}\")\n\n# Distribuzione delle classi per ciascun set\nprint(\"\\nDistribuzione delle classi:\")\ntrain_class_dist = train_data[\"category_id\"].value_counts(normalize=True) * 100\nval_class_dist = val_data[\"category_id\"].value_counts(normalize=True) * 100\ntest_class_dist = test_data[\"category_id\"].value_counts(normalize=True) * 100\n\nprint(\"Train set:\")\nprint(train_class_dist.sort_index())\n\nprint(\"\\nValidation set:\")\nprint(val_class_dist.sort_index())\n\nprint(\"\\nTest set:\")\nprint(test_class_dist.sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:15.624367Z","iopub.execute_input":"2024-12-19T15:37:15.624951Z","iopub.status.idle":"2024-12-19T15:37:16.084053Z","shell.execute_reply.started":"2024-12-19T15:37:15.624917Z","shell.execute_reply":"2024-12-19T15:37:16.083261Z"}},"outputs":[{"name":"stdout","text":"Splitting completato. File salvati: train.json, val.json, test.json.\n\nPercentuale dei dati per ciascun set:\nTrain: 80.00%\nValidation: 10.00%\nTest: 10.00%\n\nDistribuzione delle classi:\nTrain set:\ncategory_id\n1     24.472269\n2     15.370415\n3      2.576451\n4      5.160928\n5      2.287503\n6     44.293282\n7      0.088290\n8      0.995264\n9      3.900795\n10     0.172566\n11     0.682238\nName: proportion, dtype: float64\n\nValidation set:\ncategory_id\n1     24.462279\n2     15.377207\n3      2.600321\n4      5.136437\n5      2.279294\n6     44.301766\n7      0.064205\n8      0.995185\n9      3.916533\n10     0.160514\n11     0.706260\nName: proportion, dtype: float64\n\nTest set:\ncategory_id\n1     24.462279\n2     15.345104\n3      2.568218\n4      5.168539\n5      2.311396\n6     44.301766\n7      0.096308\n8      0.995185\n9      3.884430\n10     0.192616\n11     0.674157\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        # Carica il file JSON\n        self.data = load_json(json_file)\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor()                  # Converte l'immagine in un tensore\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:16.085676Z","iopub.execute_input":"2024-12-19T15:37:16.086037Z","iopub.status.idle":"2024-12-19T15:37:16.093111Z","shell.execute_reply.started":"2024-12-19T15:37:16.085998Z","shell.execute_reply":"2024-12-19T15:37:16.092269Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"test_ds = CustomDataset(test_path)\ntrain_ds = CustomDataset(train_path)\nval_ds = CustomDataset(val_path)\n\nTrainLoader = DataLoader(train_ds, batch_size=64, shuffle=True)\nValLoader = DataLoader(val_ds, batch_size=64, shuffle=False)\nTestLoader = DataLoader(test_ds, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:16.094173Z","iopub.execute_input":"2024-12-19T15:37:16.094450Z","iopub.status.idle":"2024-12-19T15:37:16.217490Z","shell.execute_reply.started":"2024-12-19T15:37:16.094421Z","shell.execute_reply":"2024-12-19T15:37:16.216423Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)  \n        final = self.fn10(feature)\n\n        return feature, final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:16.220002Z","iopub.execute_input":"2024-12-19T15:37:16.220680Z","iopub.status.idle":"2024-12-19T15:37:16.229223Z","shell.execute_reply.started":"2024-12-19T15:37:16.220636Z","shell.execute_reply":"2024-12-19T15:37:16.228202Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class AlexNetFeatureExtractor(nn.Module):\n    def __init__(self, num_classes):\n        super(AlexNetFeatureExtractor, self).__init__()\n        self.model = torchvision.models.alexnet(pretrained=True)\n        \n        # Sostituisci l'ultimo layer per il numero di classi\n        self.model.classifier[6] = torch.nn.Linear(4096, num_classes)\n    \n    def forward(self, x):\n        # Estrai le feature dalla parte convoluzionale\n        x = self.model.features(x)\n        \n        # Pooling adattivo per ottenere una dimensione uniforme prima dei fully connected layers\n        x = self.model.avgpool(x)\n        \n        # Appiattisci il tensore per passarlo nei fully connected layer\n        x = torch.flatten(x, 1)  # Appiattimento corretto: dimensione (batch_size, 256*6*6)\n\n        # Estrai le feature prima della classificazione\n        feature = self.model.classifier[0](x)  # Primo layer del classificatore (4096)\n        \n        # Estrai la previsione finale passando attraverso tutti i layer del classificatore\n        final_output = self.model.classifier(x)  # Passa attraverso l'intero classificatore\n\n        return feature, final_output\n        \n        # Pooling adattivo per ottenere una dimensione uniforme prima dei fully connected layers\n        x = self.model.avgpool(x)\n        \n        # Appiattisci il tensore per passarlo nei fully connected layer\n        x = torch.flatten(x, 1)  # Appiattimento corretto: dimensione (batch_size, 256*6*6)\n\n        # Estrai le feature prima della classificazione\n        feature = self.model.classifier[0](x)  # Primo layer del classificatore (4096)\n        \n        # Estrai la previsione finale\n        final_output = self.model.classifier[6](x)  # Ultimo layer (finale)\n\n        return feature, final_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:16.230342Z","iopub.execute_input":"2024-12-19T15:37:16.230565Z","iopub.status.idle":"2024-12-19T15:37:16.241397Z","shell.execute_reply.started":"2024-12-19T15:37:16.230542Z","shell.execute_reply":"2024-12-19T15:37:16.240581Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"num_classes = 12 #11 classi + sfondo\nnet = AlexNet(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:16.242375Z","iopub.execute_input":"2024-12-19T15:37:16.242620Z","iopub.status.idle":"2024-12-19T15:37:16.605905Z","shell.execute_reply.started":"2024-12-19T15:37:16.242596Z","shell.execute_reply":"2024-12-19T15:37:16.605238Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:16.606770Z","iopub.execute_input":"2024-12-19T15:37:16.607017Z","iopub.status.idle":"2024-12-19T15:37:16.611605Z","shell.execute_reply.started":"2024-12-19T15:37:16.606993Z","shell.execute_reply":"2024-12-19T15:37:16.610771Z"}},"outputs":[{"name":"stdout","text":"AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (drop8): Dropout(p=0.5, inplace=False)\n  (fn8): Linear(in_features=9216, out_features=4096, bias=True)\n  (active8): ReLU(inplace=True)\n  (drop9): Dropout(p=0.5, inplace=False)\n  (fn9): Linear(in_features=4096, out_features=4096, bias=True)\n  (active9): ReLU(inplace=True)\n  (fn10): Linear(in_features=4096, out_features=12, bias=True)\n)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Modello pre-addestrato con le modifiche necessarie\nmodel = AlexNetFeatureExtractor(num_classes=num_classes)\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:16.612603Z","iopub.execute_input":"2024-12-19T15:37:16.612900Z","iopub.status.idle":"2024-12-19T15:37:18.389015Z","shell.execute_reply.started":"2024-12-19T15:37:16.612876Z","shell.execute_reply":"2024-12-19T15:37:18.388130Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 229MB/s] \n","output_type":"stream"},{"name":"stdout","text":"AlexNetFeatureExtractor(\n  (model): AlexNet(\n    (features): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n      (1): ReLU(inplace=True)\n      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (4): ReLU(inplace=True)\n      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): ReLU(inplace=True)\n      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): ReLU(inplace=True)\n      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n    (classifier): Sequential(\n      (0): Dropout(p=0.5, inplace=False)\n      (1): Linear(in_features=9216, out_features=4096, bias=True)\n      (2): ReLU(inplace=True)\n      (3): Dropout(p=0.5, inplace=False)\n      (4): Linear(in_features=4096, out_features=4096, bias=True)\n      (5): ReLU(inplace=True)\n      (6): Linear(in_features=4096, out_features=12, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"def plot_loss(train_losses, val_losses):\n    \"\"\"\n    Funzione per fare il plot della funzione di loss durante il training e la validazione.\n\n    :param train_losses: Lista delle perdite durante il training.\n    :param val_losses: Lista delle perdite durante la validazione.\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle='-', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\", linestyle='-', marker='x')\n    \n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:18.390137Z","iopub.execute_input":"2024-12-19T15:37:18.390430Z","iopub.status.idle":"2024-12-19T15:37:18.396301Z","shell.execute_reply.started":"2024-12-19T15:37:18.390404Z","shell.execute_reply":"2024-12-19T15:37:18.395386Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def train_model(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss):\n    \"\"\"\n    Funzione per addestrare il modello con pesi per classi sbilanciate.\n\n    :param net: Modello da addestrare.\n    :param train_loader: DataLoader per il training set.\n    :param val_loader: DataLoader per il validation set.\n    :param criterion: Funzione di loss.\n    :param optimizer: Ottimizzatore.\n    :param device: Dispositivo (CPU o GPU).\n    :param epochs: Numero di epoche di training.\n    :param path_min_loss: Percorso per salvare il modello con la minore loss di validazione.\n    \"\"\"\n    min_val_loss = float('inf')\n    \n    # Liste per registrare le perdite durante il training e la validazione\n    train_losses = []\n    val_losses = []\n    \n    # Dizionario per salvare le feature\n    train_features = {}  # Chiave: epoch, Valore: lista di feature\n\n    # Aggiungi un learning rate scheduler per ridurre il learning rate quando la loss di validazione non migliora\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n\n    for epoch in range(epochs):\n        net.train()  # Modalità training\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        epoch_features = []  # Per registrare le feature di ogni batch\n\n        # Barra di avanzamento per il training\n        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n\n        for images, labels in train_progress:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            features, outputs = net(images)\n\n            # Salva le feature per il batch corrente\n            epoch_features.append(features.detach().cpu().numpy())\n\n            # Calcolo della loss pesata\n            loss = criterion(outputs, labels)\n\n            # Backward pass e aggiornamento pesi\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Statistiche\n            train_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_train += labels.size(0)\n            correct_train += predicted.eq(labels).sum().item()\n\n            # Aggiorna la barra di avanzamento con la loss corrente\n            train_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_train / total_train)\n\n        # Salva le feature per l'epoca corrente\n        train_features[epoch] = epoch_features\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n        train_accuracy = 100. * correct_train / total_train\n        train_losses.append(avg_train_loss)\n\n        # Validazione\n        net.eval()  # Modalità validazione\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        # Barra di avanzamento per la validazione\n        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", leave=False)\n\n        with torch.no_grad():\n            for images, labels in val_progress:\n                images, labels = images.to(device), labels.to(device)\n\n                # Forward pass\n                _, outputs = net(images)\n                \n                # Calcolo della loss\n                loss = criterion(outputs, labels)\n\n                # Statistiche\n                val_loss += loss.item() * images.size(0)\n                _, predicted = outputs.max(1)\n                total_val += labels.size(0)\n                correct_val += predicted.eq(labels).sum().item()\n\n                # Aggiorna la barra di avanzamento con la loss e accuracy\n                val_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_val / total_val)\n\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = 100. * correct_val / total_val\n        val_losses.append(avg_val_loss)\n\n        # Salva il modello con la loss di validazione più bassa\n        if avg_val_loss < min_val_loss:\n            print(f\"Salvataggio del miglior modello: Val Loss migliorata da {min_val_loss:.4f} a {avg_val_loss:.4f}\")\n            min_val_loss = avg_val_loss\n            torch.save(net.state_dict(), path_min_loss)\n\n        # Aggiorna il learning rate in base alla loss di validazione\n        scheduler.step(avg_val_loss)\n\n        # Stampa statistiche per epoca\n        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    print(\"Training completato!\")\n\n    # Chiamata alla funzione per tracciare il grafico\n    plot_loss(train_losses, val_losses)\n\n    return train_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:18.399038Z","iopub.execute_input":"2024-12-19T15:37:18.399357Z","iopub.status.idle":"2024-12-19T15:37:18.416342Z","shell.execute_reply.started":"2024-12-19T15:37:18.399322Z","shell.execute_reply":"2024-12-19T15:37:18.415347Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss, json_path, original_img_path, reference_json_path):\n    \"\"\"\n    Funzione per testare il modello e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param original_img_path: Percorso al folder delle immagini originali.\n    :param reference_json_path: Percorso al file JSON contenente le informazioni delle immagini originali.\n    \"\"\"\n    # Carica il miglior modello salvato (con il parametro weights_only=True per evitare il warning)\n    net.load_state_dict(torch.load(path_min_loss, weights_only=True))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    # Carica i file JSON\n    with open(json_path, 'r') as f:\n        test_data = json.load(f)\n\n    with open(reference_json_path, 'r') as f:\n        reference_data = json.load(f)\n\n    # Crea un dizionario per una ricerca rapida delle immagini originali\n    id_to_filename = {img['id']: img['file_name'] for img in reference_data['images']}\n\n    # Crea un dizionario per raggruppare i bounding box per image_id\n    image_id_to_bboxes = {}\n    for image_info in test_data:\n        image_id = image_info['image_id']\n        region_bbox = image_info['region_bbox']\n        if image_id not in image_id_to_bboxes:\n            image_id_to_bboxes[image_id] = []\n        image_id_to_bboxes[image_id].append(region_bbox)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n            # Mostra alcuni esempi\n            if idx < 5:  # Mostra i primi 5 batch\n                for i in range(min(len(images), 3)):  # Mostra fino a 3 immagini per batch\n                    image_info = test_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                    image_id = image_info['image_id']\n\n                    # Trova il file_name usando l'image_id\n                    file_name = id_to_filename.get(image_id)\n                    if not file_name:\n                        print(f\"Immagine con ID {image_id} non trovata nel reference JSON.\")\n                        continue\n\n                    # Percorso dell'immagine originale\n                    img_path = os.path.join(original_img_path, file_name)\n\n                    # Carica l'immagine originale\n                    img = cv2.imread(img_path)\n                    if img is None:\n                        print(f\"Immagine non trovata: {img_path}\")\n                        continue\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n                    # Disegna tutti i bounding box per quell'immagine\n                    bboxes = image_id_to_bboxes.get(image_id, [])\n                    for j, bbox in enumerate(bboxes):\n                        x, y, w, h = bbox\n                        # Disegna il bounding box\n                        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)\n\n                        # Aggiungi la label predetta per quel bounding box (testo più piccolo)\n                        pred_label = predicted[i].item()  # Associa la predizione della classe per ogni immagine nel batch\n                        cv2.putText(img, f'{pred_label}', (x + 5, y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n\n                    # Mostra l'immagine con i bounding box e le rispettive label predette\n                    plt.figure(figsize=(6, 6))\n                    plt.imshow(img)\n                    plt.axis('off')\n                    plt.show()\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:18.417666Z","iopub.execute_input":"2024-12-19T15:37:18.417935Z","iopub.status.idle":"2024-12-19T15:37:18.435675Z","shell.execute_reply.started":"2024-12-19T15:37:18.417908Z","shell.execute_reply":"2024-12-19T15:37:18.434872Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\nepochs = 150\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n#net = AlexNet(num_classes)\n#device = torch.device(\"cuda\")\n#net = net.to(device)\n\n# Modello pre-addestrato con le modifiche necessarie\nmodel = AlexNetFeatureExtractor(num_classes=num_classes)\n\n# Passa il modello su GPU o CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ncriterion = criterion.to(device)\n\npath_min_loss = '/kaggle/working/AlexNet.pth'\n\ntrain_features = train_model(\n    net=model,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:18.436874Z","iopub.execute_input":"2024-12-19T15:37:18.437240Z","iopub.status.idle":"2024-12-19T15:37:19.952458Z","shell.execute_reply.started":"2024-12-19T15:37:18.437210Z","shell.execute_reply":"2024-12-19T15:37:19.951050Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n                                                               \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m criterion \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m path_min_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/AlexNet.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 20\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrainLoader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mValLoader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_min_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_min_loss\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[38], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Barra di avanzamento per il training\u001b[39;00m\n\u001b[1;32m     34\u001b[0m train_progress \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_progress:\n\u001b[1;32m     37\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[30], line 28\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Carica l'immagine\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Etichetta della categoria\u001b[39;00m\n\u001b[1;32m     31\u001b[0m label \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Categoria della proposta\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/activeregion-xviewdataset/proposals/image_028439.jpg'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/activeregion-xviewdataset/proposals/image_028439.jpg'","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"test_model(\n    net=net,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss,\n    json_path = test_path,\n    original_img_path = img_fldr,\n    reference_json_path = in_new_coco_json_pth\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:19.953528Z","iopub.status.idle":"2024-12-19T15:37:19.953994Z","shell.execute_reply.started":"2024-12-19T15:37:19.953761Z","shell.execute_reply":"2024-12-19T15:37:19.953788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, class_names):\n    \"\"\"\n    Funzione per calcolare e visualizzare la matrice di confusione.\n\n    :param y_true: Etichette reali\n    :param y_pred: Etichette predette dal modello\n    :param class_names: Nomi delle classi\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted labels')\n    plt.ylabel('True labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n# Durante la fase di validazione o test, calcola la matrice di confusione\ndef validate_model(net, val_loader, device, class_names):\n    net.eval()  # Modalità di valutazione\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = net(images)\n            \n            # Predizioni\n            _, predicted = outputs.max(1)\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n    \n    # Visualizza la matrice di confusione\n    plot_confusion_matrix(all_labels, all_preds, class_names)\n\n# Esempio di come chiamare la funzione\nclass_names = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3', 'Classe 4', 'Classe 5', 'Classe 6', 'Classe 7', 'Classe 8', 'Classe 9', 'Classe 10', 'Classe 11']\nvalidate_model(net, ValLoader, device, class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:19.955060Z","iopub.status.idle":"2024-12-19T15:37:19.955382Z","shell.execute_reply.started":"2024-12-19T15:37:19.955217Z","shell.execute_reply":"2024-12-19T15:37:19.955233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Box Regressor (TODO)","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_iou(boxA, boxB):\n    # Funzione per calcolare l'IoU\n    x1A, y1A = boxA[0] - boxA[2] / 2, boxA[1] - boxA[3] / 2\n    x2A, y2A = boxA[0] + boxA[2] / 2, boxA[1] + boxA[3] / 2\n    x1B, y1B = boxB[0] - boxB[2] / 2, boxB[1] - boxB[3] / 2\n    x2B, y2B = boxB[0] + boxB[2] / 2, boxB[1] + boxB[3] / 2\n\n    x1 = max(x1A, x1B)\n    y1 = max(y1A, y1B)\n    x2 = min(x2A, x2B)\n    y2 = min(y2A, y2B)\n\n    inter_width = max(0, x2 - x1)\n    inter_height = max(0, y2 - y1)\n    inter_area = inter_width * inter_height\n\n    areaA = (x2A - x1A) * (y2A - y1A)\n    areaB = (x2B - x1B) * (y2B - y1B)\n\n    iou = inter_area / (areaA + areaB - inter_area)\n    return iou\n\nclass BoundingBoxRegressor:\n    def __init__(self, lambda_reg=1000, iou_threshold=0.5):\n        self.lambda_reg = lambda_reg\n        self.iou_threshold = iou_threshold\n        self.weights = {}\n\n    def assign_pairs(self, proposals, ground_truths):\n        pairs = []\n        for P in proposals:\n            best_iou = 0\n            best_G = None\n            for G in ground_truths:\n                iou = compute_iou(P, G)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_G = G\n            if best_iou >= self.iou_threshold:\n                pairs.append((P, best_G))\n        return pairs\n\n    def train(self, features, proposals, ground_truths):\n        pairs = self.assign_pairs(proposals, ground_truths)\n        if not pairs:\n            raise ValueError(\"Nessuna proposta supera la soglia IoU!\")\n\n        N, D = len(pairs), features.shape[1]\n        valid_features = []\n        targets = np.zeros((N, 4))  # tx, ty, tw, th\n        for i, (P, G) in enumerate(pairs):\n            Px, Py, Pw, Ph = P\n            Gx, Gy, Gw, Gh = G\n            targets[i, 0] = (Gx - Px) / Pw  # tx\n            targets[i, 1] = (Gy - Py) / Ph  # ty\n            targets[i, 2] = np.log(Gw / Pw)  # tw\n            targets[i, 3] = np.log(Gh / Ph)  # th\n            valid_features.append(features[np.where((proposals == P).all(axis=1))[0][0]])\n        valid_features = np.array(valid_features)\n\n        for k, label in enumerate(['x', 'y', 'w', 'h']):\n            target_k = targets[:, k]\n            A = valid_features.T @ valid_features + self.lambda_reg * np.eye(D)\n            b = valid_features.T @ target_k\n            self.weights[label] = np.linalg.solve(A, b)\n\n    def predict(self, features, proposals):\n        N = proposals.shape[0]\n        predictions = np.zeros((N, 4))\n        for i in range(N):\n            Px, Py, Pw, Ph = proposals[i]\n            dx = features[i] @ self.weights['x']\n            dy = features[i] @ self.weights['y']\n            dw = features[i] @ self.weights['w']\n            dh = features[i] @ self.weights['h']\n            Gx_hat = Pw * dx + Px\n            Gy_hat = Ph * dy + Py\n            Gw_hat = Pw * np.exp(dw)\n            Gh_hat = Ph * np.exp(dh)\n            predictions[i] = [Gx_hat, Gy_hat, Gw_hat, Gh_hat]\n        return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:19.957026Z","iopub.status.idle":"2024-12-19T15:37:19.957347Z","shell.execute_reply.started":"2024-12-19T15:37:19.957182Z","shell.execute_reply":"2024-12-19T15:37:19.957196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inizializza il modello\nregressor = BoundingBoxRegressor(lambda_reg=1000, iou_threshold=0.5)\n \n# Addestramento\nregressor.train(features, proposals, ground_truths)\n \n# Predizione\npredictions = regressor.predict(features, proposals)\nprint(\"Bounding box predette:\\n\", predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:19.958483Z","iopub.status.idle":"2024-12-19T15:37:19.958897Z","shell.execute_reply.started":"2024-12-19T15:37:19.958681Z","shell.execute_reply":"2024-12-19T15:37:19.958702Z"}},"outputs":[],"execution_count":null}]}