{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"DataLoader","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"DataLoader Legacy","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n\n        self.img_dir = img_dir\n\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n\n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox_str = annotation['bbox']\n            bbox = list(map(float, bbox_str.strip('[]').split(', ')))\n\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug = aug    \n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n    \n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        \n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Ridimensiona i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        categories = [c for c in categories if isinstance(c, int)]\n        if not categories:\n            categories = [-1]  # Etichetta speciale per immagini senza annotazioni\n        \n        scale_x = 320 / original_width\n        scale_y = 320 / original_height\n        scaled_bboxes = [\n            torch.tensor([\n                bbox[0] * scale_x,  # x_min\n                bbox[1] * scale_y,  # y_min\n                bbox[2] * scale_x,  # x_max\n                bbox[3] * scale_y   # y_max\n            ], dtype=torch.float32)\n            for bbox in bboxes\n        ] if bboxes else [torch.zeros(4, dtype=torch.float32)]\n        \n        labels = torch.tensor(categories, dtype=torch.int64)\n        proposals_tensor = self._generate_region_proposals(image)\n        \n        # Chiama match_proposals_with_labels per associare le regioni con le etichette\n        matched_proposals, matched_labels = self.match_proposals_with_labels(proposals_tensor, scaled_bboxes, labels)\n        \n        # Elabora le proposte abbinate\n        processed_proposals = self._process_proposals(image_tensor, matched_proposals)\n        \n        return {\n            \"image\": image_tensor,\n            \"labels\": matched_labels,  # Etichette abbinate alle regioni\n            \"bboxes\": scaled_bboxes,\n            \"regions\": processed_proposals  # Lista di tensori delle region proposals\n        }\n\n    def match_proposals_with_labels(self, regions, bboxes, labels, iou_threshold=0.5):\n        matched_features = []\n        matched_labels = []\n\n        for region in regions:\n            # Calcola l'IoU per ciascun bbox\n            ious = self.calculate_iou(region, bboxes)\n            max_iou = ious.max()\n            # MANTENERE PROPORZIONE 1/3 CON \n            if max_iou > iou_threshold:\n                # Se IoU > threshold, associa l'etichetta del bbox con massimo IoU\n                matched_features.append(region)  # Region proposal\n                matched_labels.append(labels[ious.argmax()])  # Etichetta corrispondente\n            else:\n                # Se nessun bbox corrisponde, classifica come background\n                matched_features.append(region)\n                matched_labels.append(0)  # 0 indica \"background\"\n\n        return matched_features, matched_labels\n\n    def calculate_iou(self, region, bboxes):\n        # Calcola l'Intersection over Union (IoU) tra la regione proposta e i bounding boxes\n        x_min, y_min, x_max, y_max = region\n        proposal_area = (x_max - x_min) * (y_max - y_min)\n\n        ious = []\n        for bbox in bboxes:\n            bx_min, by_min, bx_max, by_max = bbox\n            inter_x_min = max(x_min, bx_min)\n            inter_y_min = max(y_min, by_min)\n            inter_x_max = min(x_max, bx_max)\n            inter_y_max = min(y_max, by_max)\n            \n            # Calcola l'area dell'intersezione\n            inter_width = max(0, inter_x_max - inter_x_min)\n            inter_height = max(0, inter_y_max - inter_y_min)\n            inter_area = inter_width * inter_height\n            \n            union_area = proposal_area + (bx_max - bx_min) * (by_max - by_min) - inter_area\n            ious.append(inter_area / union_area if union_area > 0 else 0)\n        \n        return torch.tensor(ious)\n\n    def _generate_region_proposals(self, image):\n        img_np = np.array(image)\n        \n        if len(img_np.shape) == 3 and img_np.shape[0] == 3:\n            img_np = np.transpose(img_np, (1, 2, 0))  # Da [C, H, W] a [H, W, C]\n        elif len(img_np.shape) == 2:\n            img_np = np.stack([img_np] * 3, axis=-1)  # Da [H, W] a [H, W, 3]\n        elif img_np.shape[2] < 3:\n            img_np = np.repeat(img_np, 3, axis=2)  # Da [H, W, 1] a [H, W, 3]\n        elif img_np.shape[2] != 3:\n            raise ValueError(f\"L'immagine ha una forma non valida: {img_np.shape}\")\n        \n        _, regions = selectivesearch.selective_search(img_np, scale=500, sigma=0.9, min_size=10)\n        if len(regions) == 0:\n            print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n        \n        proposals = []\n        for region in regions:\n            x, y, w, h = region['rect']\n            if w > 0 and h > 0 and w >= 10 and h >= 10:\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0])\n                proposals.append([x, y, x_max, y_max])\n        \n        filtered_proposals = self._filter_proposals(proposals, img_np.shape[1], img_np.shape[0])\n        \n        return filtered_proposals  # Restituisce una lista di coordinate (non ancora tensorizzate)\n\n    def _filter_proposals(self, proposals, img_width, img_height, min_area=100, max_area_ratio=0.8):\n        unique_proposals = set(tuple(p) for p in proposals)\n        filtered = []\n        for x_min, y_min, x_max, y_max in unique_proposals:\n            width = x_max - x_min\n            height = y_max - y_min\n            area = width * height\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\n                filtered.append((x_min, y_min, x_max, y_max))\n        return filtered\n\n    def _process_proposals(self, image_tensor, proposals, output_size=(227, 227)):\n        processed_proposals = []\n        for proposal in proposals:\n            try:\n                _, H, W = image_tensor.shape\n                x_min, y_min, x_max, y_max = map(int, proposal)\n                x_min, y_min = max(0, x_min), max(0, y_min)\n                x_max, y_max = min(W, x_max), min(H, y_max)\n    \n                # Controlla se la proposal Ã¨ valida\n                if x_min < x_max and y_min < y_max:\n                    cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]  # Ritaglio\n                    \n                    # Controlla che il ritaglio non sia vuoto\n                    if cropped_region.numel() == 0:\n                        print(f\"Ritaglio vuoto per proposal: {proposal}. Salto.\")\n                        continue\n                    \n                    # Controlla che il tensor sia 3D (C, H, W)\n                    if cropped_region.ndim != 3:\n                        print(f\"Proposal non valida per il ridimensionamento: {proposal}. Salto.\")\n                        continue\n                    \n                    resized_region = torch.nn.functional.interpolate(\n                        cropped_region.unsqueeze(0), size=output_size, mode='bilinear', align_corners=False\n                    ).squeeze(0)  # Ridimensiona\n                    \n                    processed_proposals.append(resized_region)\n            except Exception as e:\n                print(f\"Errore durante il processamento della proposal: {proposal}. Errore: {e}\")\n    \n        return processed_proposals  # Lista di tensori delle region proposals","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    images = []\n    labels = []\n    bboxes = []\n    regions = []\n\n    for sample in batch:\n        images.append(sample['image'])  # Le immagini sono giÃ  tensori\n        labels.append(torch.tensor(sample['labels'], dtype=torch.int64))  # Convertiamo in tensore\n        bboxes.append(sample['bboxes'])  # Bboxes potrebbero essere giÃ  tensori\n        regions.append(sample['regions'])  # Le region proposals potrebbero essere giÃ  tensori\n\n    # Stacking delle immagini e concatenamento delle etichette\n    images = torch.stack(images, dim=0)\n    labels = torch.cat(labels, dim=0)  # Ora labels sono tensori, possiamo concatenarle\n    bboxes = [torch.stack(b, dim=0) if len(b) > 0 else torch.zeros(1, 4) for b in bboxes]  # Gestione delle bounding boxes\n    regions = [torch.stack(r, dim=0) if len(r) > 0 else torch.zeros(1, 4) for r in regions]  # Gestione delle region proposals\n\n    return {\n        'image': images,\n        'labels': labels,\n        'bboxes': bboxes,\n        'regions': regions\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=True)\nvalid_dataset = CustomDataset(val_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \ntest_dataset = CustomDataset(test_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_txt_vs_json(txt_paths, coco_data):\n    \"\"\"\n    Controlla se le immagini del JSON sono presenti in almeno uno dei file TXT.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n        coco_data (dict): Dati in formato COCO.\n    \"\"\"\n    # Estrai i nomi delle immagini dal JSON\n    image_names = [image['file_name'] for image in coco_data['images']]\n    \n    # Inizializza un set per contenere tutte le immagini presenti nei TXT\n    txt_image_names = set()\n    \n    # Leggi i nomi delle immagini da ciascun file TXT\n    for txt_path in txt_paths:\n        with open(txt_path, 'r') as f:\n            txt_image_names.update(os.path.basename(line.strip()) for line in f.readlines())\n    \n    # Trova le immagini presenti nel JSON ma non in nessuno dei TXT\n    missing_in_txts = [name for name in image_names if name not in txt_image_names]\n    \n    # Verifica e stampa i risultati\n    print(\"\\nControllo completato:\")\n    if missing_in_txts:\n        print(f\"Errore: le seguenti immagini non sono presenti in nessuno dei file TXT forniti:\\n{missing_in_txts}\")\n    else:\n        print(\"Tutte le immagini del JSON sono presenti in almeno uno dei file TXT.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta il numero di immagini nel JSON\nnum_images = len(coco_data['images'])\nprint(f\"Numero di immagini nel JSON: {num_images}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estrai i nomi delle immagini dal JSON\nimage_names = [image['file_name'] for image in coco_data['images']]\n\n# Stampa i primi 5 nomi delle immagini nel JSON\nprint(f\"Primi 5 nomi delle immagini nel JSON: {image_names[:5]}\")   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_lines_in_txt(txt_paths):\n    \"\"\"\n    Conta il numero di righe in ciascun file TXT dato il percorso e calcola la somma totale delle righe.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n    \"\"\"\n    total_lines = 0  # Variabile per accumulare il numero totale di righe\n    \n    for txt_path in txt_paths:\n        try:\n            # Apri il file e conta le righe\n            with open(txt_path, 'r') as f:\n                num_lines = sum(1 for line in f)\n            total_lines += num_lines  # Aggiungi il numero di righe del file al totale\n            print(f\"Numero di righe nel file {txt_path}: {num_lines}\")\n        except FileNotFoundError:\n            print(f\"Errore: il file {txt_path} non Ã¨ stato trovato.\")\n        except Exception as e:\n            print(f\"Errore nel leggere il file {txt_path}: {e}\")\n    \n    # Stampa la somma totale delle righe\n    print(f\"\\nSomma totale delle righe in tutti i file: {total_lines}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_paths = [train_txt_pth, val_txt_pth, test_txt_pth]\ncount_lines_in_txt(txt_paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_txt_vs_json(txt_paths, coco_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_dataloader(loader):\n    \"\"\"\n    Funzione per controllare il comportamento di un DataLoader.\n    Visualizza alcune immagini insieme alle loro annotazioni per verificare il corretto funzionamento.\n\n    Args:\n        loader (DataLoader): Il DataLoader da verificare.\n    \"\"\"\n    for batch_idx, batch in enumerate(loader):\n        print(f\"Batch {batch_idx + 1}:\")\n        \n        # Estrai i dati dal dizionario\n        images = batch[\"image\"]\n        labels = batch[\"labels\"]\n        bboxes = batch[\"bboxes\"]\n        regions = batch[\"regions\"]\n\n        # Stampa le shape e le dimensioni dei dati\n        print(f\"  Shape delle immagini (batch): {images.shape}\")\n        print(f\"  Numero di etichette nel batch: {len(labels)}\")\n\n        # Controlla se il batch Ã¨ vuoto\n        if images.size(0) == 0:\n            print(\"Batch vuoto. Procedo con il batch successivo.\")\n            continue\n\n        # Scegli un'immagine casuale dal batch\n        random_index = random.randint(0, images.size(0) - 1)\n        image = images[random_index].permute(1, 2, 0).numpy()  # Da [C, H, W] a [H, W, C]\n\n        # Visualizza l'immagine\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.imshow(image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Immagine nel batch {batch_idx + 1}, indice {random_index}\")\n\n        # Etichette\n        label = labels[random_index]\n        label_info = label.tolist() if isinstance(label, torch.Tensor) else label\n        print(f\"  Etichette: {label_info}\")\n\n        # Bounding boxes\n        bbox = bboxes[random_index]\n        for box in bbox:\n            if isinstance(box, torch.Tensor):\n                box = box.tolist()  # Converte il bounding box in lista\n    \n            # Converti da [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            x_min, y_min, width, height = box\n            x_max = x_min + width\n            y_max = y_min + height\n    \n            # Opzionale: Verifica dei limiti dell'immagine\n            H, W, _ = image.shape\n            x_min, y_min = max(0, x_min), max(0, y_min)\n            x_max, y_max = min(W, x_max), min(H, y_max)\n    \n            # Disegna il bounding box\n            rect = plt.Rectangle((x_min, y_min), width, height,\n                                  edgecolor='green', facecolor='none', linewidth=1.5)\n            ax.add_patch(rect)\n\n        # Visualizza alcune region proposals come immagini separate\n        region_proposals = regions[random_index]\n        print(f\"  Numero di region proposals: {len(region_proposals)}\")\n\n        fig, axs = plt.subplots(1, min(5, len(region_proposals)), figsize=(15, 5))\n        for i, proposal in enumerate(region_proposals[:5]):\n            proposal_image = proposal.permute(1, 2, 0).numpy()  # Da [C, H, W] a [H, W, C]\n            axs[i].imshow(proposal_image)\n            axs[i].axis(\"off\")\n            axs[i].set_title(f\"Region Proposal {i + 1}\")\n        plt.show()\n\n        break  # Mostra solo il primo batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui il controllo per i dataloader\ncheck_dataloader(train_loader)\ncheck_dataloader(val_loader)\ncheck_dataloader(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
