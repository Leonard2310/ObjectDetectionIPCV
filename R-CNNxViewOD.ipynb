{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":10161721,"sourceType":"datasetVersion","datasetId":6274890}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per l'ottimizzazione e la gestione delle dipendenze\n#import selectivesearch\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:14:58.778335Z","iopub.execute_input":"2024-12-12T08:14:58.778871Z","iopub.status.idle":"2024-12-12T08:15:05.871478Z","shell.execute_reply.started":"2024-12-12T08:14:58.778810Z","shell.execute_reply":"2024-12-12T08:15:05.869629Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nOUT_COCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n# PROPOSALS\nOUT_PROPOSALS_FLDR_NM = 'proposals'\nprop_fldr = Path(f'/kaggle/working/{OUT_PROPOSALS_FLDR_NM}')\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\nACTPROP_WEIG_COCO_JSON_NM ='active_regions_weights.json'\nactproposalsweights_json = out_dataset_pth / ACTPROP_WEIG_COCO_JSON_NM\n\n# ACTIVE REGIONS\nact_reg_path = Path('/kaggle/input/activeregion-xviewdataset')\nact_reg_folder = Path('/kaggle/input/activeregion-xviewdataset/activeregion-xview-dataset/proposals')\n\n#DATASET\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:15:05.873991Z","iopub.execute_input":"2024-12-12T08:15:05.874725Z","iopub.status.idle":"2024-12-12T08:15:05.890268Z","shell.execute_reply.started":"2024-12-12T08:15:05.874664Z","shell.execute_reply":"2024-12-12T08:15:05.888913Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)\nclean_output(prop_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:56:18.253797Z","iopub.execute_input":"2024-12-11T15:56:18.254236Z","iopub.status.idle":"2024-12-11T15:56:18.262173Z","shell.execute_reply.started":"2024-12-11T15:56:18.254199Z","shell.execute_reply":"2024-12-11T15:56:18.260914Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\nCartella /kaggle/working/proposals non trovata. Nessuna azione necessaria.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:58:16.325058Z","iopub.execute_input":"2024-12-10T12:58:16.326099Z","iopub.status.idle":"2024-12-10T12:58:16.330997Z","shell.execute_reply.started":"2024-12-10T12:58:16.326057Z","shell.execute_reply":"2024-12-10T12:58:16.329836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"#numerosità iniziale categorie\nfrom collections import defaultdict\n\n# Carica il file JSON\nwith open(coco_json_pth, \"r\") as f:\n    coco_data = json.load(f)\n\n# Estrarre i dati principali\nannotations = coco_data.get(\"annotations\", [])\ncategories = coco_data.get(\"categories\", [])\n\n# Mappare id di categoria ai nomi delle categorie\ncategory_id_to_name = {}\nfor category in categories:\n    for cat_id, cat_name in category.items():  # Estrarre l'unico elemento dal dizionario\n        category_id_to_name[int(cat_id)] = cat_name\n\n# Contare i bounding box per categoria\nbbox_counts = defaultdict(int)\n\nfor annotation in annotations:\n    category_id = annotation[\"category_id\"]\n    bbox_counts[category_id] += 1\n\n# Creare un elenco dei risultati\nresults = [\n    (cat_id, category_id_to_name[cat_id], count)\n    for cat_id, count in bbox_counts.items()\n]\n\n# Stampare i risultati\nfor cat_id, category_name, count in results:\n    print(f\"Categoria ID {cat_id} ('{category_name}'): {count} bounding box\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:50:17.611816Z","iopub.execute_input":"2024-12-10T09:50:17.612145Z","iopub.status.idle":"2024-12-10T09:50:18.961314Z","shell.execute_reply.started":"2024-12-10T09:50:17.612116Z","shell.execute_reply":"2024-12-10T09:50:18.960465Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalità RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:58:19.947749Z","iopub.execute_input":"2024-12-10T12:58:19.948156Z","iopub.status.idle":"2024-12-10T12:58:19.956854Z","shell.execute_reply.started":"2024-12-10T12:58:19.948121Z","shell.execute_reply":"2024-12-10T12:58:19.955861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.8, min_size=20)\n\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:58:22.820382Z","iopub.execute_input":"2024-12-10T12:58:22.820790Z","iopub.status.idle":"2024-12-10T12:58:22.827612Z","shell.execute_reply.started":"2024-12-10T12:58:22.820754Z","shell.execute_reply":"2024-12-10T12:58:22.826607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:58:27.151024Z","iopub.execute_input":"2024-12-10T12:58:27.151416Z","iopub.status.idle":"2024-12-10T12:58:27.156687Z","shell.execute_reply.started":"2024-12-10T12:58:27.151381Z","shell.execute_reply":"2024-12-10T12:58:27.155519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    with open(coco_json, 'r') as f:\n        coco_data = json.load(f)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:58:30.553109Z","iopub.execute_input":"2024-12-10T12:58:30.553512Z","iopub.status.idle":"2024-12-10T12:58:30.563272Z","shell.execute_reply.started":"2024-12-10T12:58:30.553476Z","shell.execute_reply":"2024-12-10T12:58:30.562024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_dataset_proposals(coco_json_pth, img_fldr, prop_fldr, proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:40:50.269667Z","iopub.execute_input":"2024-12-10T14:40:50.270080Z","iopub.status.idle":"2024-12-10T15:19:18.646075Z","shell.execute_reply.started":"2024-12-10T14:40:50.270046Z","shell.execute_reply":"2024-12-10T15:19:18.644313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Positive Region Proposals","metadata":{}},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:49:24.608869Z","iopub.execute_input":"2024-12-11T15:49:24.609310Z","iopub.status.idle":"2024-12-11T15:49:24.618427Z","shell.execute_reply.started":"2024-12-11T15:49:24.609271Z","shell.execute_reply":"2024-12-11T15:49:24.617247Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def calculate_bbox_areas(json_file_path, category_id):\n    \"\"\"\n    Calcola la media delle aree di tutti i bounding box con category_id specificato.\n\n    :param json_file_path: Percorso al file JSON contenente le annotazioni.\n    :param category_id: ID della categoria per cui calcolare le aree.\n    :return: Media delle aree dei bounding box della categoria specificata.\n    \"\"\"\n    # Carica il file JSON\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n\n    areas = []\n\n    # Itera sulle annotazioni\n    for annotation in data.get(\"annotations\", []):\n        if annotation[\"category_id\"] == category_id:\n            bbox = annotation[\"bbox\"]  # bbox formato [x, y, width, height]\n            # Converti la stringa JSON in una lista di numeri se necessario\n            if isinstance(bbox, str):\n                bbox = json.loads(bbox)\n            bbox = list(map(float, bbox))\n            width, height = bbox[2], bbox[3]\n            area = width * height\n            areas.append(area)\n\n    # Calcola e restituisci la media delle aree\n    if areas:\n        return sum(areas) / len(areas)\n    else:\n        return 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:46:56.192119Z","iopub.execute_input":"2024-12-11T15:46:56.192522Z","iopub.status.idle":"2024-12-11T15:46:56.200915Z","shell.execute_reply.started":"2024-12-11T15:46:56.192490Z","shell.execute_reply":"2024-12-11T15:46:56.199367Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"buildings_area = calculate_bbox_areas(coco_json_pth, 6)\ncars_area = calculate_bbox_areas(coco_json_pth, 1)\n\nprint(\"Area media buildings:\", buildings_area)\nprint(\"Area media cars:\", cars_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:48:46.520997Z","iopub.execute_input":"2024-12-11T15:48:46.521391Z","iopub.status.idle":"2024-12-11T15:48:51.564645Z","shell.execute_reply.started":"2024-12-11T15:48:46.521357Z","shell.execute_reply":"2024-12-11T15:48:51.563604Z"}},"outputs":[{"name":"stdout","text":"Area media buildings: 2670.4613258448567\nArea media cars: 387.11695350469574\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def get_adaptive_threshold(bbox):\n    \"\"\"Calcola un threshold IoU adattivo in base alla dimensione del bounding box.\"\"\"\n    # bbox è un tensore o una lista con [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]  # Calcolo larghezza\n    height = bbox[3] - bbox[1]  # Calcolo altezza\n    \n    # Adattiamo la soglia in base alle dimensioni del bbox\n    area = width * height\n    if area < 500:  # Bbox piccoli (ad esempio, cars)\n        return 0.25  # Soglia più bassa per bbox piccoli\n    else:  # Bbox grandi\n        return 0.5  # Soglia più alta per bbox grandi (grandi edifici)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:13:36.427978Z","iopub.execute_input":"2024-12-11T16:13:36.428439Z","iopub.status.idle":"2024-12-11T16:13:36.434962Z","shell.execute_reply.started":"2024-12-11T16:13:36.428403Z","shell.execute_reply":"2024-12-11T16:13:36.433452Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"'''\ndef assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path):\n    \"\"\"Associa le regioni proposte ai bounding boxes secondo le specifiche e salva un nuovo JSON in formato COCO.\"\"\"\n\n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n\n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        bbox = json.loads(annot[\"bbox\"])\n        x, y, w, h = bbox\n        bbox_converted = [x, y, x + w, y + h]\n        annotations_by_image[img_id].append((torch.tensor(bbox_converted, dtype=torch.float32), annot[\"category_id\"]))\n\n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n\n    active_region_data = []  # Lista per i dati delle regioni attive\n    iou_values = []\n\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Ignora tutte le regioni se non ci sono bounding boxes per l'immagine\n            continue\n\n        gt_bboxes = [item[0] for item in gt_data]\n        gt_categories = [item[1] for item in gt_data]\n\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1],\n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        if not iou_matrix:\n            continue\n\n        iou_matrix = torch.tensor(iou_matrix)\n        max_ious, indices = torch.max(iou_matrix, dim=1)\n\n        iou_values.extend(max_ious.tolist())\n        \n        for idx, iou in enumerate(max_ious):\n            if iou < 0.2:\n                # Classifica come sfondo\n                category_id = 11\n            elif iou > 0.5:\n                # Classifica con la categoria del bounding box corrispondente\n                category_id = gt_categories[indices[idx].item()]\n            else:\n                # Ignora la regione\n                continue\n\n            x_min, y_min, x_max, y_max = proposal_coords[idx].values()\n            width = x_max - x_min\n            height = y_max - y_min\n\n            # Salva la regione selezionata\n            active_region_data.append({\n                \"image_id\": image_id,\n                \"file_name\": file_name,\n                \"category_id\": category_id,\n                \"proposal_id\": idx,\n                \"region_bbox\": [x_min, y_min, width, height]\n            })\n\n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)\n\n    return iou_values\n'''\n\ndef assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path):\n    \"\"\"Associa le regioni proposte ai bounding boxes secondo le specifiche e salva un nuovo JSON in formato COCO.\"\"\"\n\n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n\n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        bbox = json.loads(annot[\"bbox\"])\n        x, y, w, h = bbox\n        bbox_converted = [x, y, x + w, y + h]\n        annotations_by_image[img_id].append((torch.tensor(bbox_converted, dtype=torch.float32), annot[\"category_id\"]))\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    active_region_data = []  # Lista per i dati delle regioni attive\n    iou_values = []\n\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            continue\n\n        gt_bboxes = [item[0] for item in gt_data]\n        gt_categories = [item[1] for item in gt_data]\n\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1],\n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        if not iou_matrix:\n            continue\n\n        iou_matrix = torch.tensor(iou_matrix)\n        max_ious, indices = torch.max(iou_matrix, dim=1)\n\n        iou_values.extend(max_ious.tolist())\n\n        for idx, iou in enumerate(max_ious):\n            # Calcolare il threshold IoU adattivo in base alla dimensione del bounding box\n            adaptive_threshold = get_adaptive_threshold(gt_bboxes[indices[idx].item()])\n        \n            if iou >= adaptive_threshold:\n                # La regione appartiene alla categoria corrispondente al massimo IoU\n                category_id = gt_categories[indices[idx].item()]\n            elif iou < 0.01:\n                # Classifica come sfondo\n                category_id = 11\n            else:\n                # Ignora la regione\n                continue\n\n            x_min, y_min, x_max, y_max = proposal_coords[idx].values()\n            width = x_max - x_min\n            height = y_max - y_min\n\n            # Salva la regione selezionata\n            active_region_data.append({\n                \"image_id\": image_id,\n                \"file_name\": file_name,\n                \"category_id\": category_id,\n                \"proposal_id\": idx,\n                \"region_bbox\": [x_min, y_min, width, height]\n            })\n\n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)\n\n    return iou_values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:13:37.958488Z","iopub.execute_input":"2024-12-11T16:13:37.958903Z","iopub.status.idle":"2024-12-11T16:13:37.980387Z","shell.execute_reply.started":"2024-12-11T16:13:37.958863Z","shell.execute_reply":"2024-12-11T16:13:37.978894Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def create_iou_histogram(iou_values, bins=50, range=(0, 1), output_path=None):\n    \"\"\"Crea un istogramma dei valori IoU.\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.hist(iou_values, bins=bins, range=range, color='blue', alpha=0.7)\n    plt.title('Distribuzione dei valori IoU')\n    plt.xlabel('IoU')\n    plt.ylabel('Frequenza')\n    \n    if output_path:\n        plt.savefig(output_path)\n        print(f\"Istogramma salvato in {output_path}\")\n    else:\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:50:26.074365Z","iopub.execute_input":"2024-12-11T15:50:26.074750Z","iopub.status.idle":"2024-12-11T15:50:26.081231Z","shell.execute_reply.started":"2024-12-11T15:50:26.074716Z","shell.execute_reply":"2024-12-11T15:50:26.080144Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#assign_and_save_regions(proposals_json, coco_json_pth, img_fldr, prop_fldr, actproposals_json)\n#assign_and_save_regions('/kaggle/input/regions/proposals.json', coco_json_pth, img_fldr,'/kaggle/input/regions/proposals', actproposals_json)\n\n# Esegui l'assegnazione e ottieni i valori IoU\niou_values = assign_and_save_regions('/kaggle/input/regions/proposals.json', coco_json_pth, img_fldr,'/kaggle/input/regions/proposals', actproposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:13:45.236301Z","iopub.execute_input":"2024-12-11T16:13:45.236664Z","iopub.status.idle":"2024-12-11T16:22:33.198219Z","shell.execute_reply.started":"2024-12-11T16:13:45.236633Z","shell.execute_reply":"2024-12-11T16:22:33.196973Z"}},"outputs":[{"name":"stderr","text":"Elaborazione immagini: 100%|██████████| 32199/32199 [08:28<00:00, 63.27it/s] \n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Crea l'istogramma dei valori IoU\ncreate_iou_histogram(iou_values, bins=20, range=(0, 1), output_path=\"/kaggle/working/iou_histogram.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:09:36.478972Z","iopub.execute_input":"2024-12-11T16:09:36.479436Z","iopub.status.idle":"2024-12-11T16:09:39.497644Z","shell.execute_reply.started":"2024-12-11T16:09:36.479400Z","shell.execute_reply":"2024-12-11T16:09:39.496496Z"}},"outputs":[{"name":"stdout","text":"Istogramma salvato in /kaggle/working/iou_histogram.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA2wAAAIjCAYAAAB/FZhcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKKElEQVR4nO3de1hVZf7//xegGxAFj4AkeS4lTxMqkpqZJBpZln1TM8NTjoZ9UspTOWJOM5YzlTYeaKoRmxkntTFLTdTwNClpoUxqSqaWNQpqClvJAGH9/ujHyi2oQOi+1efjuvZ1udd677Xee7Eu4tW91r08LMuyBAAAAAAwjqe7GwAAAAAAlI7ABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGADeQadOmycPD46rs66677tJdd91lv9+4caM8PDz03nvvXZX9F0tKSpKHh4e++eabq7rfynDhMSyPRo0aaciQIZXaT1l4eHho2rRplbrNa/lnCAC/FoENAK5RxX/EFr98fHwUEhKi6Ohovf766zp9+nSl7OfIkSOaNm2a0tPTK2V7wJVS/D8kTpw4Ue7PDhkyRNWrV7/o+urVq7slAAMAgQ0ArnHTp0/X3//+d82fP19PPfWUJGns2LFq3bq1vvjiC5faKVOm6OzZs+Xa/pEjR/TCCy+UO7CtXbtWa9euLddnroTBgwfr7NmzatiwobtbuaoyMjL05ptvuruNSnGj/gwBQJKquLsBAMCv07t3b7Vv395+P3nyZK1fv1733Xef7r//fu3du1e+vr6SpCpVqqhKlSv7q//HH39UtWrV5HA4ruh+ysrLy0teXl7ubuOq8/b2dncLv1pubq78/Pxu2J8hAEiMsAHAdenuu+/W7373O3377bf6xz/+YS8v7R62devWqUuXLqpZs6aqV6+uW2+9Vc8995ykn+8769ChgyRp6NCh9uWXSUlJkn6+x6pVq1ZKS0vTnXfeqWrVqtmfvdj9V4WFhXruuecUHBwsPz8/3X///fruu+9cai52/9WF22zUqJHLZaHnvzZu3Cjp4vc/zZs3T7fddpu8vb0VEhKiuLg4ZWdnl9hfq1at9OWXX6p79+6qVq2abrrpJs2cObNEb3l5eUpISFCzZs3k7e2t0NBQTZgwQXl5eSVqS/PXv/5VTZs2la+vrzp27Kj//Oc/pdaVdT+Xu4etoKBAtWvX1tChQ0usczqd8vHx0bPPPitJys/P19SpUxUeHq6AgAD5+fmpa9eu2rBhQ5m+286dO9W7d2/5+/urevXq6tGjhz799FOXmuKf06ZNm/Tkk08qMDBQDRo0cFlX0XvYli5dqvDwcPn6+qpu3bp67LHH9L///a9C2wKAq40RNgC4Tg0ePFjPPfec1q5dqyeeeKLUmj179ui+++5TmzZtNH36dHl7e+vrr7/Wli1bJEktW7bU9OnTNXXqVI0cOVJdu3aVJN1xxx32Nn744Qf17t1bAwYM0GOPPaagoKBL9vWHP/xBHh4emjhxoo4dO6ZZs2YpKipK6enp9khgWc2aNUtnzpxxWfbaa68pPT1dderUuejnpk2bphdeeEFRUVEaPXq0MjIyNH/+fH322WfasmWLqlatateeOnVKvXr10kMPPaRHHnlE7733niZOnKjWrVurd+/ekqSioiLdf//9+uSTTzRy5Ei1bNlSu3bt0muvvaavvvpKy5cvv+T3ePvtt/Xb3/5Wd9xxh8aOHauDBw/q/vvvV+3atRUaGmrX/dr9nK9q1ap68MEHtWzZMr3xxhsuI6LLly9XXl6eBgwYIOnnAPfWW29p4MCBeuKJJ3T69Gm9/fbbio6O1vbt29WuXbuL7mfPnj3q2rWr/P39NWHCBFWtWlVvvPGG7rrrLm3atEkREREu9U8++aTq1aunqVOnKjc3t8zf52KSkpI0dOhQdejQQTNmzFBWVpZmz56tLVu2aOfOnapZs+av3gcAXFEWAOCatGDBAkuS9dlnn120JiAgwPrNb35jv09ISLDO/9X/2muvWZKs48ePX3Qbn332mSXJWrBgQYl13bp1syRZiYmJpa7r1q2b/X7Dhg2WJOumm26ynE6nvXzJkiWWJGv27Nn2soYNG1qxsbGX3eaFirc1ffp0e1nxcTp06JBlWZZ17Ngxy+FwWD179rQKCwvtujlz5liSrL/97W8lvt8777xjL8vLy7OCg4Otfv362cv+/ve/W56entZ//vMfl34SExMtSdaWLVsu2nN+fr4VGBhotWvXzsrLy7OX//Wvf7UkuXzf8uznYsfwfGvWrLEkWStWrHBZfu+991pNmjSx3587d86lN8uyrFOnTllBQUHWsGHDXJZLshISEuz3ffv2tRwOh3XgwAF72ZEjR6waNWpYd955p72s+OfUpUsX69y5cy7bvPBneDHF53fx+Vx8bFu1amWdPXvWrlu5cqUlyZo6daq9LDY21vLz87votv38/C57PAHgSuCSSAC4jlWvXv2Ss0UWjy588MEHKioqqtA+vL29S72s7mIef/xx1ahRw37/8MMPq379+vroo48qtP9iX375pYYNG6YHHnhAU6ZMuWjdxx9/rPz8fI0dO1aenr/8Z/CJJ56Qv7+/Vq1a5VJfvXp1PfbYY/Z7h8Ohjh076uDBg/aypUuXqmXLlmrRooVOnDhhv+6++25JuuSlg59//rmOHTumUaNGuYxyDRkyRAEBAS61v2Y/pbn77rtVt25dLV682F526tQprVu3Tv3797eXeXl52b0VFRXp5MmTOnfunNq3b68dO3ZcdPuFhYVau3at+vbtqyZNmtjL69evr0cffVSffPKJnE6ny2eeeOKJSrtfrfjYPvnkk/Lx8bGXx8TEqEWLFiV+1gBgIgIbAFzHzpw54xKOLtS/f3917txZI0aMUFBQkAYMGKAlS5aUK7zddNNN5ZpgpHnz5i7vPTw81KxZs1/1jC2n06mHHnpIN910k955551LPmvu22+/lSTdeuutLssdDoeaNGliry/WoEGDEturVauWTp06Zb/fv3+/9uzZo3r16rm8brnlFknSsWPHLtvPhcelatWqLiHn1+6nNFWqVFG/fv30wQcf2PfALVu2TAUFBS6BTZIWLlyoNm3ayMfHR3Xq1FG9evW0atUq5eTkXHT7x48f148//ljiWEs/X25bVFRU4v7Fxo0bl+s7XMrFftaS1KJFixI/68u5Ws8wBIDzcQ8bAFynvv/+e+Xk5KhZs2YXrfH19dXmzZu1YcMGrVq1SsnJyVq8eLHuvvturV27tkwjHeW976wsLvaHcWFhYak9DRkyREeOHNH27dvl7+9fqb1c7BhYlmX/u6ioSK1bt9arr75aau3596H9GldiPwMGDNAbb7yh1atXq2/fvlqyZIlatGihtm3b2jX/+Mc/NGTIEPXt21fjx49XYGCgvLy8NGPGDB04cKDC36c0V+J8KgsfHx/l5eXJsqwS559lWfrpp59cRukA4GohsAHAdervf/+7JCk6OvqSdZ6enurRo4d69OihV199VX/84x/1/PPPa8OGDYqKiqr0UYX9+/e7vLcsS19//bXatGljL6tVq1aJGRuln0dMLhx1eumll7R8+XItW7ZMLVq0uOz+i5/llZGR4bKt/Px8HTp0SFFRUeX5OpKkpk2b6r///a969OhR7uNV3M/+/fvtSxuln2dxPHTokEtw+jX7uZg777xT9evX1+LFi9WlSxetX79ezz//vEvNe++9pyZNmmjZsmUu+01ISLjktuvVq6dq1aopIyOjxLp9+/bJ09Oz0sJsac7/WZ9/bIuXnf9ct4YNG+rcuXM6cOBAif/J8fXXX6uwsJDnwAFwCy6JBIDr0Pr16/X73/9ejRs31qBBgy5ad/LkyRLLimf8K75Ezs/PT5JKDVAV8c4777jcV/fee+/p6NGj9oyL0s/B5NNPP1V+fr69bOXKlSUun/v44481ZcoUPf/88+rbt2+Z9h8VFSWHw6HXX3/dZZTs7bffVk5OjmJiYsr9nR555BH973//K/VB1WfPnr3kbIft27dXvXr1lJiY6PJ9k5KSShzzX7Ofi/H09NTDDz+sFStW6O9//7vOnTtX4nLI4lHG84/Xtm3blJqaeslte3l5qWfPnvrggw9cLnnNysrSokWL1KVLl0ofET1f+/btFRgYqMTERJfHHqxevVp79+51+VkXn39z5swpsZ25c+e61ADA1cQIGwBc41avXq19+/bp3LlzysrK0vr167Vu3To1bNhQH3744SUv45o+fbo2b96smJgYNWzYUMeOHdO8efPUoEEDdenSRdLP4almzZpKTExUjRo15Ofnp4iIiArfa1S7dm116dJFQ4cOVVZWlmbNmqVmzZq5PHpgxIgReu+999SrVy898sgjOnDggP7xj3+oadOmLtsaOHCg6tWrp+bNm7s8b06S7rnnnlIfMVCvXj1NnjxZL7zwgnr16qX7779fGRkZmjdvnjp06OAywUhZDR48WEuWLNGoUaO0YcMGde7cWYWFhdq3b5+WLFmiNWvWuDzc/HxVq1bViy++qN/+9re6++671b9/fx06dEgLFiwoMZr4a/ZzKf3799df/vIXJSQkqHXr1mrZsqXL+vvuu0/Lli3Tgw8+qJiYGB06dEiJiYkKCwsr8ViFC7344ov2s/6efPJJValSRW+88Yby8vJKfZ5dZapatapefvllDR06VN26ddPAgQPtaf0bNWqkcePG2bXt2rXTiBEjNHv2bO3fv1/33HOPpJ+fU/jRRx9pxIgRLqOdAHDVuHOKSgBAxRVPdV78cjgcVnBwsHXPPfdYs2fPdpk6v9iF0/qnpKRYDzzwgBUSEmI5HA4rJCTEGjhwoPXVV1+5fO6DDz6wwsLCrCpVqrhM8d+tWzfrtttuK7W/i03r/69//cuaPHmyFRgYaPn6+loxMTHWt99+W+Lzr7zyinXTTTdZ3t7eVufOna3PP/+8xDbP//4XvjZs2OBynC6cEn7OnDlWixYtrKpVq1pBQUHW6NGjrVOnTpX4DqV9v9jYWKthw4Yuy/Lz862XX37Zuu222yxvb2+rVq1aVnh4uPXCCy9YOTk5pR6j882bN89q3Lix5e3tbbVv397avHlzqY8xKOt+yjKtf7GioiIrNDTUkmS9+OKLpa7/4x//aDVs2NDy9va2fvOb31grV64s9Tjogmn9LcuyduzYYUVHR1vVq1e3qlWrZnXv3t3aunWrS82lHlNR0Wn9iy1evNj6zW9+Y3l7e1u1a9e2Bg0aZH3//fclPl9YWGjNnj3batu2reXj42P5+PhYbdu2tV5//XWXR0AAwNXkYVnnXd8AAAAAADAG97ABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgenH0VFRUV6ciRI6pRo4Y8PDzc3Q4AAAAAN7EsS6dPn1ZISIg8PS8+jkZgu4qOHDmi0NBQd7cBAAAAwBDfffedGjRocNH1BLarqEaNGpJ+/qH4+/u7uRsAAAAA7uJ0OhUaGmpnhIshsF1FxZdB+vv7E9gAAAAAXPZWKSYdAQAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMFQVdzcA9+nTx90d/GLFCnd3AAAAAJiHETYAAAAAMBSBDQAAAAAM5dbANn/+fLVp00b+/v7y9/dXZGSkVq9eba//6aefFBcXpzp16qh69erq16+fsrKyXLZx+PBhxcTEqFq1agoMDNT48eN17tw5l5qNGzfq9ttvl7e3t5o1a6akpKQSvcydO1eNGjWSj4+PIiIitH37dpf1ZekFAAAAACqTWwNbgwYN9NJLLyktLU2ff/657r77bj3wwAPas2ePJGncuHFasWKFli5dqk2bNunIkSN66KGH7M8XFhYqJiZG+fn52rp1qxYuXKikpCRNnTrVrjl06JBiYmLUvXt3paena+zYsRoxYoTWrFlj1yxevFjx8fFKSEjQjh071LZtW0VHR+vYsWN2zeV6AQAAAIDK5mFZluXuJs5Xu3Zt/elPf9LDDz+sevXqadGiRXr44YclSfv27VPLli2VmpqqTp06afXq1brvvvt05MgRBQUFSZISExM1ceJEHT9+XA6HQxMnTtSqVau0e/duex8DBgxQdna2kpOTJUkRERHq0KGD5syZI0kqKipSaGionnrqKU2aNEk5OTmX7aUsnE6nAgIClJOTI39//0o7ZhXFpCMAAACAe5Q1GxhzD1thYaHeffdd5ebmKjIyUmlpaSooKFBUVJRd06JFC918881KTU2VJKWmpqp169Z2WJOk6OhoOZ1Oe5QuNTXVZRvFNcXbyM/PV1pamkuNp6enoqKi7Jqy9FKavLw8OZ1OlxcAAAAAlJXbA9uuXbtUvXp1eXt7a9SoUXr//fcVFhamzMxMORwO1axZ06U+KChImZmZkqTMzEyXsFa8vnjdpWqcTqfOnj2rEydOqLCwsNSa87dxuV5KM2PGDAUEBNiv0NDQsh0UAAAAAJABge3WW29Venq6tm3bptGjRys2NlZffvmlu9uqFJMnT1ZOTo79+u6779zdEgAAAIBriNsfnO1wONSsWTNJUnh4uD777DPNnj1b/fv3V35+vrKzs11GtrKyshQcHCxJCg4OLjGbY/HMjefXXDibY1ZWlvz9/eXr6ysvLy95eXmVWnP+Ni7XS2m8vb3l7e1djqMBAAAAAL9w+wjbhYqKipSXl6fw8HBVrVpVKSkp9rqMjAwdPnxYkZGRkqTIyEjt2rXLZTbHdevWyd/fX2FhYXbN+dsorinehsPhUHh4uEtNUVGRUlJS7Jqy9AIAAAAAlc2tI2yTJ09W7969dfPNN+v06dNatGiRNm7cqDVr1iggIEDDhw9XfHy8ateuLX9/fz311FOKjIy0Z2Xs2bOnwsLCNHjwYM2cOVOZmZmaMmWK4uLi7JGtUaNGac6cOZowYYKGDRum9evXa8mSJVq1apXdR3x8vGJjY9W+fXt17NhRs2bNUm5uroYOHSpJZeoFAAAAACqbWwPbsWPH9Pjjj+vo0aMKCAhQmzZttGbNGt1zzz2SpNdee02enp7q16+f8vLyFB0drXnz5tmf9/Ly0sqVKzV69GhFRkbKz89PsbGxmj59ul3TuHFjrVq1SuPGjdPs2bPVoEEDvfXWW4qOjrZr+vfvr+PHj2vq1KnKzMxUu3btlJyc7DIRyeV6AQAAAIDKZtxz2K5nPIft4ngOGwAAAG4k19xz2AAAAAAArghsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAodwa2GbMmKEOHTqoRo0aCgwMVN++fZWRkeFSc9ddd8nDw8PlNWrUKJeaw4cPKyYmRtWqVVNgYKDGjx+vc+fOudRs3LhRt99+u7y9vdWsWTMlJSWV6Gfu3Llq1KiRfHx8FBERoe3bt7us/+mnnxQXF6c6deqoevXq6tevn7KysirnYAAAAADABdwa2DZt2qS4uDh9+umnWrdunQoKCtSzZ0/l5ua61D3xxBM6evSo/Zo5c6a9rrCwUDExMcrPz9fWrVu1cOFCJSUlaerUqXbNoUOHFBMTo+7duys9PV1jx47ViBEjtGbNGrtm8eLFio+PV0JCgnbs2KG2bdsqOjpax44ds2vGjRunFStWaOnSpdq0aZOOHDmihx566AoeIQAAAAA3Mg/Lsix3N1Hs+PHjCgwM1KZNm3TnnXdK+nmErV27dpo1a1apn1m9erXuu+8+HTlyREFBQZKkxMRETZw4UcePH5fD4dDEiRO1atUq7d692/7cgAEDlJ2dreTkZElSRESEOnTooDlz5kiSioqKFBoaqqeeekqTJk1STk6O6tWrp0WLFunhhx+WJO3bt08tW7ZUamqqOnXqdNnv53Q6FRAQoJycHPn7+1f4OFWWPn3c3cEvVqxwdwcAAADA1VPWbGDUPWw5OTmSpNq1a7ss/+c//6m6deuqVatWmjx5sn788Ud7XWpqqlq3bm2HNUmKjo6W0+nUnj177JqoqCiXbUZHRys1NVWSlJ+fr7S0NJcaT09PRUVF2TVpaWkqKChwqWnRooVuvvlmu+ZCeXl5cjqdLi8AAAAAKKsq7m6gWFFRkcaOHavOnTurVatW9vJHH31UDRs2VEhIiL744gtNnDhRGRkZWrZsmSQpMzPTJaxJst9nZmZessbpdOrs2bM6deqUCgsLS63Zt2+fvQ2Hw6GaNWuWqCnez4VmzJihF154oZxHAgAAAAB+Zkxgi4uL0+7du/XJJ5+4LB85cqT979atW6t+/frq0aOHDhw4oKZNm17tNstl8uTJio+Pt987nU6Fhoa6sSMAAAAA1xIjLokcM2aMVq5cqQ0bNqhBgwaXrI2IiJAkff3115Kk4ODgEjM1Fr8PDg6+ZI2/v798fX1Vt25deXl5lVpz/jby8/OVnZ190ZoLeXt7y9/f3+UFAAAAAGXl1sBmWZbGjBmj999/X+vXr1fjxo0v+5n09HRJUv369SVJkZGR2rVrl8tsjuvWrZO/v7/CwsLsmpSUFJftrFu3TpGRkZIkh8Oh8PBwl5qioiKlpKTYNeHh4apatapLTUZGhg4fPmzXAAAAAEBlcuslkXFxcVq0aJE++OAD1ahRw74XLCAgQL6+vjpw4IAWLVqke++9V3Xq1NEXX3yhcePG6c4771SbNm0kST179lRYWJgGDx6smTNnKjMzU1OmTFFcXJy8vb0lSaNGjdKcOXM0YcIEDRs2TOvXr9eSJUu0atUqu5f4+HjFxsaqffv26tixo2bNmqXc3FwNHTrU7mn48OGKj49X7dq15e/vr6eeekqRkZFlmiESAAAAAMrLrYFt/vz5kn6euv98CxYs0JAhQ+RwOPTxxx/b4Sk0NFT9+vXTlClT7FovLy+tXLlSo0ePVmRkpPz8/BQbG6vp06fbNY0bN9aqVas0btw4zZ49Ww0aNNBbb72l6Ohou6Z///46fvy4pk6dqszMTLVr107JyckuE5G89tpr8vT0VL9+/ZSXl6fo6GjNmzfvCh0dAAAAADc6o57Ddr3jOWwXx3PYAAAAcCO5Jp/DBgAAAAD4BYENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwlFsD24wZM9ShQwfVqFFDgYGB6tu3rzIyMlxqfvrpJ8XFxalOnTqqXr26+vXrp6ysLJeaw4cPKyYmRtWqVVNgYKDGjx+vc+fOudRs3LhRt99+u7y9vdWsWTMlJSWV6Gfu3Llq1KiRfHx8FBERoe3bt5e7FwAAAACoLG4NbJs2bVJcXJw+/fRTrVu3TgUFBerZs6dyc3PtmnHjxmnFihVaunSpNm3apCNHjuihhx6y1xcWFiomJkb5+fnaunWrFi5cqKSkJE2dOtWuOXTokGJiYtS9e3elp6dr7NixGjFihNasWWPXLF68WPHx8UpISNCOHTvUtm1bRUdH69ixY2XuBQAAAAAqk4dlWZa7myh2/PhxBQYGatOmTbrzzjuVk5OjevXqadGiRXr44YclSfv27VPLli2VmpqqTp06afXq1brvvvt05MgRBQUFSZISExM1ceJEHT9+XA6HQxMnTtSqVau0e/due18DBgxQdna2kpOTJUkRERHq0KGD5syZI0kqKipSaGionnrqKU2aNKlMvVyO0+lUQECAcnJy5O/vX6nHriL69HF3B79YscLdHQAAAABXT1mzgVH3sOXk5EiSateuLUlKS0tTQUGBoqKi7JoWLVro5ptvVmpqqiQpNTVVrVu3tsOaJEVHR8vpdGrPnj12zfnbKK4p3kZ+fr7S0tJcajw9PRUVFWXXlKWXC+Xl5cnpdLq8AAAAAKCsjAlsRUVFGjt2rDp37qxWrVpJkjIzM+VwOFSzZk2X2qCgIGVmZto154e14vXF6y5V43Q6dfbsWZ04cUKFhYWl1py/jcv1cqEZM2YoICDAfoWGhpbxaAAAAACAQYEtLi5Ou3fv1rvvvuvuVirN5MmTlZOTY7++++47d7cEAAAA4BpSxd0NSNKYMWO0cuVKbd68WQ0aNLCXBwcHKz8/X9nZ2S4jW1lZWQoODrZrLpzNsXjmxvNrLpzNMSsrS/7+/vL19ZWXl5e8vLxKrTl/G5fr5ULe3t7y9vYux5EAAAAAgF+4dYTNsiyNGTNG77//vtavX6/GjRu7rA8PD1fVqlWVkpJiL8vIyNDhw4cVGRkpSYqMjNSuXbtcZnNct26d/P39FRYWZtecv43imuJtOBwOhYeHu9QUFRUpJSXFrilLLwAAAABQmdw6whYXF6dFixbpgw8+UI0aNex7wQICAuTr66uAgAANHz5c8fHxql27tvz9/fXUU08pMjLSnpWxZ8+eCgsL0+DBgzVz5kxlZmZqypQpiouLs0e3Ro0apTlz5mjChAkaNmyY1q9fryVLlmjVqlV2L/Hx8YqNjVX79u3VsWNHzZo1S7m5uRo6dKjd0+V6AQAAAIDK5NbANn/+fEnSXXfd5bJ8wYIFGjJkiCTptddek6enp/r166e8vDxFR0dr3rx5dq2Xl5dWrlyp0aNHKzIyUn5+foqNjdX06dPtmsaNG2vVqlUaN26cZs+erQYNGuitt95SdHS0XdO/f38dP35cU6dOVWZmptq1a6fk5GSXiUgu1wsAAAAAVCajnsN2veM5bBfHc9gAAABwI7kmn8MGAAAAAPgFgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQ1Wp6Ae///57ffjhhzp8+LDy8/Nd1r366qu/ujEAAAAAuNFVKLClpKTo/vvvV5MmTbRv3z61atVK33zzjSzL0u23317ZPQIAAADADalCl0ROnjxZzz77rHbt2iUfHx/9+9//1nfffadu3brp//2//1fZPQIAAADADalCgW3v3r16/PHHJUlVqlTR2bNnVb16dU2fPl0vv/xypTYIAAAAADeqCgU2Pz8/+761+vXr68CBA/a6EydOVE5nAAAAAHCDq9A9bJ06ddInn3yili1b6t5779UzzzyjXbt2admyZerUqVNl9wgAAAAAN6QKBbZXX31VZ86ckSS98MILOnPmjBYvXqzmzZszQyQAAAAAVJIKBbYmTZrY//bz81NiYmKlNQQAAAAA+FmF7mEbNmyYFi5cWGK50+nUsGHDfnVTAAAAAIAKBrakpCQ9+eST+r//+z8VFRXZy8+ePVtqkAMAAAAAlF+FApskrVq1Sh999JGio6N16tSpyuwJAAAAAKBfEdjCwsK0bds2FRQUqGPHjtq7d29l9gUAAAAAN7wKBTYPDw9JUp06dfTxxx+rW7duioyM1IcfflipzQEAAADAjaxCs0RalvXLBqpU0VtvvaWwsDA9+eSTldYYAAAAANzoKhTYNmzYoNq1a7ssi4+PV5s2bbRly5ZKaQwAAAAAbnQVCmzdunUrdXlUVJSioqJ+VUMAAAAAgJ9VKLAVFhYqKSlJKSkpOnbsmMvU/pK0fv36SmkOAAAAAG5kFQpsTz/9tJKSkhQTE6NWrVrZk5AAAAAAACpPhQLbu+++qyVLlujee++t7H4AAAAAAP+/Ck3r73A41KxZs8ruBQAAAABwngoFtmeeeUazZ892md4fAAAAAFC5KnRJ5CeffKINGzZo9erVuu2221S1alWX9cuWLauU5gAAAADgRlahwFazZk09+OCDld0LAAAAAOA8FQpsCxYsqOw+AAAAAAAXqNA9bJJ07tw5ffzxx3rjjTd0+vRpSdKRI0d05syZSmsOAAAAAG5kFRph+/bbb9WrVy8dPnxYeXl5uueee1SjRg29/PLLysvLU2JiYmX3CQAAAAA3nAqNsD399NNq3769Tp06JV9fX3v5gw8+qJSUlEprDgAAAABuZBUaYfvPf/6jrVu3yuFwuCxv1KiR/ve//1VKYwAAAABwo6vQCFtRUZEKCwtLLP/+++9Vo0aNX90UAAAAAKCCga1nz56aNWuW/d7Dw0NnzpxRQkKC7r333srqDQAAAABuaBW6JPKVV15RdHS0wsLC9NNPP+nRRx/V/v37VbduXf3rX/+q7B4BAAAA4IZUocDWoEED/fe//9W7776rL774QmfOnNHw4cM1aNAgl0lIAAAAAAAVV6HAJklVqlTRY489Vpm9AAAAAADOU6HA9s4771xy/eOPP16hZgAAAAAAv6hQYHv66add3hcUFOjHH3+Uw+FQtWrVCGwAAAAAUAkqNEvkqVOnXF5nzpxRRkaGunTpwqQjAAAAAFBJKhTYStO8eXO99NJLJUbfAAAAAAAVU2mBTfp5IpIjR45U5iYBAAAA4IZVoXvYPvzwQ5f3lmXp6NGjmjNnjjp37lwpjQEAAADAja5Cga1v374u7z08PFSvXj3dfffdeuWVVyqjLwAAAAC44VUosBUVFVV2HwAAAACAC1TqPWwAAAAAgMpTocAWHx9f5telbN68WX369FFISIg8PDy0fPlyl/VDhgyRh4eHy6tXr14uNSdPntSgQYPk7++vmjVravjw4Tpz5oxLzRdffKGuXbvKx8dHoaGhmjlzZoleli5dqhYtWsjHx0etW7fWRx995LLesixNnTpV9evXl6+vr6KiorR///5yHDUAAAAAKJ8KXRK5c+dO7dy5UwUFBbr11lslSV999ZW8vLx0++2323UeHh6X3E5ubq7atm2rYcOG6aGHHiq1plevXlqwYIH93tvb22X9oEGDdPToUa1bt04FBQUaOnSoRo4cqUWLFkmSnE6nevbsqaioKCUmJmrXrl0aNmyYatasqZEjR0qStm7dqoEDB2rGjBm67777tGjRIvXt21c7duxQq1atJEkzZ87U66+/roULF6px48b63e9+p+joaH355Zfy8fEp5xEEAAAAgMvzsCzLKu+HXn31VW3cuFELFy5UrVq1JP38MO2hQ4eqa9eueuaZZ8rfiIeH3n//fZcJTYYMGaLs7OwSI2/F9u7dq7CwMH322Wdq3769JCk5OVn33nuvvv/+e4WEhGj+/Pl6/vnnlZmZKYfDIUmaNGmSli9frn379kmS+vfvr9zcXK1cudLedqdOndSuXTslJibKsiyFhITomWee0bPPPitJysnJUVBQkJKSkjRgwIAyfUen06mAgADl5OTI39+/vIeo0vXp4+4OfrFihbs7AAAAAK6esmaDCl0S+corr2jGjBl2WJOkWrVq6cUXX6z0WSI3btyowMBA3XrrrRo9erR++OEHe11qaqpq1qxphzVJioqKkqenp7Zt22bX3HnnnXZYk6To6GhlZGTo1KlTdk1UVJTLfqOjo5WamipJOnTokDIzM11qAgICFBERYdeUJi8vT06n0+UFAAAAAGVVocDmdDp1/PjxEsuPHz+u06dP/+qmivXq1UvvvPOOUlJS9PLLL2vTpk3q3bu3CgsLJUmZmZkKDAx0+UyVKlVUu3ZtZWZm2jVBQUEuNcXvL1dz/vrzP1daTWlmzJihgIAA+xUaGlqu7w8AAADgxlahe9gefPBBDR06VK+88oo6duwoSdq2bZvGjx9/0XvRKuL8Sw1bt26tNm3aqGnTptq4caN69OhRafu5UiZPnuwy8YrT6SS0AQAAACizCo2wJSYmqnfv3nr00UfVsGFDNWzYUI8++qh69eqlefPmVXaPtiZNmqhu3br6+uuvJUnBwcE6duyYS825c+d08uRJBQcH2zVZWVkuNcXvL1dz/vrzP1daTWm8vb3l7+/v8gIAAACAsqpQYKtWrZrmzZunH374wZ4x8uTJk5o3b578/Pwqu0fb999/rx9++EH169eXJEVGRio7O1tpaWl2zfr161VUVKSIiAi7ZvPmzSooKLBr1q1bp1tvvdW+By8yMlIpKSku+1q3bp0iIyMlSY0bN1ZwcLBLjdPp1LZt2+waAAAAAKhsv+rB2UePHtXRo0fVvHlz+fn5qbwTTp45c0bp6elKT0+X9PPkHunp6Tp8+LDOnDmj8ePH69NPP9U333yjlJQUPfDAA2rWrJmio6MlSS1btlSvXr30xBNPaPv27dqyZYvGjBmjAQMGKCQkRJL06KOPyuFwaPjw4dqzZ48WL16s2bNnu1yq+PTTTys5OVmvvPKK9u3bp2nTpunzzz/XmDFjJP08g+XYsWP14osv6sMPP9SuXbv0+OOPKyQkxGVWSwAAAACoTBW6h+2HH37QI488og0bNsjDw0P79+9XkyZNNHz4cNWqVavMM0V+/vnn6t69u/2+OETFxsZq/vz5+uKLL7Rw4UJlZ2crJCREPXv21O9//3uXZ7H985//1JgxY9SjRw95enqqX79+ev311+31AQEBWrt2reLi4hQeHq66detq6tSp9jPYJOmOO+7QokWLNGXKFD333HNq3ry5li9fbj+DTZImTJig3NxcjRw5UtnZ2erSpYuSk5N5BhsAAACAK6ZCz2F7/PHHdezYMb311ltq2bKl/vvf/6pJkyZas2aN4uPjtWfPnivR6zWP57BdHM9hAwAAwI2krNmgQiNsa9eu1Zo1a9SgQQOX5c2bN9e3335bkU0CAAAAAC5QoXvYcnNzVa1atRLLT5486XK5IgAAAACg4ioU2Lp27ap33nnHfu/h4aGioiLNnDnT5Z40AAAAAEDFVeiSyJkzZ6pHjx76/PPPlZ+frwkTJmjPnj06efKktmzZUtk9AgAAAMANqUIjbK1atdJXX32lLl266IEHHlBubq4eeugh7dy5U02bNq3sHgEAAADghlTuEbaCggL16tVLiYmJev75569ETwAAAAAAVWCErWrVqvriiy+uRC8AAAAAgPNU6JLIxx57TG+//XZl9wIAAAAAOE+FJh05d+6c/va3v+njjz9WeHi4/Pz8XNa/+uqrldIcAAAAANzIyhXYDh48qEaNGmn37t26/fbbJUlfffWVS42Hh0fldQcAAAAAN7ByBbbmzZvr6NGj2rBhgySpf//+ev311xUUFHRFmgMAAACAG1m57mGzLMvl/erVq5Wbm1upDQEAAAAAflahSUeKXRjgAAAAAACVp1yBzcPDo8Q9atyzBgAAAABXRrnuYbMsS0OGDJG3t7ck6aefftKoUaNKzBK5bNmyyusQAAAAAG5Q5QpssbGxLu8fe+yxSm0GAAAAAPCLcgW2BQsWXKk+AAAAAAAX+FWTjgAAAAAArhwCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYqoq7GwAkqU8fd3fgasUKd3cAAAAAMMIGAAAAAMYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGquLuBgAT9enj7g5+sWKFuzsAAACAuzDCBgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAotwa2zZs3q0+fPgoJCZGHh4eWL1/ust6yLE2dOlX169eXr6+voqKitH//fpeakydPatCgQfL391fNmjU1fPhwnTlzxqXmiy++UNeuXeXj46PQ0FDNnDmzRC9Lly5VixYt5OPjo9atW+ujjz4qdy8AAAAAUJncGthyc3PVtm1bzZ07t9T1M2fO1Ouvv67ExERt27ZNfn5+io6O1k8//WTXDBo0SHv27NG6deu0cuVKbd68WSNHjrTXO51O9ezZUw0bNlRaWpr+9Kc/adq0afrrX/9q12zdulUDBw7U8OHDtXPnTvXt21d9+/bV7t27y9ULAAAAAFQmD8uyLHc3IUkeHh56//331bdvX0k/j2iFhITomWee0bPPPitJysnJUVBQkJKSkjRgwADt3btXYWFh+uyzz9S+fXtJUnJysu699159//33CgkJ0fz58/X8888rMzNTDodDkjRp0iQtX75c+/btkyT1799fubm5Wrlypd1Pp06d1K5dOyUmJpapl7JwOp0KCAhQTk6O/P39K+W4/Rp9+ri7A5TFihXu7gAAAACVrazZwNh72A4dOqTMzExFRUXZywICAhQREaHU1FRJUmpqqmrWrGmHNUmKioqSp6entm3bZtfceeeddliTpOjoaGVkZOjUqVN2zfn7Ka4p3k9ZeilNXl6enE6nywsAAAAAysrYwJaZmSlJCgoKclkeFBRkr8vMzFRgYKDL+ipVqqh27douNaVt4/x9XKzm/PWX66U0M2bMUEBAgP0KDQ29zLcGAAAAgF8YG9iuB5MnT1ZOTo79+u6779zdEgAAAIBriLGBLTg4WJKUlZXlsjwrK8teFxwcrGPHjrmsP3funE6ePOlSU9o2zt/HxWrOX3+5Xkrj7e0tf39/lxcAAAAAlJWxga1x48YKDg5WSkqKvczpdGrbtm2KjIyUJEVGRio7O1tpaWl2zfr161VUVKSIiAi7ZvPmzSooKLBr1q1bp1tvvVW1atWya87fT3FN8X7K0gsAAAAAVDa3BrYzZ84oPT1d6enpkn6e3CM9PV2HDx+Wh4eHxo4dqxdffFEffvihdu3apccff1whISH2TJItW7ZUr1699MQTT2j79u3asmWLxowZowEDBigkJESS9Oijj8rhcGj48OHas2ePFi9erNmzZys+Pt7u4+mnn1ZycrJeeeUV7du3T9OmTdPnn3+uMWPGSFKZegEAAACAylbFnTv//PPP1b17d/t9cYiKjY1VUlKSJkyYoNzcXI0cOVLZ2dnq0qWLkpOT5ePjY3/mn//8p8aMGaMePXrI09NT/fr10+uvv26vDwgI0Nq1axUXF6fw8HDVrVtXU6dOdXlW2x133KFFixZpypQpeu6559S8eXMtX75crVq1smvK0gsAAAAAVCZjnsN2I+A5bKgInsMGAABw/bnmn8MGAAAAADc6t14SCeDyTBoJZbQPAADg6mKEDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDVXF3AwCuHX36uLuDX6xY4e4OAAAArjxG2AAAAADAUAQ2AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUFXc3QAAVESfPu7u4BcrVri7AwAAcL1ihA0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUgQ0AAAAADFXF3Q0AwLWuTx93d+BqxQp3dwAAACoLI2wAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhqri7gYAAJWrTx93d/CLFSvc3QEAANc2RtgAAAAAwFBGB7Zp06bJw8PD5dWiRQt7/U8//aS4uDjVqVNH1atXV79+/ZSVleWyjcOHDysmJkbVqlVTYGCgxo8fr3PnzrnUbNy4Ubfffru8vb3VrFkzJSUllehl7ty5atSokXx8fBQREaHt27dfke8MAAAAAMWMDmySdNttt+no0aP265NPPrHXjRs3TitWrNDSpUu1adMmHTlyRA899JC9vrCwUDExMcrPz9fWrVu1cOFCJSUlaerUqXbNoUOHFBMTo+7duys9PV1jx47ViBEjtGbNGrtm8eLFio+PV0JCgnbs2KG2bdsqOjpax44duzoHAQAAAMANycOyLMvdTVzMtGnTtHz5cqWnp5dYl5OTo3r16mnRokV6+OGHJUn79u1Ty5YtlZqaqk6dOmn16tW67777dOTIEQUFBUmSEhMTNXHiRB0/flwOh0MTJ07UqlWrtHv3bnvbAwYMUHZ2tpKTkyVJERER6tChg+bMmSNJKioqUmhoqJ566ilNmjSpzN/H6XQqICBAOTk58vf3r+hhqTQm3ecC4PrEPWwAAJSurNnA+BG2/fv3KyQkRE2aNNGgQYN0+PBhSVJaWpoKCgoUFRVl17Zo0UI333yzUlNTJUmpqalq3bq1HdYkKTo6Wk6nU3v27LFrzt9GcU3xNvLz85WWluZS4+npqaioKLvmYvLy8uR0Ol1eAAAAAFBWRge2iIgIJSUlKTk5WfPnz9ehQ4fUtWtXnT59WpmZmXI4HKpZs6bLZ4KCgpSZmSlJyszMdAlrxeuL112qxul06uzZszpx4oQKCwtLrSnexsXMmDFDAQEB9is0NLTcxwAAAADAjcvoaf179+5t/7tNmzaKiIhQw4YNtWTJEvn6+rqxs7KZPHmy4uPj7fdOp5PQBgAAAKDMjA5sF6pZs6ZuueUWff3117rnnnuUn5+v7Oxsl1G2rKwsBQcHS5KCg4NLzOZYPIvk+TUXziyZlZUlf39/+fr6ysvLS15eXqXWFG/jYry9veXt7V2h7woA1wOT7pXlfjoAwLXI6EsiL3TmzBkdOHBA9evXV3h4uKpWraqUlBR7fUZGhg4fPqzIyEhJUmRkpHbt2uUym+O6devk7++vsLAwu+b8bRTXFG/D4XAoPDzcpaaoqEgpKSl2DQAAAABcCUYHtmeffVabNm3SN998o61bt+rBBx+Ul5eXBg4cqICAAA0fPlzx8fHasGGD0tLSNHToUEVGRqpTp06SpJ49eyosLEyDBw/Wf//7X61Zs0ZTpkxRXFycPfI1atQoHTx4UBMmTNC+ffs0b948LVmyROPGjbP7iI+P15tvvqmFCxdq7969Gj16tHJzczV06FC3HBcAAAAANwajL4n8/vvvNXDgQP3www+qV6+eunTpok8//VT16tWTJL322mvy9PRUv379lJeXp+joaM2bN8/+vJeXl1auXKnRo0crMjJSfn5+io2N1fTp0+2axo0ba9WqVRo3bpxmz56tBg0a6K233lJ0dLRd079/fx0/flxTp05VZmam2rVrp+Tk5BITkQAAAABAZTL6OWzXG57DBgDuwz1sAACTXDfPYQMAAACAGxWBDQAAAAAMRWADAAAAAEMR2AAAAADAUEbPEgkAQGUxaaIlJkABAJQVI2wAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGIpp/QEAuMpMesSAxGMGAMBkjLABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYiuewAQBwgzPpuXA8Ew4AXDHCBgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKSUcAAIAxmAAFAFwxwgYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIZilkgAAIBSMGMlABMwwgYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYiklHAAAADGfSBCgSk6AAVxMjbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYChmiQQAAEC5mDRrJTNW4nrHCBsAAAAAGIrABgAAAACGIrABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhmJafwAAAFyzeMQArneMsAEAAACAoQhsAAAAAGAoAhsAAAAAGIrABgAAAACGIrABAAAAgKGYJRIAAACoBMxYiSuBETYAAAAAMBSBDQAAAAAMRWADAAAAAEMR2AAAAADAUAQ2AAAAADAUs0QCAAAA1xlmrLx+MMJWTnPnzlWjRo3k4+OjiIgIbd++3d0tAQAAALhOEdjKYfHixYqPj1dCQoJ27Nihtm3bKjo6WseOHXN3awAAAACuQwS2cnj11Vf1xBNPaOjQoQoLC1NiYqKqVaumv/3tb+5uDQAAAMB1iHvYyig/P19paWmaPHmyvczT01NRUVFKTU0t9TN5eXnKy8uz3+fk5EiSnE7nlW22jAoK3N0BAAAArne9erm7A1dLlri7g58VZwLLsi5ZR2AroxMnTqiwsFBBQUEuy4OCgrRv375SPzNjxgy98MILJZaHhoZekR4BAAAAXFpAgLs7cHX69GkFXKIpAtsVNHnyZMXHx9vvi4qKdPLkSdWpU0ceHh5u7OznRB8aGqrvvvtO/v7+bu0F1wbOGZQX5wzKi3MG5cU5g/Iy6ZyxLEunT59WSEjIJesIbGVUt25deXl5KSsry2V5VlaWgoODS/2Mt7e3vL29XZbVrFnzSrVYIf7+/m4/WXFt4ZxBeXHOoLw4Z1BenDMoL1POmUuNrBVj0pEycjgcCg8PV0pKir2sqKhIKSkpioyMdGNnAAAAAK5XjLCVQ3x8vGJjY9W+fXt17NhRs2bNUm5uroYOHeru1gAAAABchwhs5dC/f38dP35cU6dOVWZmptq1a6fk5OQSE5FcC7y9vZWQkFDikk3gYjhnUF6cMygvzhmUF+cMyutaPGc8rMvNIwkAAAAAcAvuYQMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQAAAAAMRWC7js2dO1eNGjWSj4+PIiIitH379kvWL126VC1atJCPj49at26tjz766Cp1ClOU55x588031bVrV9WqVUu1atVSVFTUZc8xXH/K+3um2LvvvisPDw/17dv3yjYI45T3nMnOzlZcXJzq168vb29v3XLLLfz36QZT3nNm1qxZuvXWW+Xr66vQ0FCNGzdOP/3001XqFu60efNm9enTRyEhIfLw8NDy5csv+5mNGzfq9ttvl7e3t5o1a6akpKQr3md5EdiuU4sXL1Z8fLwSEhK0Y8cOtW3bVtHR0Tp27Fip9Vu3btXAgQM1fPhw7dy5U3379lXfvn21e/fuq9w53KW858zGjRs1cOBAbdiwQampqQoNDVXPnj31v//97yp3Dncp7zlT7JtvvtGzzz6rrl27XqVOYYrynjP5+fm655579M033+i9995TRkaG3nzzTd10001XuXO4S3nPmUWLFmnSpElKSEjQ3r179fbbb2vx4sV67rnnrnLncIfc3Fy1bdtWc+fOLVP9oUOHFBMTo+7duys9PV1jx47ViBEjtGbNmivcaTlZuC517NjRiouLs98XFhZaISEh1owZM0qtf+SRR6yYmBiXZREREdZvf/vbK9onzFHec+ZC586ds2rUqGEtXLjwSrUIw1TknDl37px1xx13WG+99ZYVGxtrPfDAA1ehU5iivOfM/PnzrSZNmlj5+flXq0UYprznTFxcnHX33Xe7LIuPj7c6d+58RfuEeSRZ77///iVrJkyYYN12220uy/r3729FR0dfwc7KjxG261B+fr7S0tIUFRVlL/P09FRUVJRSU1NL/UxqaqpLvSRFR0dftB7Xl4qcMxf68ccfVVBQoNq1a1+pNmGQip4z06dPV2BgoIYPH3412oRBKnLOfPjhh4qMjFRcXJyCgoLUqlUr/fGPf1RhYeHVahtuVJFz5o477lBaWpp92eTBgwf10Ucf6d57770qPePacq38/VvF3Q2g8p04cUKFhYUKCgpyWR4UFKR9+/aV+pnMzMxS6zMzM69YnzBHRc6ZC02cOFEhISElfvHh+lSRc+aTTz7R22+/rfT09KvQIUxTkXPm4MGDWr9+vQYNGqSPPvpIX3/9tZ588kkVFBQoISHharQNN6rIOfPoo4/qxIkT6tKliyzL0rlz5zRq1CguiUSpLvb3r9Pp1NmzZ+Xr6+umzlwxwgbgV3vppZf07rvv6v3335ePj4+724GBTp8+rcGDB+vNN99U3bp13d0OrhFFRUUKDAzUX//6V4WHh6t///56/vnnlZiY6O7WYKiNGzfqj3/8o+bNm6cdO3Zo2bJlWrVqlX7/+9+7uzWgwhhhuw7VrVtXXl5eysrKclmelZWl4ODgUj8THBxcrnpcXypyzhT785//rJdeekkff/yx2rRpcyXbhEHKe84cOHBA33zzjfr06WMvKyoqkiRVqVJFGRkZatq06ZVtGm5Vkd8z9evXV9WqVeXl5WUva9mypTIzM5Wfny+Hw3FFe4Z7VeSc+d3vfqfBgwdrxIgRkqTWrVsrNzdXI0eO1PPPPy9PT8Yq8IuL/f3r7+9vzOiaxAjbdcnhcCg8PFwpKSn2sqKiIqWkpCgyMrLUz0RGRrrUS9K6desuWo/rS0XOGUmaOXOmfv/73ys5OVnt27e/Gq3CEOU9Z1q0aKFdu3YpPT3dft1///32zFyhoaFXs324QUV+z3Tu3Flff/21He4l6auvvlL9+vUJazeAipwzP/74Y4lQVhz4Lcu6cs3imnTN/P3r7llPcGW8++67lre3t5WUlGR9+eWX1siRI62aNWtamZmZlmVZ1uDBg61JkybZ9Vu2bLGqVKli/fnPf7b27t1rJSQkWFWrVrV27drlrq+Aq6y858xLL71kORwO67333rOOHj1qv06fPu2ur4CrrLznzIWYJfLGU95z5vDhw1aNGjWsMWPGWBkZGdbKlSutwMBA68UXX3TXV8BVVt5zJiEhwapRo4b1r3/9yzp48KC1du1aq2nTptYjjzzirq+Aq+j06dPWzp07rZ07d1qSrFdffdXauXOn9e2331qWZVmTJk2yBg8ebNcfPHjQqlatmjV+/Hhr79691ty5cy0vLy8rOTnZXV+hVAS269hf/vIX6+abb7YcDofVsWNH69NPP7XXdevWzYqNjXWpX7JkiXXLLbdYDofDuu2226xVq1Zd5Y7hbuU5Zxo2bGhJKvFKSEi4+o3Dbcr7e+Z8BLYbU3nPma1bt1oRERGWt7e31aRJE+sPf/iDde7cuavcNdypPOdMQUGBNW3aNKtp06aWj4+PFRoaaj355JPWqVOnrn7juOo2bNhQ6t8mxedIbGys1a1btxKfadeuneVwOKwmTZpYCxYsuOp9X46HZTE+DAAAAAAm4h42AAAAADAUgQ0AAAAADEVgAwAAAABDEdgAAAAAwFAENgAAAAAwFIENAAAAAAxFYAMAAAAAQxHYAAAAAMBQBDYAAAAAMBSBDQCASjJkyBD17du3TLUbN26Uh4eHsrOzS6xr1KiRZs2aVam9AQCuTQQ2AAAAADAUgQ0AgCsgLy9P//d//6fAwED5+PioS5cu+uyzz9zdFgDgGkNgAwDgCpgwYYL+/e9/a+HChdqxY4eaNWum6OhonTx50t2tAQCuIQQ2AAAqWW5urubPn68//elP6t27t8LCwvTmm2/K19dXb7/9trvbAwBcQwhsAABUsgMHDqigoECdO3e2l1WtWlUdO3bU3r173dgZAOBaQ2ADAMAN/P39JUk5OTkl1mVnZysgIOBqtwQAMBCBDQCASta0aVM5HA5t2bLFXlZQUKDPPvtMYWFhkqTmzZvL09NTaWlpLp89ePCgcnJydMstt1zVngEAZqri7gYAALje+Pn5afTo0Ro/frxq166tm2++WTNnztSPP/6o4cOHS5Jq1KihESNG6JlnnlGVKlXUunVrfffdd5o4caI6deqkO+64w83fAgBgAgIbAABXwEsvvaSioiINHjxYp0+fVvv27bVmzRrVqlXLrpk9e7ZeeuklTZw4Ud9++62Cg4N1zz336A9/+IM8PDzc2D0AwBQelmVZ7m4CAAAAAFAS97ABAAAAgKEIbAAAAABgKAIbAAAAABiKwAYAAAAAhiKwAQAAAIChCGwAAAAAYCgCGwAAAAAYisAGAAAAAIYisAEAAACAoQhsAAAAAGAoAhsAAAAAGOr/A2bzxx+uPKlXAAAAAElFTkSuQmCC"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"print(ignored_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:57:58.949427Z","iopub.execute_input":"2024-12-10T18:57:58.949894Z","iopub.status.idle":"2024-12-10T18:57:58.956105Z","shell.execute_reply.started":"2024-12-10T18:57:58.949845Z","shell.execute_reply":"2024-12-10T18:57:58.954908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# controllo sulle regioni\nfile_path = '/kaggle/working/active_regions.json'\n\n#carica il file JSON\nwith open(file_path, 'r') as f:\n    data = json.load(f)\n\n#conta numero di regioni\nnum_regioni = len(data)\nprint(f\"Numero di regioni: {num_regioni}\")\n\n# occorrenze dei category_id\ncategory_ids = [entry['category_id'] for entry in data]\ncategory_counts = Counter(category_ids)\nprint(\"Occorrenze dei category_id:\", category_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:22:43.419280Z","iopub.execute_input":"2024-12-11T16:22:43.419721Z","iopub.status.idle":"2024-12-11T16:22:44.416613Z","shell.execute_reply.started":"2024-12-11T16:22:43.419680Z","shell.execute_reply":"2024-12-11T16:22:44.415498Z"}},"outputs":[{"name":"stdout","text":"Numero di regioni: 391423\nOccorrenze dei category_id: Counter({11: 202110, 6: 162061, 1: 10104, 2: 4040, 4: 3672, 9: 3609, 0: 2733, 5: 1385, 8: 1192, 3: 350, 10: 99, 7: 68})\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import zipfile\nimport os\n\nworking_dir = \"/kaggle/working\"\n\n# Nome del file zip da creare\nzip_file_name = \"regions-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"proposals\",\n    \"proposals.json\"\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, working_dir)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:21:36.569318Z","iopub.execute_input":"2024-12-10T15:21:36.569818Z","iopub.status.idle":"2024-12-10T15:21:44.410751Z","shell.execute_reply.started":"2024-12-10T15:21:36.569776Z","shell.execute_reply":"2024-12-10T15:21:44.409466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Positive Region Proposals with mAP","metadata":{}},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON e restituisce i dati.\n    \"\"\"\n    with open(file_path, 'r') as f:\n        return json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:14:47.382949Z","iopub.execute_input":"2024-12-12T09:14:47.383595Z","iopub.status.idle":"2024-12-12T09:14:47.389779Z","shell.execute_reply.started":"2024-12-12T09:14:47.383547Z","shell.execute_reply":"2024-12-12T09:14:47.388467Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef calculate_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:14:50.407686Z","iopub.execute_input":"2024-12-12T09:14:50.408119Z","iopub.status.idle":"2024-12-12T09:14:50.417889Z","shell.execute_reply.started":"2024-12-12T09:14:50.408081Z","shell.execute_reply":"2024-12-12T09:14:50.416550Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def assign_labels_with_map(proposals, annotations, map_threshold, categories):\n    \"\"\"\n    Assegna etichette alle regioni proposte basandosi sulle annotations utilizzando mAP.\n\n    Parameters:\n        proposals (list): Lista delle regioni proposte per un'immagine.\n        annotations (list): Lista dei bounding box annotati con etichette.\n        map_threshold (float): Soglia mAP per considerare una corrispondenza positiva.\n        categories (dict): Dizionario di mapping category_id -> nome categoria.\n\n    Returns:\n        list: Lista delle proposte con le etichette assegnate.\n    \"\"\"\n    labeled_proposals = []\n\n    for proposal in proposals:\n        best_iou = 0\n        best_annotation = None\n\n        for annotation in annotations:\n            bbox = annotation['bbox']\n            annotation_bbox_x = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]  # Conversione in [x, y, x+w, y+h]\n            annotation_bbox = [float(value) for value in annotation_bbox_x]\n            \n            # Calcola l'IoU\n            iou = calculate_iou(proposal['coordinates'], annotation_bbox)\n            if iou > best_iou:\n                best_iou = iou\n                best_annotation = annotation\n\n        if best_iou >= map_threshold and best_annotation:\n            labeled_proposals.append({\n                'proposal_id': proposal['proposal_id'],\n                'coordinates': proposal['coordinates'],\n                'label': categories.get(best_annotation['category_id'], 'unknown'),\n                'iou': best_iou\n            })\n        else:\n            labeled_proposals.append({\n                'proposal_id': proposal['proposal_id'],\n                'coordinates': proposal['coordinates'],\n                'label': 'background',  # Classe \"sfondo\"\n                'iou': best_iou\n            })\n\n    return labeled_proposals\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:29:58.471889Z","iopub.execute_input":"2024-12-12T09:29:58.472883Z","iopub.status.idle":"2024-12-12T09:29:58.481267Z","shell.execute_reply.started":"2024-12-12T09:29:58.472838Z","shell.execute_reply":"2024-12-12T09:29:58.479985Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def calculate_map(results):\n    \"\"\"\n    Calcola la Mean Average Precision (mAP) sui risultati etichettati.\n\n    Parameters:\n        results (list): Lista di tutte le proposte etichettate.\n\n    Returns:\n        float: Valore mAP.\n    \"\"\"\n    ap_per_class = {}\n\n    for result in results:\n        for proposal in result['labeled_proposals']:\n            label = proposal['label']\n            if label == 'background':\n                continue\n\n            if label not in ap_per_class:\n                ap_per_class[label] = {'precision': [], 'recall': []}\n\n            # Placeholder per precisione e richiamo (da calcolare separatamente)\n            ap_per_class[label]['precision'].append(proposal.get('precision', 1))\n            ap_per_class[label]['recall'].append(proposal.get('recall', 1))\n\n    mean_ap = 0\n    num_classes = len(ap_per_class)\n\n    for label, metrics in ap_per_class.items():\n        sorted_indices = sorted(range(len(metrics['recall'])), key=lambda i: metrics['recall'][i])\n        precisions = [metrics['precision'][i] for i in sorted_indices]\n        recalls = [metrics['recall'][i] for i in sorted_indices]\n\n        ap = 0\n        for i in range(1, len(recalls)):\n            ap += (recalls[i] - recalls[i - 1]) * precisions[i]\n\n        mean_ap += ap\n\n    return mean_ap / num_classes if num_classes > 0 else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:14:57.107530Z","iopub.execute_input":"2024-12-12T09:14:57.107966Z","iopub.status.idle":"2024-12-12T09:14:57.117417Z","shell.execute_reply.started":"2024-12-12T09:14:57.107925Z","shell.execute_reply":"2024-12-12T09:14:57.116160Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def process_images_with_map(region_file, annotation_file, output_file):\n    \"\"\"\n    Processa tutte le immagini, collega le regioni proposte con le annotations e calcola la mAP.\n\n    Parameters:\n        region_file (str): Path al file JSON con le regioni proposte.\n        annotation_file (str): Path al file JSON con le annotations.\n        output_file (str): Path al file JSON di output.\n    \"\"\"\n    # Carica i file JSON\n    regions_data = load_json(region_file)\n    coco_data = load_json(annotation_file)\n\n    # Prepara le categorie\n    categories = {}\n    for cat in coco_data['categories']:\n        cat_id, cat_name = list(cat.items())[0]\n        categories[int(cat_id)] = cat_name\n\n    # Prepara le annotations\n    annotations_by_image = {}\n    for annotation in coco_data['annotations']:\n        img_id = annotation['image_id']\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        # Mantenere il formato bbox originale, senza conversione qui\n        annotations_by_image[img_id].append({\n            \"bbox\": annotation[\"bbox\"],  # Mantieni il bbox nel formato [x, y, w, h]\n            \"category_id\": annotation['category_id']\n        })\n\n    results = []\n\n    for image_data in tqdm(regions_data, desc=\"Processing Images\"):\n        image_id = image_data['image_id']\n        proposals = image_data['proposals']\n\n        # Trova le annotations corrispondenti all'immagine corrente\n        annotations = annotations_by_image.get(image_id, [])\n\n        # Calcola la soglia mAP dinamica basata sul numero di annotations\n        map_threshold = 0.5 if len(annotations) == 0 else min(0.5 + 0.05 * (len(annotations) - 1), 0.95)\n\n        # Associa le etichette alle proposte\n        labeled_proposals = assign_labels_with_map(proposals, annotations, map_threshold, categories)\n\n        # Aggiungi al risultato\n        results.append({\n            'image_id': image_id,\n            'file_name': image_data['file_name'],\n            'labeled_proposals': labeled_proposals\n        })\n\n    # Calcola la mAP complessiva\n    mean_ap = calculate_map(results)\n\n    # Salva i risultati in un file JSON\n    with open(output_file, 'w') as f:\n        json.dump({\"results\": results, \"mean_ap\": mean_ap}, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:30:02.725403Z","iopub.execute_input":"2024-12-12T09:30:02.725816Z","iopub.status.idle":"2024-12-12T09:30:02.736543Z","shell.execute_reply.started":"2024-12-12T09:30:02.725778Z","shell.execute_reply":"2024-12-12T09:30:02.735276Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"process_images_with_map('/kaggle/input/regions/proposals.json', coco_json_pth, actproposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:30:06.500259Z","iopub.execute_input":"2024-12-12T09:30:06.501305Z","iopub.status.idle":"2024-12-12T09:30:15.319683Z","shell.execute_reply.started":"2024-12-12T09:30:06.501247Z","shell.execute_reply":"2024-12-12T09:30:15.318090Z"}},"outputs":[{"name":"stderr","text":"Processing Images:   0%|          | 0/32199 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_images_with_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/regions/proposals.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoco_json_pth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactproposals_json\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 45\u001b[0m, in \u001b[0;36mprocess_images_with_map\u001b[0;34m(region_file, annotation_file, output_file)\u001b[0m\n\u001b[1;32m     42\u001b[0m map_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(annotations) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(annotations) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Associa le etichette alle proposte\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m labeled_proposals \u001b[38;5;241m=\u001b[39m \u001b[43massign_labels_with_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Aggiungi al risultato\u001b[39;00m\n\u001b[1;32m     48\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m: image_id,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m: image_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabeled_proposals\u001b[39m\u001b[38;5;124m'\u001b[39m: labeled_proposals\n\u001b[1;32m     52\u001b[0m })\n","Cell \u001b[0;32mIn[49], line 23\u001b[0m, in \u001b[0;36massign_labels_with_map\u001b[0;34m(proposals, annotations, map_threshold, categories)\u001b[0m\n\u001b[1;32m     21\u001b[0m bbox \u001b[38;5;241m=\u001b[39m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m annotation_bbox_x \u001b[38;5;241m=\u001b[39m [bbox[\u001b[38;5;241m0\u001b[39m], bbox[\u001b[38;5;241m1\u001b[39m], bbox[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m bbox[\u001b[38;5;241m2\u001b[39m], bbox[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m bbox[\u001b[38;5;241m3\u001b[39m]]  \u001b[38;5;66;03m# Conversione in [x, y, x+w, y+h]\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m annotation_bbox \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m annotation_bbox_x]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calcola l'IoU\u001b[39;00m\n\u001b[1;32m     26\u001b[0m iou \u001b[38;5;241m=\u001b[39m calculate_iou(proposal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m], annotation_bbox)\n","Cell \u001b[0;32mIn[49], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m bbox \u001b[38;5;241m=\u001b[39m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m annotation_bbox_x \u001b[38;5;241m=\u001b[39m [bbox[\u001b[38;5;241m0\u001b[39m], bbox[\u001b[38;5;241m1\u001b[39m], bbox[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m bbox[\u001b[38;5;241m2\u001b[39m], bbox[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m bbox[\u001b[38;5;241m3\u001b[39m]]  \u001b[38;5;66;03m# Conversione in [x, y, x+w, y+h]\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m annotation_bbox \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m annotation_bbox_x]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calcola l'IoU\u001b[39;00m\n\u001b[1;32m     26\u001b[0m iou \u001b[38;5;241m=\u001b[39m calculate_iou(proposal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m], annotation_bbox)\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: '['"],"ename":"ValueError","evalue":"could not convert string to float: '['","output_type":"error"}],"execution_count":51},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"# Percorso del file JSON e del percorso base\ninput_json_path = \"/kaggle/input/activeregion-xviewdataset/activeregion-xview-dataset/active_regions.json\"\n\n# Carica il dataset dal JSON\nwith open(input_json_path, \"r\") as f:\n    data = json.load(f)\n\n# Converti in DataFrame per una gestione più comoda\ndf = pd.DataFrame(data)\n\n# Estrai il nome del file dal campo 'saved_path'\ndf[\"file_name\"] = df[\"saved_path\"].apply(lambda x: os.path.basename(x))\n\n# Aggiungi il percorso base al campo 'saved_path'\ndf[\"saved_path\"] = df[\"file_name\"].apply(lambda x: str(act_reg_folder / x))\n\n# Estrai i dati e le etichette\nX = df.index  # Indici delle righe\ny = df[\"category_id\"]  # Etichetta per stratificazione\n\n# Step 1: Train + Val/Test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Step 2: Val/Test split\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\n# Creazione dei dataset finali\ntrain_data = df.loc[X_train]\nval_data = df.loc[X_val]\ntest_data = df.loc[X_test]\n\n# Salva i dataset in nuovi file JSON\ntrain_data.to_json(\"train.json\", orient=\"records\", lines=False)\nval_data.to_json(\"val.json\", orient=\"records\", lines=False)\ntest_data.to_json(\"test.json\", orient=\"records\", lines=False)\n\nprint(\"Splitting completato. File salvati: train.json, val.json, test.json.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:09.455287Z","iopub.execute_input":"2024-12-09T15:55:09.456141Z","iopub.status.idle":"2024-12-09T15:55:12.584134Z","shell.execute_reply.started":"2024-12-09T15:55:09.456087Z","shell.execute_reply":"2024-12-09T15:55:12.583216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        with open(json_file, 'r') as f:\n            self.data = json.load(f)  # Carica il file JSON\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor()                 # Converte l'immagine in un tensore\n            #transforms.Normalize(                  # Normalizzazione con valori di ImageNet\n            #    mean=[0.485, 0.456, 0.406], \n            #    std=[0.229, 0.224, 0.225]\n            #)\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:17.533632Z","iopub.execute_input":"2024-12-09T15:55:17.534111Z","iopub.status.idle":"2024-12-09T15:55:17.541627Z","shell.execute_reply.started":"2024-12-09T15:55:17.534070Z","shell.execute_reply":"2024-12-09T15:55:17.540448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds = CustomDataset(test_path)\ntrain_ds = CustomDataset(train_path)\nval_ds = CustomDataset(val_path)\n\nTrainLoader = DataLoader(train_ds, batch_size=64, shuffle=True)\nValLoader = DataLoader(val_ds, batch_size=64, shuffle=False)\nTestLoader = DataLoader(test_ds, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:19.887131Z","iopub.execute_input":"2024-12-09T15:55:19.887466Z","iopub.status.idle":"2024-12-09T15:55:20.956676Z","shell.execute_reply.started":"2024-12-09T15:55:19.887437Z","shell.execute_reply":"2024-12-09T15:55:20.955687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)  \n        #final = func.sigmoid(self.fn10(feature))\n        final = self.fn10(feature)\n\n        return feature, final","metadata":{"execution":{"iopub.status.busy":"2024-12-09T16:57:42.091423Z","iopub.execute_input":"2024-12-09T16:57:42.092167Z","iopub.status.idle":"2024-12-09T16:57:42.101018Z","shell.execute_reply.started":"2024-12-09T16:57:42.092099Z","shell.execute_reply":"2024-12-09T16:57:42.099977Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12 #11 classi + sfondo\nnet = AlexNet(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:28:06.806844Z","iopub.execute_input":"2024-12-09T17:28:06.807183Z","iopub.status.idle":"2024-12-09T17:28:07.304442Z","shell.execute_reply.started":"2024-12-09T17:28:06.807152Z","shell.execute_reply":"2024-12-09T17:28:07.303636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:28:08.636863Z","iopub.execute_input":"2024-12-09T17:28:08.637481Z","iopub.status.idle":"2024-12-09T17:28:08.642476Z","shell.execute_reply.started":"2024-12-09T17:28:08.637448Z","shell.execute_reply":"2024-12-09T17:28:08.641470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_loss(train_losses, val_losses):\n    \"\"\"\n    Funzione per fare il plot della funzione di loss durante il training e la validazione.\n\n    :param train_losses: Lista delle perdite durante il training.\n    :param val_losses: Lista delle perdite durante la validazione.\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle='-', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\", linestyle='-', marker='x')\n    \n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:10:14.961027Z","iopub.execute_input":"2024-12-09T17:10:14.961731Z","iopub.status.idle":"2024-12-09T17:10:14.967187Z","shell.execute_reply.started":"2024-12-09T17:10:14.961693Z","shell.execute_reply":"2024-12-09T17:10:14.966232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_class_weights(train_loader, num_classes=12, background_weight=2.0):\n    \"\"\"\n    Calcola i pesi per ciascuna classe nel dataset in base alla frequenza delle etichette,\n    includendo un peso per la classe \"sfondo\" (ID=11) che potrebbe non essere presente nei dati.\n\n    :param train_loader: DataLoader per il training set.\n    :param num_classes: Numero totale di classi nel dataset (incluso lo sfondo).\n    :param background_weight: Peso assegnato alla classe \"sfondo\" (ID=11).\n    :return: Tensor dei pesi per ciascuna classe.\n    \"\"\"\n    # Estrai tutte le etichette dal dataset nel train_loader\n    all_labels = []\n    for _, labels in tqdm(train_loader, desc=\"Estrazione etichette\", leave=False):\n        all_labels.extend(labels.numpy())  # Estrae le etichette dalle immagini\n\n    # Conta le occorrenze di ciascuna classe\n    class_counts = Counter(all_labels)\n    \n    # Calcola il peso per ogni classe (inversamente proporzionale alla frequenza)\n    total_samples = len(all_labels)\n    class_weights = {class_id: total_samples / count for class_id, count in class_counts.items()}\n    \n    # Normalizza i pesi (in modo che il peso massimo sia 1)\n    max_weight = max(class_weights.values())\n    normalized_weights = {class_id: weight / max_weight for class_id, weight in class_weights.items()}\n    \n    # Aggiungi un peso elevato per la classe sfondo (ID=11)\n    normalized_weights[11] = background_weight  # Assegna un peso maggiore alla classe sfondo\n    \n    # Crea un tensor dei pesi, dove ogni peso corrisponde alla classe\n    weights = torch.tensor([normalized_weights.get(i, 1.0) for i in range(num_classes)], dtype=torch.float32)\n    \n    return weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:22:47.953332Z","iopub.execute_input":"2024-12-09T18:22:47.953714Z","iopub.status.idle":"2024-12-09T18:22:47.960758Z","shell.execute_reply.started":"2024-12-09T18:22:47.953681Z","shell.execute_reply":"2024-12-09T18:22:47.959857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss, class_weights):\n    \"\"\"\n    Funzione per addestrare il modello con pesi per classi sbilanciate.\n\n    :param net: Modello da addestrare.\n    :param train_loader: DataLoader per il training set.\n    :param val_loader: DataLoader per il validation set.\n    :param criterion: Funzione di loss.\n    :param optimizer: Ottimizzatore.\n    :param device: Dispositivo (CPU o GPU).\n    :param epochs: Numero di epoche di training.\n    :param path_min_loss: Percorso per salvare il modello con la minore loss di validazione.\n    \"\"\"\n    min_val_loss = float('inf')\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n\n    # Liste per registrare le perdite durante il training e la validazione\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        net.train()  # Modalità training\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        # Barra di avanzamento per il training\n        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n\n        for images, labels in train_progress:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss pesata\n            loss = criterion(outputs, labels)\n\n            # Backward pass e aggiornamento pesi\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Statistiche\n            train_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_train += labels.size(0)\n            correct_train += predicted.eq(labels).sum().item()\n\n            # Aggiorna la barra di avanzamento con la loss corrente\n            train_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_train / total_train)\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n        train_accuracy = 100. * correct_train / total_train\n        train_losses.append(avg_train_loss)  # Aggiungi il valore di train_loss\n\n        # Validazione\n        net.eval()  # Modalità validazione\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        # Barra di avanzamento per la validazione\n        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", leave=False)\n\n        with torch.no_grad():\n            for images, labels in val_progress:\n                images, labels = images.to(device), labels.to(device)\n\n                # Forward pass\n                _, outputs = net(images)\n\n                # Calcolo della loss\n                loss = criterion(outputs, labels)\n\n                # Statistiche\n                val_loss += loss.item() * images.size(0)\n                _, predicted = outputs.max(1)\n                total_val += labels.size(0)\n                correct_val += predicted.eq(labels).sum().item()\n\n                # Aggiorna la barra di avanzamento con la loss e accuracy\n                val_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_val / total_val)\n\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = 100. * correct_val / total_val\n        val_losses.append(avg_val_loss)  # Aggiungi il valore di val_loss\n\n        # Salva il modello con la loss di validazione più bassa\n        if avg_val_loss < min_val_loss:\n            print(f\"Salvataggio del miglior modello: Val Loss migliorata da {min_val_loss:.4f} a {avg_val_loss:.4f}\")\n            min_val_loss = avg_val_loss\n            torch.save(net.state_dict(), path_min_loss)\n\n        # Stampa statistiche per epoca\n        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    print(\"Training completato!\")\n\n    # Chiamata alla funzione per tracciare il grafico\n    plot_loss(train_losses, val_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:37:08.760299Z","iopub.execute_input":"2024-12-09T17:37:08.761075Z","iopub.status.idle":"2024-12-09T17:37:08.772940Z","shell.execute_reply.started":"2024-12-09T17:37:08.761036Z","shell.execute_reply":"2024-12-09T17:37:08.771964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss):\n    \"\"\"\n    Funzione per testare il modello.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    \"\"\"\n    # Carica il miglior modello salvato\n    net.load_state_dict(torch.load(path_min_loss))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:55:35.602023Z","iopub.execute_input":"2024-12-09T15:55:35.602918Z","iopub.status.idle":"2024-12-09T15:55:35.609314Z","shell.execute_reply.started":"2024-12-09T15:55:35.602863Z","shell.execute_reply":"2024-12-09T15:55:35.608417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calcola i pesi per ogni classe dal train_loader\nclass_weights = compute_class_weights(TrainLoader)\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:43:26.664026Z","iopub.execute_input":"2024-12-09T17:43:26.664672Z","iopub.status.idle":"2024-12-09T17:50:17.385314Z","shell.execute_reply.started":"2024-12-09T17:43:26.664629Z","shell.execute_reply":"2024-12-09T17:50:17.384330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\nepochs = 10\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\ndevice = torch.device(\"cuda\")\n\nnet = net.to(device)\ncriterion = criterion.to(device)\n\npath_min_loss = '/kaggle/working/AlexNet.pth'\n\ntrain_model(\n    net=net,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    class_weights=class_weights\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:50:35.762331Z","iopub.execute_input":"2024-12-09T17:50:35.763129Z","iopub.status.idle":"2024-12-09T18:22:21.341423Z","shell.execute_reply.started":"2024-12-09T17:50:35.763092Z","shell.execute_reply":"2024-12-09T18:22:21.339728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=net,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:23:05.370465Z","iopub.execute_input":"2024-12-09T18:23:05.370839Z","iopub.status.idle":"2024-12-09T18:25:40.418621Z","shell.execute_reply.started":"2024-12-09T18:23:05.370807Z","shell.execute_reply":"2024-12-09T18:25:40.417655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    \"\"\"\n    Funzione per calcolare e visualizzare la matrice di confusione.\n\n    :param y_true: Etichette reali\n    :param y_pred: Etichette predette dal modello\n    :param class_names: Nomi delle classi\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted labels')\n    plt.ylabel('True labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n# Durante la fase di validazione o test, calcola la matrice di confusione\ndef validate_model(net, val_loader, device, class_names):\n    net.eval()  # Modalità di valutazione\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Forward pass\n            _, outputs = net(images)\n            \n            # Predizioni\n            _, predicted = outputs.max(1)\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n    \n    # Visualizza la matrice di confusione\n    plot_confusion_matrix(all_labels, all_preds, class_names)\n\n# Esempio di come chiamare la funzione\nclass_names = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3', 'Classe 4', 'Classe 5', 'Classe 6', 'Classe 7', 'Classe 8', 'Classe 9', 'Classe 10']\nvalidate_model(net, ValLoader, device, class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:27:40.856352Z","iopub.execute_input":"2024-12-09T18:27:40.857110Z","iopub.status.idle":"2024-12-09T18:28:34.851618Z","shell.execute_reply.started":"2024-12-09T18:27:40.857061Z","shell.execute_reply":"2024-12-09T18:28:34.850651Z"}},"outputs":[],"execution_count":null}]}