{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport selectivesearch\n\nclass CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n\n        self.img_dir = img_dir\n\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n\n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox_str = annotation['bbox']\n            bbox = list(map(float, bbox_str.strip('[]').split(', ')))\n\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug = aug\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Image {img_name} not found in the COCO file.\")\n    \n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Image not found at path: {img_path}\")\n        \n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Scale the bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        categories = [c for c in categories if isinstance(c, int)]\n        if not categories:\n            categories = [-1]  # Special label for images without annotations\n        \n        scale_x = 320 / original_width\n        scale_y = 320 / original_height\n        scaled_bboxes = [\n            torch.tensor([\n                bbox[0] * scale_x,  # x_min\n                bbox[1] * scale_y,  # y_min\n                bbox[2] * scale_x,  # x_max\n                bbox[3] * scale_y   # y_max\n            ], dtype=torch.float32)\n            for bbox in bboxes\n        ] if bboxes else [torch.zeros(4, dtype=torch.float32)]\n        \n        labels = torch.tensor(categories, dtype=torch.int64)\n\n        # Generate and process region proposals\n        proposals = self._generate_region_proposals(image)\n        proposals = self._filter_proposals(proposals, original_width, original_height)\n        processed_proposals = self._process_proposals(image_tensor, proposals)\n\n        return {\n            \"image\": image_tensor,               # Scaled input image\n            \"labels\": labels,                    # Object class labels\n            \"bboxes\": scaled_bboxes,             # Scaled bounding boxes\n            \"regions\": processed_proposals       # Processed region proposals\n        }\n\n    def _generate_region_proposals(self, image):\n        img_np = np.array(image)\n        \n        if len(img_np.shape) == 3 and img_np.shape[0] == 3:\n            img_np = np.transpose(img_np, (1, 2, 0))  # Convert [C, H, W] to [H, W, C]\n        elif len(img_np.shape) == 2:\n            img_np = np.stack([img_np] * 3, axis=-1)  # Grayscale to RGB\n        elif img_np.shape[2] < 3:\n            img_np = np.repeat(img_np, 3, axis=2)  # Single-channel to 3-channel\n        elif img_np.shape[2] != 3:\n            raise ValueError(f\"Invalid image shape: {img_np.shape}\")\n        \n        _, regions = selectivesearch.selective_search(img_np, scale=500, sigma=0.9, min_size=10)\n        proposals = []\n        for region in regions:\n            x, y, w, h = region['rect']\n            if w > 0 and h > 0 and w >= 10 and h >= 10:\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0])\n                proposals.append([x, y, x_max, y_max])\n        \n        return proposals\n\n    def _filter_proposals(self, proposals, img_width, img_height, min_area=100, max_area_ratio=0.8):\n        unique_proposals = set(tuple(p) for p in proposals)\n        filtered = []\n        for x_min, y_min, x_max, y_max in unique_proposals:\n            width = x_max - x_min\n            height = y_max - y_min\n            area = width * height\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\n                filtered.append((x_min, y_min, x_max, y_max))\n        return filtered\n\n    def _process_proposals(self, image_tensor, proposals, output_size=(227, 227)):\n        processed_proposals = []\n        for proposal in proposals:\n            try:\n                _, H, W = image_tensor.shape\n                x_min, y_min, x_max, y_max = map(int, proposal)\n                x_min, y_min = max(0, x_min), max(0, y_min)\n                x_max, y_max = min(W, x_max), min(H, y_max)\n    \n                if x_min < x_max and y_min < y_max:\n                    cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]\n                    if cropped_region.numel() == 0 or cropped_region.ndim != 3:\n                        continue\n                    \n                    resized_region = torch.nn.functional.interpolate(\n                        cropped_region.unsqueeze(0), size=output_size, mode='bilinear', align_corners=False\n                    ).squeeze(0)\n                    processed_proposals.append(resized_region)\n            except Exception as e:\n                print(f\"Error processing proposal: {proposal}. Error: {e}\")\n    \n        return processed_proposals","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"markdown","source":"# SPPNet (ZF-5)","metadata":{}},{"cell_type":"code","source":"# Define the SPP Layer\nclass SpatialPyramidPooling(nn.Module):\n    def __init__(self, output_sizes):\n        \"\"\"\n        :param output_sizes: List of output sizes for pyramid levels (e.g., [1, 2, 4]).\n        \"\"\"\n        super(SpatialPyramidPooling, self).__init__()\n        self.output_sizes = output_sizes\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        pooled_outputs = []\n        for output_size in self.output_sizes:\n            kernel_size = (height // output_size, width // output_size)\n            stride = kernel_size\n            padding = (height % output_size // 2, width % output_size // 2)\n            pooled = F.adaptive_max_pool2d(x, output_size)\n            pooled_outputs.append(pooled.view(batch_size, -1))  # Flatten each level\n        return torch.cat(pooled_outputs, dim=1)  # Concatenate pyramid levels\n\n# Define the ZFNet Backbone with SPP Layer\nclass SPPNetZF5(nn.Module):\n    def __init__(self, num_classes=1000, spp_output_sizes=[1, 2, 4]):\n        super(SPPNetZF5, self).__init__()\n        # ZFNet convolutional layers\n        self.conv1 = nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=2, padding=2)\n        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n\n        self.spp = SpatialPyramidPooling(output_sizes=spp_output_sizes)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(self._calculate_fc_input_size(spp_output_sizes), 4096)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, num_classes)\n\n    def _calculate_fc_input_size(self, spp_output_sizes):\n        \"\"\"\n        Calculate the total size of the output vector from the SPP layer.\n        \"\"\"\n        return sum([size * size for size in spp_output_sizes]) * 256\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = self.spp(x)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.fc3(x)\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, device, optimizer, criterion, train_loader, val_loader):\n        \"\"\"\n        :param model: The SPPNet model.\n        :param device: The device (CPU or GPU).\n        :param optimizer: Optimizer for training.\n        :param criterion: Loss function.\n        :param train_loader: DataLoader for training.\n        :param val_loader: DataLoader for validation.\n        \"\"\"\n        self.model = model.to(device)\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n\n            # Forward\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n\n            # Backward\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # Metrics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            if batch_idx % 10 == 0:\n                print(\n                    f'Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, '\n                    f'Loss: {loss.item():.4f}, Accuracy: {100. * correct / total:.2f}%'\n                )\n\n        train_loss = running_loss / len(self.train_loader)\n        train_accuracy = 100. * correct / total\n        return train_loss, train_accuracy\n\n    def validate(self):\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, targets in self.val_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n\n                # Forward\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n\n                # Metrics\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n        val_loss = running_loss / len(self.val_loader)\n        val_accuracy = 100. * correct / total\n        return val_loss, val_accuracy\n\n    def fit(self, epochs):\n        for epoch in range(epochs):\n            train_loss, train_accuracy = self.train_one_epoch(epoch)\n            val_loss, val_accuracy = self.validate()\n\n            print(\n                f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n                f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%'\n            )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}