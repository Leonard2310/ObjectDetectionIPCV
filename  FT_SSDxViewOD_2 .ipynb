{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import delle librerie","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport random\nimport xml.etree.ElementTree as ET\nimport torchvision.transforms.functional as FT\n\nimport torch\nfrom tqdm import tqdm\nfrom pprint import PrettyPrinter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.086351Z","iopub.execute_input":"2024-12-11T15:01:52.086755Z","iopub.status.idle":"2024-12-11T15:01:52.092662Z","shell.execute_reply.started":"2024-12-11T15:01:52.086719Z","shell.execute_reply":"2024-12-11T15:01:52.091487Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Path","metadata":{}},{"cell_type":"code","source":"# path del dataset\nbase_dict = '/kaggle/input/our-xview-dataset'\n\n# path della cartella contenente le immagini\nimg_dict = '/kaggle/input/our-xview-dataset/images'\n\n# path dei file .txt da utilizzare per prelevare rispettivamente le immagini per il train, la validation e il test\n# MANCANO -> servono per provare il codice\ntrain_img_path = '/kaggle/input/our-xview-dataset/YOLO_cfg/train.txt' # file contenete i path delle immagini del dataset di train\nval_img_path = '/kaggle/input/our-xview-dataset/YOLO_cfg/val.txt'\ntest_img_path = '/kaggle/input/our-xview-dataset/YOLO_cfg/test.txt'\n\n# path contenente le annotazioni in formato .json\nannotations = os.path.join(base_dict, 'COCO_annotations_new.json') # non va bene il file !\nclass_map = os.path.join(base_dict, 'xView_class_map.json') \n\n# path di output\noutput_folder = '/kaggle/working/'\n\n# path file per il training\ntrain_image = os.path.join(output_folder, 'TRAIN_images.json')\ntrain_bbox = os.path.join(output_folder, 'TRAIN_objects.json')\ntrain_label = os.path.join(output_folder, 'TRAIN_label_map.json')\n\n# path file per la validation\nval_image = os.path.join(output_folder, 'VAL_images.json')\nval_bbox = os.path.join(output_folder, 'VAL_objects.json')\nval_label = os.path.join(output_folder, 'VAL_label_map.json')\n\n# path file per il test\ntest_image = os.path.join(output_folder, 'TEST_images.json')\ntest_bbox = os.path.join(output_folder, 'TEST_objects.json')\ntest_label = os.path.join(output_folder, 'TEST_label_map.json')\n\ncheckpoint_path = './checkpoint_ssd300.pth.tar'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.098451Z","iopub.execute_input":"2024-12-11T15:01:52.098910Z","iopub.status.idle":"2024-12-11T15:01:52.110694Z","shell.execute_reply.started":"2024-12-11T15:01:52.098862Z","shell.execute_reply":"2024-12-11T15:01:52.109216Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.112808Z","iopub.execute_input":"2024-12-11T15:01:52.113214Z","iopub.status.idle":"2024-12-11T15:01:52.132290Z","shell.execute_reply.started":"2024-12-11T15:01:52.113179Z","shell.execute_reply":"2024-12-11T15:01:52.130654Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Pre-elaborazione del dataset","metadata":{}},{"cell_type":"markdown","source":"### creazione del file per reperire gli elementi del train e della validation","metadata":{}},{"cell_type":"markdown","source":"def create_data_lists(img_dict, train_img_path, val_img_path, test_img_path, output_folder): # VA MODIFICATA IN BASE AL DATASET\n    \"\"\"\n    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n    - param output_folder: folder where the JSONs must be saved\n    \"\"\"\n    \n    # TRAIN\n    train_images = list() #lista per la memorizzazione di tutte le immagini presenti nel dataset di train\n    train_objects = list() # lista di bounding_boxes \n    train_label_map = label_map() #richiama la funzione label_map()\n    n_objects = 0 # numero di bounding boxes nel train\n\n    # Training data\n    # Find IDs of images in training data\n    with open(train_img_path) as f:\n        ids = f.read().splitlines()\n\n    for id in ids: #ricavo per ogni immagine nel set di training la lista di bbox con le corrispondenti labels\n        # VA CAMBIATO IN BASE AL DATASET\n        objects = parse_annotation(os.path.join(img_dict, 'Annotations', id + '.xml')) # richiama la funzione parse annotation -> vedi dopo\n        if len(objects['boxes']) == 0: #verifico che ci siano bbox nell'immagine altrimenti passo alla prossima immagine\n            continue\n        n_objects += len(objects) \n        #inserisco i bbox e le labels nelle liste preposte\n        train_objects.append(objects)\n        train_images.append(os.path.join(img_dict, 'JPEGImages', id + '.jpg'))\n\n    assert len(train_objects) == len(train_images)\n\n    # Save to file\n    with open(train_image, 'w') as j:\n        json.dump(train_images, j)\n    with open(train_bbox, 'w') as j:\n        json.dump(train_objects, j)\n    with open(train_label, 'w') as j:\n        json.dump(train_label_map, j)  # save label map too\n\n    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n        len(train_images), n_objects, os.path.abspath(output_folder)))\n\n\n    # VALIDATION\n    val_images = list() #lista per la memorizzazione di tutte le immagini presenti nel dataset di train\n    val_objects = list() # lista di bounding_boxes \n    val_label_map = label_map() #richiama la funzione label_map()\n    n_objects = 0 # numero di bounding boxes nel train\n    \n    # Validation data\n    # Find IDs of images in training data\n    with open(val_img_path) as f:\n        ids = f.read().splitlines()\n\n    for id in ids: #ricavo per ogni immagine nel set di training la lista di bbox con le corrispondenti labels\n        # VA CAMBIATO IN BASE AL DATASET\n        objects = parse_annotation(os.path.join(img_dict, 'Annotations', id + '.xml'))\n        if len(objects['boxes']) == 0: #verifico che ci siano bbox nell'immagine altrimenti passo alla prossima immagine\n            continue\n        n_objects += len(objects) \n        #inserisco i bbox e le labels nelle liste preposte\n        val_objects.append(objects)\n        val_images.append(os.path.join(img_dict, 'JPEGImages', id + '.jpg'))\n\n    assert len(val_objects) == len(val_images)\n\n    # Save to file\n    with open(val_image, 'w') as j:\n        json.dump(val_images, j)\n    with open(val_bbox, 'w') as j:\n        json.dump(val_objects, j)\n    with open(val_label, 'w') as j:\n        json.dump(val_label_map, j)  # save label map too\n\n    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n        len(train_images), n_objects, os.path.abspath(output_folder)))\n\n\n    # TEST\n    test_images = list()\n    test_objects = list()\n    n_objects = 0\n\n    # Find IDs of images in the test data\n    with open(os.path.join(test_img_path, 'ImageSets/Main/test.txt')) as f:\n        ids = f.read().splitlines()\n\n    for id in ids:\n        # VA CAMBIATO IN BASE AL DATASET\n        objects = parse_annotation(os.path.join(test_img_path, 'Annotations', id + '.xml'))\n        if len(objects) == 0:\n            continue\n        test_objects.append(objects)\n        n_objects += len(objects)\n        test_images.append(os.path.join(test_img_path, 'JPEGImages', id + '.jpg'))\n\n    assert len(test_objects) == len(test_images)\n\n    # Save to file\n    with open(test_image, 'w') as j:\n        json.dump(test_images, j)\n    with open(test_bbox, 'w') as j:\n        json.dump(test_objects, j)\n\n    print('\\nThere are %d test images containing a total of %d objects. Files have been saved to %s.' % (\n        len(test_images), n_objects, os.path.abspath(output_folder)))\n","metadata":{}},{"cell_type":"code","source":"def create_data_lists(img_dict, annotations_path, train_img_path, val_img_path, test_img_path, output_folder):\n    \"\"\"\n    Create lists of images, bounding boxes, and labels, and save them to files.\n    - param img_dict: Directory containing the images.\n    - param annotations_path: Path to the JSON file with annotations.\n    - param train_img_path: Path to file with IDs of training images.\n    - param val_img_path: Path to file with IDs of validation images.\n    - param test_img_path: Path to file with IDs of test images.\n    - param output_folder: Folder where the JSONs must be saved.\n    \"\"\"\n\n    # Load the annotations JSON\n    with open(annotations_path, 'r') as f:\n        annotations_data = json.load(f)\n\n    def create_annotations_dict(data):\n        \"\"\"\n        Organizza le annotazioni in un dizionario basato sull'image_id.\n    \n        :param data: Dizionario contenente \"images\" e \"annotations\".\n        :return: Dizionario con image_id come chiave e lista di annotazioni come valore.\n        \"\"\"\n        # Verifica che il formato dei dati sia corretto\n        if \"annotations\" not in data or not isinstance(data[\"annotations\"], list):\n            raise ValueError(\"Il file JSON non contiene una chiave 'annotations' valida.\")\n        \n        # Creazione del dizionario organizzato per image_id\n        annotations_dict = {}\n        for annotation in data[\"annotations\"]:\n            image_id = annotation[\"image_id\"]  # Estrai l'image_id\n            if image_id not in annotations_dict:\n                annotations_dict[image_id] = []  # Inizializza una lista per il nuovo image_id\n            annotations_dict[image_id].append(annotation)  # Aggiungi l'annotazione alla lista\n    \n        return annotations_dict\n\n\n    # Convert annotations data into a dictionary for faster access\n    annotations_dict = {str(item['id']): item for item in annotations_data}\n    #annotations_for_image = annotations_dict.get(image_id, [])\n\n    def process_image_set(image_ids_path):\n        \"\"\"\n        Process a set of images (train, val, or test).\n        \"\"\"\n        images = []\n        objects = []\n        n_objects = 0\n\n        # Read image IDs\n        with open(image_ids_path, 'r') as f: # da modificare perchè i path sono nel formato /kaggle/working/images/img_886_2240_2240.jpg e non /kaggle/input/our-xview-dataset/images/img_100_0_0.jpg \n            image_id = f.read().splitlines()\n            image_ids = [line.strip().replace('/kaggle/working/images/', '/kaggle/input/our-xview-dataset/images/') for line in image_id]\n\n        for image_id in image_ids:\n            # Parse annotations for this image\n            image_id = image_id\n            annotation = parse_annotation(image_id, annotations_dict)\n\n            # Skip images without bounding boxes\n            if len(annotation['boxes']) == 0:\n                continue\n\n            # Add image path and annotations\n            file_name = annotations_dict[str(image_id)]['file_name']\n            images.append(os.path.join(img_dict, file_name))\n            objects.append(annotation)\n            n_objects += len(annotation['boxes'])\n\n        return images, objects, n_objects\n\n    # Process TRAINING data\n    train_images, train_objects, train_n_objects = process_image_set(train_img_path)\n    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n        json.dump(train_images, j)\n    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n        json.dump(train_objects, j)\n\n    print(f'\\nThere are {len(train_images)} training images containing a total of {train_n_objects} objects. '\n          f'Files have been saved to {os.path.abspath(output_folder)}.')\n\n    # Process VALIDATION data\n    val_images, val_objects, val_n_objects = process_image_set(val_img_path)\n    with open(os.path.join(output_folder, 'VAL_images.json'), 'w') as j:\n        json.dump(val_images, j)\n    with open(os.path.join(output_folder, 'VAL_objects.json'), 'w') as j:\n        json.dump(val_objects, j)\n\n    print(f'\\nThere are {len(val_images)} validation images containing a total of {val_n_objects} objects. '\n          f'Files have been saved to {os.path.abspath(output_folder)}.')\n\n    # Process TEST data\n    test_images, test_objects, test_n_objects = process_image_set(test_img_path)\n    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n        json.dump(test_images, j)\n    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n        json.dump(test_objects, j)\n\n    print(f'\\nThere are {len(test_images)} test images containing a total of {test_n_objects} objects. '\n          f'Files have been saved to {os.path.abspath(output_folder)}.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.134435Z","iopub.execute_input":"2024-12-11T15:01:52.134939Z","iopub.status.idle":"2024-12-11T15:01:52.154940Z","shell.execute_reply.started":"2024-12-11T15:01:52.134886Z","shell.execute_reply":"2024-12-11T15:01:52.153678Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Conversione label/classi numeriche e viceversa","metadata":{}},{"cell_type":"markdown","source":"### associazione colori differenti ai bounding box relativi a classi differenti","metadata":{}},{"cell_type":"code","source":"def label_map():\n    \"\"\"\n    Create a label map for the dataset, mapping class names to unique numeric identifiers.\n    \"\"\"\n    labels = [\n        \"Fixed-wing Aircraft\", \"Small Aircraft\", \"Passenger/Cargo Plane\", \"Helicopter\",\n        \"Passenger Vehicle\", \"Small Car\", \"Bus\", \"Pickup Truck\", \"Utility Truck\", \"Truck\",\n        \"Cargo Truck\", \"Truck Tractor w/ Box Trailer\", \"Truck Tractor\", \"Trailer\",\n        \"Truck Tractor w/ Flatbed Trailer\", \"Truck Tractor w/ Liquid Tank\", \"Crane Truck\",\n        \"Railway Vehicle\", \"Passenger Car\", \"Cargo/Container Car\", \"Flat Car\", \"Tank car\",\n        \"Locomotive\", \"Maritime Vessel\", \"Motorboat\", \"Sailboat\", \"Tugboat\", \"Barge\",\n        \"Fishing Vessel\", \"Ferry\", \"Yacht\", \"Container Ship\", \"Oil Tanker\", \"Engineering Vehicle\",\n        \"Tower crane\", \"Container Crane\", \"Reach Stacker\", \"Straddle Carrier\", \"Mobile Crane\",\n        \"Dump Truck\", \"Haul Truck\", \"Scraper/Tractor\", \"Front loader/Bulldozer\", \"Excavator\",\n        \"Cement Mixer\", \"Ground Grader\", \"Hut/Tent\", \"Shed\", \"Building\", \"Aircraft Hangar\",\n        \"Damaged Building\", \"Facility\", \"Construction Site\", \"Vehicle Lot\", \"Helipad\",\n        \"Storage Tank\", \"Shipping container lot\", \"Shipping Container\", \"Pylon\", \"Tower\"\n    ]\n    \n    # Generate a mapping from label names to unique numeric identifiers\n    return {label: idx + 1 for idx, label in enumerate(labels)}\n\n\ndef rev_label_msp():\n    label_map['background'] = 0 # classe dello sfondo\n    rev_label_map = {v: k for k, v in label_map.items()}  # ricavo le lable dalle etichette numeriche\n    return rev_label_map\n\ndef label_color_map():\n    # Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n    distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n                   '#d2f53c', '#fabebe', '#008080', '#000080']\n    label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n    return label_color_map\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.156899Z","iopub.execute_input":"2024-12-11T15:01:52.157427Z","iopub.status.idle":"2024-12-11T15:01:52.177020Z","shell.execute_reply.started":"2024-12-11T15:01:52.157387Z","shell.execute_reply":"2024-12-11T15:01:52.175889Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## Pre-elaborazione delle immagini","metadata":{}},{"cell_type":"code","source":"def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n    \"\"\"\n    Resize image. For the SSD300, resize to (300, 300).\n\n    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n    you may choose to retain them.\n\n    :param image: image, a PIL Image\n    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n    \"\"\"\n    # Resize image\n    new_image = FT.resize(image, dims)\n\n    # Resize bounding boxes\n    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n    new_boxes = boxes / old_dims  # percent coordinates\n\n    if not return_percent_coords:\n        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n        new_boxes = new_boxes * new_dims\n\n    return new_image, new_boxes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.341410Z","iopub.execute_input":"2024-12-11T15:01:52.341829Z","iopub.status.idle":"2024-12-11T15:01:52.349303Z","shell.execute_reply.started":"2024-12-11T15:01:52.341790Z","shell.execute_reply":"2024-12-11T15:01:52.348116Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def transform(image, boxes, labels):\n    \"\"\"\n    Apply the transformations above.\n\n    :param image: image, a PIL Image\n    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n    :param labels: labels of objects, a tensor of dimensions (n_objects)\n    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n    :param split: one of 'TRAIN' or 'TEST', since different sets of transformations are applied\n    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n    \"\"\"\n\n    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n    # see: https://pytorch.org/docs/stable/torchvision/models.html\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n\n    new_image = image\n    new_boxes = boxes\n    new_labels = labels\n\n    # Resize image to (300, 300) - this also converts absolute boundary coordinates to their fractional form\n    new_image, new_boxes = resize(new_image, new_boxes, dims=(300, 300))\n\n    # Convert PIL image to Torch tensor\n    new_image = FT.to_tensor(new_image)\n\n    # Normalize by mean and standard deviation of ImageNet data that our base VGG was trained on\n    new_image = FT.normalize(new_image, mean=mean, std=std)\n\n    return new_image, new_boxes, new_labels\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.351050Z","iopub.execute_input":"2024-12-11T15:01:52.351508Z","iopub.status.idle":"2024-12-11T15:01:52.368783Z","shell.execute_reply.started":"2024-12-11T15:01:52.351457Z","shell.execute_reply":"2024-12-11T15:01:52.367335Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## Gestione dei bounding boxes","metadata":{}},{"cell_type":"code","source":"def xy_to_cxcy(xy):\n    \"\"\"\n    Convert bounding boxes from boundary coordinates (x_min, y_min, x_max, y_max) to center-size coordinates (c_x, c_y, w, h).\n\n    :param xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n    :return: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n    \"\"\"\n    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n\n\ndef cxcy_to_xy(cxcy):\n    \"\"\"\n    Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max).\n\n    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n    :return: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n    \"\"\"\n    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.370402Z","iopub.execute_input":"2024-12-11T15:01:52.370861Z","iopub.status.idle":"2024-12-11T15:01:52.387415Z","shell.execute_reply.started":"2024-12-11T15:01:52.370745Z","shell.execute_reply":"2024-12-11T15:01:52.386095Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n    \"\"\"\n    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n\n    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n\n    In the model, we are predicting bounding box coordinates in this encoded form.\n\n    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n    \"\"\"\n\n    # The 10 and 5 below are referred to as 'variances' in the original Caffe repo, completely empirical\n    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n    # See https://github.com/weiliu89/caffe/issues/155\n    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n\n\ndef gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n    \"\"\"\n    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n\n    They are decoded into center-size coordinates.\n\n    This is the inverse of the function above.\n\n    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n    \"\"\"\n\n    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.389864Z","iopub.execute_input":"2024-12-11T15:01:52.390342Z","iopub.status.idle":"2024-12-11T15:01:52.405791Z","shell.execute_reply.started":"2024-12-11T15:01:52.390306Z","shell.execute_reply":"2024-12-11T15:01:52.404395Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def parse_annotation(image_file_name, data):\n    \"\"\"\n    Extract bounding boxes and labels for a given image file name from JSON annotations.\n\n    :param image_file_name: Path or name of the image file (e.g., \"/path/to/img_2355_0_640.jpg\").\n    :param data: Dictionary containing both 'images' and 'annotations'.\n    :return: Dictionary with 'boxes' and 'labels' for the given image file name.\n    \"\"\"\n    # Step 1: Extract the base name of the file (remove path)\n    base_name = os.path.basename(image_file_name)\n\n    # Step 2: Convert the base name to corresponding image ID\n    # Remove \"img_\" and \"_\" to create a numeric string, then convert to integer\n    try:\n        image_id = int(base_name.replace(\"img_\", \"\").replace(\"_\", \"\").replace(\".jpg\", \"\"))\n    except ValueError:\n        raise ValueError(f\"Invalid image file name format: {image_file_name}\")\n\n    #print(image_id)\n\n    # Step 3: Find the corresponding image entry in 'images'\n    image_entry = None\n    for image in data.get(\"images\", []):\n        if image[\"id\"] == image_id:\n            image_entry = image\n            print(image)\n            break\n\n    #print(image_entry)\n\n    if not image_entry:\n        return {'boxes': [], 'labels': []}  # No matching image found\n\n    # Step 4: Find all annotations linked to the image ID\n    annotations = [\n        ann for ann in data.get(\"annotations\", [])\n        if ann[\"image_id\"] == image_id\n    ]\n\n    print(annotations)\n\n    if not annotations:\n        return {'boxes': [], 'labels': []}  # No annotations for this image\n\n    # Step 5: Extract bounding boxes and category labels\n    boxes = [eval(ann[\"bbox\"]) for ann in annotations]  # Use eval to convert string to list\n    labels = [ann[\"category_id\"] for ann in annotations]\n\n    return {'boxes': boxes, 'labels': labels}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.407336Z","iopub.execute_input":"2024-12-11T15:01:52.407789Z","iopub.status.idle":"2024-12-11T15:01:52.426540Z","shell.execute_reply.started":"2024-12-11T15:01:52.407737Z","shell.execute_reply":"2024-12-11T15:01:52.425175Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"create_data_lists(img_dict, annotations, train_img_path, val_img_path, test_img_path, output_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:52.428081Z","iopub.execute_input":"2024-12-11T15:01:52.428462Z","iopub.status.idle":"2024-12-11T15:01:54.962080Z","shell.execute_reply.started":"2024-12-11T15:01:52.428427Z","shell.execute_reply":"2024-12-11T15:01:54.959947Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_data_lists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_img_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_img_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_img_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[33], line 39\u001b[0m, in \u001b[0;36mcreate_data_lists\u001b[0;34m(img_dict, annotations_path, train_img_path, val_img_path, test_img_path, output_folder)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m annotations_dict\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Convert annotations data into a dictionary for faster access\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m annotations_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]): item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m annotations_data}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image_set\u001b[39m(image_ids_path):\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    Process a set of images (train, val, or test).\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","Cell \u001b[0;32mIn[33], line 39\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m annotations_dict\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Convert annotations data into a dictionary for faster access\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m annotations_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m): item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m annotations_data}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image_set\u001b[39m(image_ids_path):\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    Process a set of images (train, val, or test).\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: string indices must be integers"],"ename":"TypeError","evalue":"string indices must be integers","output_type":"error"}],"execution_count":40},{"cell_type":"markdown","source":"### per la gestione dell'interazione tra più bounding boxes","metadata":{}},{"cell_type":"code","source":"def find_intersection(set_1, set_2):\n    \"\"\"\n    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\n\ndef find_jaccard_overlap(set_1, set_2):\n    \"\"\"\n    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # Find intersections\n    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.962914Z","iopub.status.idle":"2024-12-11T15:01:54.963359Z","shell.execute_reply.started":"2024-12-11T15:01:54.963166Z","shell.execute_reply":"2024-12-11T15:01:54.963187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset): # da modificare in base al dataset\n    \"\"\"\n    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n    \"\"\"\n\n    def __init__(self, path_image, path_bbox, aug=False):\n        \"\"\"\n        :param data_folder: folder where data files are stored\n        :param split: split, one of 'TRAIN' or 'TEST'\n        :param keep_difficult: keep or discard objects that are considered difficult to detect?\n        \"\"\"\n\n        self.data_folder = data_folder\n        self.aug = aug # è presente ma in realtà non serve perchè applico lo stesso tipo di trasformazione per tutti i set di dati\n\n        # Read data files\n        with open(path_image, 'r') as j:\n            self.images = json.load(j)\n        with open(path_bbox, 'r') as j:\n            self.objects = json.load(j)\n\n        assert len(self.images) == len(self.objects)\n\n    def __getitem__(self, i):\n        # Read image\n        image = Image.open(self.images[i], mode='r') \n        image = image.convert('RGB')\n\n        # Read objects in this image (bounding boxes, labels, difficulties)\n        objects = self.objects[i] # dal file bbox rixhavo i bounding box presenti nell'immagine con le relative label\n        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n\n        boxes = boxes[1 - difficulties]\n        labels = labels[1 - difficulties]\n\n        # Apply transformations\n        image, boxes, labels = transform(image, boxes, labels)\n\n        return image, boxes, labels\n\n    def __len__(self):\n        return len(self.images)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.964841Z","iopub.status.idle":"2024-12-11T15:01:54.965433Z","shell.execute_reply.started":"2024-12-11T15:01:54.965148Z","shell.execute_reply":"2024-12-11T15:01:54.965186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(self, batch):\n        \"\"\"\n        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n\n        This describes how to combine these tensors of different sizes. We use lists.\n\n        Note: this need not be defined in this Class, can be standalone.\n\n        :param batch: an iterable of N sets from __getitem__()\n        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n        \"\"\"\n\n        images = list()\n        boxes = list()\n        labels = list()\n\n        for b in batch:\n            images.append(b[0])\n            boxes.append(b[1])\n            labels.append(b[2])\n\n        images = torch.stack(images, dim=0)\n\n        return images, boxes, labels  # tensor (N, 3, 300, 300), 3 lists of N tensors each","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.967229Z","iopub.status.idle":"2024-12-11T15:01:54.967755Z","shell.execute_reply.started":"2024-12-11T15:01:54.967495Z","shell.execute_reply":"2024-12-11T15:01:54.967521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CustomDataset(train_image, train_bbox)\nval_dataset = CustomDataset(val_image, val_bbox)\ntest_dataset = CustomDataset(test_image, test_bbox)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                                               collate_fn=train_dataset.collate_fn, num_workers=num_workers, pin_memory=True)\nval_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                                             collate_fn=train_dataset.collate_fn, num_workers=num_workers, pin_memory=True)\ntest_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                                              collate_fn=train_dataset.collate_fn, num_workers=num_workers, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.969334Z","iopub.status.idle":"2024-12-11T15:01:54.969706Z","shell.execute_reply.started":"2024-12-11T15:01:54.969530Z","shell.execute_reply":"2024-12-11T15:01:54.969547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"fare l'import da GitHub https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"def adjust_learning_rate(optimizer, scale):\n    \"\"\"\n    Scale learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param scale: factor to multiply learning rate with.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * scale\n    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n\n\ndef accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)\n\n\ndef save_checkpoint(epoch, model, optimizer):\n    \"\"\"\n    Save model checkpoint.\n\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model,\n             'optimizer': optimizer}\n    filename = 'checkpoint_ssd300.pth.tar'\n    torch.save(state, filename)\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_dataset, train_dataloader, criterion, optimizer, batch_size, num_workers, device, \n                 grad_clip=None, print_freq=10, iterations=120000, decay_lr_at=None, decay_lr_to=0.1, \n                 momentum=0.9, weight_decay=5e-4):\n        \"\"\"\n        Initialize the Trainer.\n        \n        :param model: SSD300 model instance\n        :param train_dataset: Dataset object\n        :param criterion: Loss function\n        :param optimizer: Optimizer\n        :param batch_size: Training batch size\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for training ('cuda' or 'cpu')\n        :param grad_clip: Gradient clipping value (default: None)\n        :param print_freq: Frequency of printing training progress\n        :param iterations: Total number of training iterations\n        :param decay_lr_at: Iterations to decay learning rate\n        :param decay_lr_to: Learning rate decay factor\n        :param momentum: Momentum for optimizer\n        :param weight_decay: Weight decay for optimizer\n        \"\"\"\n        self.model = model\n        self.train_dataset = train_dataset\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.grad_clip = grad_clip\n        self.print_freq = print_freq\n        self.iterations = iterations\n        self.decay_lr_at = decay_lr_at if decay_lr_at is not None else [80000, 100000]\n        self.decay_lr_to = decay_lr_to\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n\n        # Prepare dataloader\n        self.train_loader = train_dataloader\n\n        # Calculate epochs and decay epochs\n        self.epochs = iterations // (len(train_dataset) // 32)\n        self.decay_epochs = [it // (len(train_dataset) // 32) for it in self.decay_lr_at]\n\n    def adjust_learning_rate(self, epoch):\n        \"\"\"\n        Adjust the learning rate at specific epochs.\n        \"\"\"\n        if epoch in self.decay_epochs:\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = param_group['lr'] * self.decay_lr_to\n            print(f\"Learning rate adjusted to {param_group['lr']} at epoch {epoch}\")\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Perform one epoch of training.\n        \"\"\"\n        self.model.train()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n\n        start = time.time()\n\n        for i, (images, boxes, labels, _) in enumerate(self.train_loader):\n            data_time.update(time.time() - start)\n\n            # Move to device\n            images = images.to(self.device)\n            boxes = [b.to(self.device) for b in boxes]\n            labels = [l.to(self.device) for l in labels]\n\n            # Forward pass\n            predicted_locs, predicted_scores = self.model(images)\n\n            # Compute loss\n            loss = self.criterion(predicted_locs, predicted_scores, boxes, labels)\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n\n            # Gradient clipping\n            if self.grad_clip is not None:\n                clip_gradient(self.optimizer, self.grad_clip)\n\n            # Update model parameters\n            self.optimizer.step()\n\n            # Update metrics\n            losses.update(loss.item(), images.size(0))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            # Print status\n            if i % self.print_freq == 0:\n                print('Epoch: [{0}][{1}/{2}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(self.train_loader),\n                                                                      batch_time=batch_time,\n                                                                      data_time=data_time,\n                                                                      loss=losses))\n\n        del predicted_locs, predicted_scores, images, boxes, labels\n\n    def save_checkpoint(self, epoch):\n        \"\"\"\n        Save model checkpoint.\n        \"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model': self.model,\n            'optimizer': self.optimizer,\n        }, f'checkpoint_epoch_{epoch}.pth')\n        print(f\"Checkpoint saved for epoch {epoch}.\")\n\n    def train(self, start_epoch=0):\n        \"\"\"\n        Train the model across all epochs.\n        \"\"\"\n        for epoch in range(start_epoch, self.epochs):\n            self.adjust_learning_rate(epoch)\n            self.train_one_epoch(epoch)\n            self.save_checkpoint(epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.972157Z","iopub.status.idle":"2024-12-11T15:01:54.972706Z","shell.execute_reply.started":"2024-12-11T15:01:54.972438Z","shell.execute_reply":"2024-12-11T15:01:54.972468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model parameters\nn_classes = len(label_map())  # number of different types of objects\n\n# Learning parameters\ncheckpoint = None  # path to model checkpoint, None if none\nbatch_size = 8  # batch size\niterations = 10  # number of iterations to train\nworkers = 4  # number of workers for loading data in the DataLoader\nprint_freq = 200  # print training status every __ batches\nlr = 1e-3  # learning rate\ndecay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\ndecay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\nmomentum = 0.9  # momentum\nweight_decay = 5e-4  # weight decay\ngrad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.973936Z","iopub.status.idle":"2024-12-11T15:01:54.974489Z","shell.execute_reply.started":"2024-12-11T15:01:54.974222Z","shell.execute_reply":"2024-12-11T15:01:54.974251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n\n# Ottimizzatore\nbiases = [param for name, param in model.named_parameters() if param.requires_grad and name.endswith('.bias')]\nnot_biases = [param for name, param in model.named_parameters() if param.requires_grad and not name.endswith('.bias')]\noptimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n                            lr=lr, momentum=momentum, weight_decay=weight_decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.975753Z","iopub.status.idle":"2024-12-11T15:01:54.976168Z","shell.execute_reply.started":"2024-12-11T15:01:54.975954Z","shell.execute_reply":"2024-12-11T15:01:54.975971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione e avvio del trainer\ntrainer = Trainer(model, train_dataset, criterion, optimizer, batch_size, workers, device, grad_clip=grad_clip)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.977345Z","iopub.status.idle":"2024-12-11T15:01:54.977731Z","shell.execute_reply.started":"2024-12-11T15:01:54.977557Z","shell.execute_reply":"2024-12-11T15:01:54.977575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing sulle predizioni","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, test_dataset, batch_size, num_workers, device):\n        \"\"\"\n        Initialize the Evaluator.\n        \n        :param model: Trained SSD model to be evaluated\n        :param test_dataset: Dataset object for testing\n        :param batch_size: Batch size for evaluation\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for evaluation ('cuda' or 'cpu')\n        \"\"\"\n        self.model = model.to(device)\n        self.test_dataset = test_dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.pp = PrettyPrinter()  # For printing APs nicely\n\n        # Prepare dataloader\n        self.test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=test_dataset.collate_fn,\n            num_workers=num_workers,\n            pin_memory=True\n        )\n\n    def evaluate(self):\n        \"\"\"\n        Perform evaluation and compute mAP.\n        \"\"\"\n        self.model.eval()\n\n        # Lists to store detected and true boxes, labels, scores\n        det_boxes = list()\n        det_labels = list()\n        det_scores = list()\n        true_boxes = list()\n        true_labels = list()\n        true_difficulties = list()\n\n        with torch.no_grad():\n            for i, (images, boxes, labels, difficulties) in enumerate(tqdm(self.test_loader, desc='Evaluating')):\n                images = images.to(self.device)\n\n                # Forward pass\n                predicted_locs, predicted_scores = self.model(images)\n\n                # Detect objects\n                det_boxes_batch, det_labels_batch, det_scores_batch = self.model.detect_objects(\n                    predicted_locs, predicted_scores,\n                    min_score=0.01, max_overlap=0.45, top_k=200\n                )\n\n                # Store this batch's results\n                boxes = [b.to(self.device) for b in boxes]\n                labels = [l.to(self.device) for l in labels]\n                difficulties = [d.to(self.device) for d in difficulties]\n\n                det_boxes.extend(det_boxes_batch)\n                det_labels.extend(det_labels_batch)\n                det_scores.extend(det_scores_batch)\n                true_boxes.extend(boxes)\n                true_labels.extend(labels)\n                true_difficulties.extend(difficulties)\n\n        # Calculate mAP\n        APs, mAP = self.calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n\n        # Print AP for each class\n        self.pp.pprint(APs)\n        print('\\nMean Average Precision (mAP): %.3f' % mAP)\n\n    @staticmethod\n    def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n        \"\"\"\n        Calculate Mean Average Precision (mAP).\n        Placeholder for an actual implementation.\n        \n        :param det_boxes: Detected boxes\n        :param det_labels: Detected labels\n        :param det_scores: Detected scores\n        :param true_boxes: Ground truth boxes\n        :param true_labels: Ground truth labels\n        :param true_difficulties: Ground truth difficulties\n        :return: APs and mAP\n        \"\"\"\n        # Replace this with your actual mAP calculation logic\n        APs = {f'class_{i}': 0.0 for i in range(1, 21)}  # Dummy values for each class\n        mAP = 0.0  # Dummy value for mAP\n        return APs, mAP\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.979649Z","iopub.status.idle":"2024-12-11T15:01:54.980022Z","shell.execute_reply.started":"2024-12-11T15:01:54.979840Z","shell.execute_reply":"2024-12-11T15:01:54.979858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Caricamento del modello\ncheckpoint = torch.load(checkpoint_path)\nmodel = checkpoint['model']\n\n\n# Creazione e avvio del valutatore\nevaluator = Evaluator(model=model, test_dataset=test_dataset, batch_size=64, num_workers=4, device=device)\nevaluator.evaluate()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.981530Z","iopub.status.idle":"2024-12-11T15:01:54.981894Z","shell.execute_reply.started":"2024-12-11T15:01:54.981713Z","shell.execute_reply":"2024-12-11T15:01:54.981730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n    \"\"\"\n    Calculate the Mean Average Precision (mAP) of detected objects.\n\n    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n\n    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n    :param true_difficulties: list of tensors, one tensor for each image containing actual objects' difficulty (0 or 1)\n    :return: list of average precisions for all classes, mean average precision (mAP)\n    \"\"\"\n    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(true_labels)  # these are all lists of tensors of the same length, i.e. number of images\n    n_classes = len(label_map)\n\n    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n    true_images = list()\n    for i in range(len(true_labels)):\n        true_images.extend([i] * true_labels[i].size(0))\n    true_images = torch.LongTensor(true_images).to(\n        device)  # (n_objects), n_objects is the total no. of objects across all images\n    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n\n    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n\n    # Store all detections in a single continuous tensor while keeping track of the image it is from\n    det_images = list()\n    for i in range(len(det_labels)):\n        det_images.extend([i] * det_labels[i].size(0))\n    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n\n    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n\n    # Calculate APs for each class (except background)\n    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n    for c in range(1, n_classes):\n        # Extract only objects with this class\n        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n\n        # Keep track of which true objects with this class have already been 'detected'\n        # So far, none\n        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(\n            device)  # (n_class_objects)\n\n        # Extract only detections with this class\n        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n        n_class_detections = det_class_boxes.size(0)\n        if n_class_detections == 0:\n            continue\n\n        # Sort detections in decreasing order of confidence/scores\n        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n\n        # In the order of decreasing scores, check if true or false positive\n        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        for d in range(n_class_detections):\n            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n            this_image = det_class_images[d]  # (), scalar\n\n            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n            # If no such object in this image, then the detection is a false positive\n            if object_boxes.size(0) == 0:\n                false_positives[d] = 1\n                continue\n\n            # Find maximum overlap of this detection with objects in this image of this class\n            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n\n            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n            # We need 'original_ind' to update 'true_class_boxes_detected'\n\n            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n            if max_overlap.item() > 0.5:\n                # If this object has already not been detected, it's a true positive\n                if true_class_boxes_detected[original_ind] == 0:\n                    true_positives[d] = 1\n                    true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n                # Otherwise, it's a false positive (since this object is already accounted for)\n                else:\n                    false_positives[d] = 1\n            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n            else:\n                false_positives[d] = 1\n\n        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n        cumul_precision = cumul_true_positives / (\n                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n\n        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n        for i, t in enumerate(recall_thresholds):\n            recalls_above_t = cumul_recall >= t\n            if recalls_above_t.any():\n                precisions[i] = cumul_precision[recalls_above_t].max()\n            else:\n                precisions[i] = 0.\n        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n\n    # Calculate Mean Average Precision (mAP)\n    mean_average_precision = average_precisions.mean().item()\n\n    # Keep class-wise average precisions in a dictionary\n    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n\n    return average_precisions, mean_average_precision\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.983571Z","iopub.status.idle":"2024-12-11T15:01:54.983921Z","shell.execute_reply.started":"2024-12-11T15:01:54.983755Z","shell.execute_reply":"2024-12-11T15:01:54.983772Z"}},"outputs":[],"execution_count":null}]}