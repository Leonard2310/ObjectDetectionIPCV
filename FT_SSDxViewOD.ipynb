{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library import","metadata":{}},{"cell_type":"markdown","source":"### import librerie","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nimport json\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nimport ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:07:42.853240Z","iopub.execute_input":"2024-12-09T10:07:42.853616Z","iopub.status.idle":"2024-12-09T10:07:50.410922Z","shell.execute_reply.started":"2024-12-09T10:07:42.853570Z","shell.execute_reply":"2024-12-09T10:07:50.410231Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:07:50.412404Z","iopub.execute_input":"2024-12-09T10:07:50.412812Z","iopub.status.idle":"2024-12-09T10:08:07.688064Z","shell.execute_reply.started":"2024-12-09T10:07:50.412767Z","shell.execute_reply":"2024-12-09T10:08:07.687018Z"}},"outputs":[{"name":"stdout","text":"Collecting triton\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.15.1)\nDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\nSuccessfully installed triton-3.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Path","metadata":{}},{"cell_type":"code","source":"# file contenente i path delle immagini del dataset\ntxt_file = \"/kaggle/input/our-xview-dataset/xView_class_map.json\"\nimg_dir = \"/kaggle/input/our-xview-dataset/images\"\n\nannotation_file = \"/kaggle/input/our-xview-dataset/COCO_annotations_new.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:08:07.689340Z","iopub.execute_input":"2024-12-09T10:08:07.689639Z","iopub.status.idle":"2024-12-09T10:08:07.694070Z","shell.execute_reply.started":"2024-12-09T10:08:07.689611Z","shell.execute_reply":"2024-12-09T10:08:07.693282Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils', trust_repo=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:08:07.695806Z","iopub.execute_input":"2024-12-09T10:08:07.696050Z","iopub.status.idle":"2024-12-09T10:08:15.694840Z","shell.execute_reply.started":"2024-12-09T10:08:07.696026Z","shell.execute_reply":"2024-12-09T10:08:15.694082Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n  warnings.warn(\n/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, utils, aug=False):\n        \"\"\"\n        Args:\n            annotations_file (str): Path al file JSON delle annotazioni (es. formato COCO).\n            img_dir (str): Path alla directory delle immagini.\n            transform (callable, optional): Trasformazioni da applicare alle immagini.\n        \"\"\"\n\n        with open(annotations_file, 'r') as f:\n            self.annotations = json.load(f)\n        self.img_dir = img_dir\n        self.utils = utils\n        self.aug = aug\n        self. transform = transforms.Compose([  transforms.ToTensor(),  # Converte in formato [C, H, W]\n                                                transforms.Resize((320, 320)),  # Ridimensiona l'immagine\n                                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                              ])\n\n    def __len__(self):\n        return len(self.annotations['images'])\n\n    def __getitem__(self, idx):\n        # Leggi i dettagli dell'immagine\n        img_info = self.annotations['images'][idx]\n        img_path = os.path.join(self.img_dir, img_info['file_name'])\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        #if self.aug:\n        image = self.transform(image)\n        \n        # Leggi le annotazioni\n        img_id = img_info['id']\n        annotations = [ann for ann in self.annotations['annotations'] if ann['image_id'] == img_id]\n        \n        #bboxes = np.array([ann['bbox'] for ann in annotations], dtype=np.float32)\n        #bboxes = np.array([ast.literal_eval(ann['bbox']) for ann in annotations], dtype=np.float32)\n        if len(annotations) == 0:\n            bboxes = np.zeros((0, 4), dtype=np.float32)\n            labels = np.zeros((0,), dtype=np.int64)\n        else:\n            bboxes = np.array([ast.literal_eval(ann['bbox']) if isinstance(ann['bbox'], str) else ann['bbox'] for ann in annotations], dtype=np.float32)\n            labels = np.array([ann['category_id'] for ann in annotations], dtype=np.int64)\n            bboxes[:, 2:] += bboxes[:, :2] # Converti bboxes nel formato richiesto (x_min, y_min, x_max, y_max) \n\n        target = {\n            \"image\": torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1),  # Converti immagine in tensore\n            'boxes': torch.tensor(bboxes, dtype=torch.float32),\n            'labels': torch.tensor(labels, dtype=torch.int64),\n        }\n\n        #return image, target\n        return target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:47.408683Z","iopub.execute_input":"2024-12-09T10:18:47.409358Z","iopub.status.idle":"2024-12-09T10:18:47.419145Z","shell.execute_reply.started":"2024-12-09T10:18:47.409323Z","shell.execute_reply":"2024-12-09T10:18:47.418421Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def collate_fn(batch):\n    # Separate images and targets from the batch\n    images = [item['image'] for item in batch]\n    targets = [item for item in batch]\n    \n    # Stack images into a tensor of shape (batch_size, C, H, W)\n    images = torch.stack(images, dim=0)\n    \n    # List of bounding boxes and labels\n    boxes_list = [target['boxes'] for target in targets]\n    labels_list = [target['labels'] for target in targets]\n    \n    # Concatenate the boxes and labels using cat to form the targets for each image\n    all_boxes = torch.cat(boxes_list, dim=0)  # This will concatenate all the boxes from the batch\n    all_labels = torch.cat(labels_list, dim=0)  # Similarly concatenate all labels\n    \n    # Return images and targets (list of dictionaries)\n    return images, [{'boxes': boxes, 'labels': labels} for boxes, labels in zip(boxes_list, labels_list)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:47.464182Z","iopub.execute_input":"2024-12-09T10:18:47.465075Z","iopub.status.idle":"2024-12-09T10:18:47.470590Z","shell.execute_reply.started":"2024-12-09T10:18:47.465038Z","shell.execute_reply":"2024-12-09T10:18:47.469736Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(annotation_file, img_dir, utils, aug=True) \nvalid_dataset = CustomDataset(annotation_file, img_dir, utils, aug=False)  \ntest_dataset = CustomDataset(annotation_file, img_dir, utils, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:47.474984Z","iopub.execute_input":"2024-12-09T10:18:47.475271Z","iopub.status.idle":"2024-12-09T10:18:51.587226Z","shell.execute_reply.started":"2024-12-09T10:18:47.475245Z","shell.execute_reply":"2024-12-09T10:18:51.586264Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Network","metadata":{}},{"cell_type":"code","source":"class SSDModel(nn.Module):\n    def __init__(self, num_classes):\n        super(SSDModel, self).__init__()\n\n        ## Model -> per info https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD\n        self.ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd') # modello pre-addestrato su dataset COCO\n\n    def forward(self, images):\n\n        # Calcola le previsioni con il modello SSD\n        predictions = self.ssd_model(images)  # Output grezzo del modello SSD\n        \n        return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:51.588897Z","iopub.execute_input":"2024-12-09T10:18:51.589252Z","iopub.status.idle":"2024-12-09T10:18:51.594961Z","shell.execute_reply.started":"2024-12-09T10:18:51.589206Z","shell.execute_reply":"2024-12-09T10:18:51.593969Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class SSDLoss(nn.Module):\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Combina Smooth L1 Loss per la regressione dei bounding box e\n        Cross Entropy Loss per la classificazione.\n\n        Args:\n            alpha (float): Peso per bilanciare la regressione e la classificazione.\n        \"\"\"\n        super(SSDLoss, self).__init__()\n        self.alpha = alpha\n        self.smooth_l1 = nn.SmoothL1Loss(reduction='none')\n        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Calcola la perdita SSD.\n\n        Args:\n            predictions (tuple): (loc_preds, conf_preds), dove\n                loc_preds: tensor (N, num_boxes, 4) - Predizioni dei bounding box.\n                conf_preds: tensor (N, num_boxes, num_classes) - Predizioni delle classi.\n            targets (tuple): (loc_targets, conf_targets), dove\n                loc_targets: tensor (N, num_boxes, 4) - Bounding box reali.\n                conf_targets: tensor (N, num_boxes) - Classi reali.\n\n        Returns:\n            torch.Tensor: Valore della perdita.\n        \"\"\"\n        loc_preds, conf_preds = predictions\n        loc_targets, conf_targets = targets\n\n        # Loss per la regressione del bounding box\n        loc_loss = self.smooth_l1(loc_preds, loc_targets)\n        loc_loss = loc_loss.sum(dim=-1)  # Somma per box\n\n        # Loss per la classificazione\n        conf_loss = self.cross_entropy(conf_preds.transpose(2, 1), conf_targets)\n        \n        # Filtro per box positivi (label diversi da 0 o -1)\n        pos_mask = conf_targets > 0\n        neg_mask = conf_targets == 0\n\n        # Calcola la perdita per i positivi\n        loc_loss = (loc_loss * pos_mask).sum()  # Loss solo per i positivi\n        conf_loss = conf_loss[pos_mask | neg_mask].sum()  # Loss per positivi e negativi\n\n        # Combinazione di loss\n        total_loss = loc_loss + self.alpha * conf_loss\n        return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:51.596376Z","iopub.execute_input":"2024-12-09T10:18:51.596711Z","iopub.status.idle":"2024-12-09T10:18:51.613380Z","shell.execute_reply.started":"2024-12-09T10:18:51.596686Z","shell.execute_reply":"2024-12-09T10:18:51.612615Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_loader, val_loader, criterion, optimizer=None, device='cuda', checkpoint_dir='checkpoints'):\n        \"\"\"\n        Inizializza la classe Trainer.\n        \n        Args:\n            model: Il modello SSD.\n            train_loader: DataLoader per il training set.\n            val_loader: DataLoader per il validation set.\n            criterion: Funzione di perdita.\n            optimizer: Ottimizzatore (opzionale, di default Adam).\n            device: Dispositivo per il calcolo ('cuda' o 'cpu').\n            checkpoint_dir: Directory per salvare i checkpoint.\n        \"\"\"\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.criterion = criterion\n        self.optimizer = optimizer if optimizer else Adam(model.parameters(), lr=1e-4)\n        self.device = device\n        self.checkpoint_dir = checkpoint_dir\n        \n        if not os.path.exists(self.checkpoint_dir):\n            os.makedirs(self.checkpoint_dir)\n\n    def process_targets(self, targets):\n        processed_targets = []\n        for target in targets:\n            target_dict = {\n                'boxes': target['boxes'],  # Mantieni le bounding boxes come sono\n                'labels': target['labels'],  # Mantieni le etichette come sono\n            }\n            processed_targets.append(target_dict)\n        return processed_targets\n\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        epoch_loss = 0\n        for images, targets in self.train_loader:\n            images = images.to(self.device)\n            \n            # Processa i targets (in questo caso non vengono 'stackati', sono una lista)\n            processed_targets = self.process_targets(targets)\n            \n            # Esegui il forward pass\n            predictions = self.model(images)\n            \n            # Calcola la perdita (utilizzando il formato richiesto dal modello)\n            loss = self.compute_loss(predictions, processed_targets)\n            \n            # Ottimizzazione\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        return epoch_loss\n\n    \n    def validate_one_epoch(self, epoch):\n        \"\"\"\n        Esegue un'epoca di validazione.\n        \"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        pbar = tqdm(self.val_loader, desc=f\"Validation Epoch {epoch}\")\n        \n        with torch.no_grad():\n            for images, targets in pbar:\n                # Sposta immagini sul dispositivo\n                images = images.to(self.device)\n                \n                # Elabora i targets\n                targets = self.process_targets(targets)\n            \n                # Forward pass\n                predictions = self.model(images)\n                \n                # Calcolo della perdita\n                loss = self.criterion(predictions, (targets['loc_targets'], targets['conf_targets']))\n                running_loss += loss.item()\n                \n                pbar.set_postfix({\"val_loss\": running_loss / len(self.val_loader)})\n        \n        return running_loss / len(self.val_loader)\n    \n    def save_checkpoint(self, epoch, train_loss, val_loss, best=False):\n        \"\"\"\n        Salva un checkpoint del modello.\n        \n        Args:\n            epoch: Epoca corrente.\n            train_loss: Perdita di training corrente.\n            val_loss: Perdita di validazione corrente.\n            best: Se True, salva il checkpoint come il migliore.\n        \"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'train_loss': train_loss,\n            'val_loss': val_loss\n        }\n        filename = f\"best_model.pth\" if best else f\"checkpoint_epoch_{epoch}.pth\"\n        path = os.path.join(self.checkpoint_dir, filename)\n        torch.save(checkpoint, path)\n        print(f\"Checkpoint salvato: {path}\")\n    \n    def fit(self, epochs):\n        \"\"\"\n        Esegue l'addestramento e la validazione per un dato numero di epoche.\n        \"\"\"\n        train_losses = []\n        val_losses = []\n        best_val_loss = float('inf')\n        \n        for epoch in range(1, epochs + 1):\n            train_loss = self.train_one_epoch(epoch)\n            val_loss = self.validate_one_epoch(epoch)\n            \n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            \n            # Salvataggio del checkpoint per ogni epoca\n            self.save_checkpoint(epoch, train_loss, val_loss)\n            \n            # Salvataggio del miglior modello\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                self.save_checkpoint(epoch, train_loss, val_loss, best=True)\n            \n            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n        \n        return train_losses, val_losses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:51.615542Z","iopub.execute_input":"2024-12-09T10:18:51.615901Z","iopub.status.idle":"2024-12-09T10:18:51.640449Z","shell.execute_reply.started":"2024-12-09T10:18:51.615867Z","shell.execute_reply":"2024-12-09T10:18:51.639680Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"num_classes = 12\nssd_model = SSDModel(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:51.641521Z","iopub.execute_input":"2024-12-09T10:18:51.642228Z","iopub.status.idle":"2024-12-09T10:18:52.104111Z","shell.execute_reply.started":"2024-12-09T10:18:51.642188Z","shell.execute_reply":"2024-12-09T10:18:52.103153Z"}},"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Addestra il modello, con validazione ad ogni epoca\nloss = SSDLoss(alpha=1.0)\n\n# Inizializza il trainer\ntrainer = Trainer(\n    model=ssd_model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=loss,\n    optimizer=Adam(ssd_model.parameters(), lr=1e-4),\n    device='cuda'\n)\n\n# Avvia il training\ntrain_losses, val_losses = trainer.fit(epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:18:52.105361Z","iopub.execute_input":"2024-12-09T10:18:52.105740Z","iopub.status.idle":"2024-12-09T10:18:54.213278Z","shell.execute_reply.started":"2024-12-09T10:18:52.105692Z","shell.execute_reply":"2024-12-09T10:18:54.211919Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mssd_model,\n\u001b[1;32m      7\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Avvia il training\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[21], line 120\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    117\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_one_epoch(epoch)\n\u001b[1;32m    123\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     44\u001b[0m processed_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_targets(targets)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Esegui il forward pass\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Calcola la perdita (utilizzando il formato richiesto dal modello)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(predictions, processed_targets)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mSSDModel.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Calcola le previsioni con il modello SSD\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssd_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Output grezzo del modello SSD\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Detection/SSD/ssd/model.py:118\u001b[0m, in \u001b[0;36mSSD300.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 118\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     detection_feed \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_blocks:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Detection/SSD/ssd/model.py:51\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[32, 320, 3, 320] to have 3 channels, but got 320 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 3, 7, 7], expected input[32, 320, 3, 320] to have 3 channels, but got 320 channels instead","output_type":"error"}],"execution_count":23},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}