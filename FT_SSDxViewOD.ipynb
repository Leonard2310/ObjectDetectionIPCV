{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import delle librerie","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport random\nimport xml.etree.ElementTree as ET\nimport torchvision.transforms.functional as FT\n\nimport torch\nfrom tqdm import tqdm\nfrom pprint import PrettyPrinter\n\nimport json\nimport random\nfrom collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:01:19.614137Z","iopub.execute_input":"2024-12-12T09:01:19.614661Z","iopub.status.idle":"2024-12-12T09:01:19.620332Z","shell.execute_reply.started":"2024-12-12T09:01:19.614608Z","shell.execute_reply":"2024-12-12T09:01:19.619308Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Path","metadata":{}},{"cell_type":"code","source":"# path del dataset\nbase_dict = '/kaggle/input/our-xview-dataset'\n\n# path della cartella contenente le immagini\nimg_dict = '/kaggle/input/our-xview-dataset/images'\n\n# path dei file .txt da utilizzare per prelevare rispettivamente le immagini per il train, la validation e il test\n# MANCANO -> servono per provare il codice\ntrain_img_path = '/kaggle/input/our-xview-dataset/YOLO_cfg/train.txt' # file contenete i path delle immagini del dataset di train\nval_img_path = '/kaggle/input/our-xview-dataset/YOLO_cfg/val.txt'\ntest_img_path = '/kaggle/input/our-xview-dataset/YOLO_cfg/test.txt'\n\n# path di output\noutput_folder = '/kaggle/working/'\n\n# path contenente le annotazioni in formato .json\ncoco_json_path = os.path.join(base_dict, 'COCO_annotations_new.json') \nnew_coco_json_path = os.path.join(output_folder, 'mod_COCO_annotations.json') \n\n# path file per il training\ntrain_image = os.path.join(output_folder, 'TRAIN_images.json')\ntrain_bbox = os.path.join(output_folder, 'TRAIN_objects.json')\n\n# path file per la validation\nval_image = os.path.join(output_folder, 'VAL_images.json')\nval_bbox = os.path.join(output_folder, 'VAL_objects.json')\n\n# path file per il test\ntest_image = os.path.join(output_folder, 'TEST_images.json')\ntest_bbox = os.path.join(output_folder, 'TEST_objects.json')\n\ncheckpoint_path = './checkpoint_ssd300.pth.tar'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:21:41.949606Z","iopub.execute_input":"2024-12-12T09:21:41.950029Z","iopub.status.idle":"2024-12-12T09:21:41.957767Z","shell.execute_reply.started":"2024-12-12T09:21:41.949969Z","shell.execute_reply":"2024-12-12T09:21:41.956537Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:21:44.169628Z","iopub.execute_input":"2024-12-12T09:21:44.170042Z","iopub.status.idle":"2024-12-12T09:21:44.175252Z","shell.execute_reply.started":"2024-12-12T09:21:44.169995Z","shell.execute_reply":"2024-12-12T09:21:44.174195Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    with open(input_path, 'r') as f:\n        data = json.load(f)\n \n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Lista di nuove annotazioni da aggiungere per immagini senza bbox\n    new_annotations = []\n \n    # Elenco di annotazioni da rimuovere\n    annotations_to_remove = []\n \n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        x, y, width, height = annotation['bbox']\n        xmin = x\n        xmax = x + width\n        ymin = y\n        ymax = y + height\n        # Verifica che xmin < xmax e ymin < ymax\n        if xmin >= xmax or ymin >= ymax:\n            annotations_to_remove.append(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, ymin, xmax, ymax]\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = [ann for ann in data['annotations'] if ann['id'] not in annotations_to_remove]\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, image['width'], 0.0, image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n \n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n \n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:01:19.652231Z","iopub.execute_input":"2024-12-12T09:01:19.652570Z","iopub.status.idle":"2024-12-12T09:01:19.669169Z","shell.execute_reply.started":"2024-12-12T09:01:19.652526Z","shell.execute_reply":"2024-12-12T09:01:19.668210Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"process_custom_coco_json(coco_json_path, new_coco_json_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:01:19.671250Z","iopub.execute_input":"2024-12-12T09:01:19.671635Z","iopub.status.idle":"2024-12-12T09:01:36.531054Z","shell.execute_reply.started":"2024-12-12T09:01:19.671597Z","shell.execute_reply":"2024-12-12T09:01:36.529954Z"}},"outputs":[{"name":"stderr","text":"Processing Categories: 100%|██████████| 11/11 [00:00<00:00, 108558.46it/s]\nBuilding Image Annotations Dictionary: 100%|██████████| 670213/670213 [00:00<00:00, 946920.69it/s] \nProcessing Annotations: 100%|██████████| 670213/670213 [00:03<00:00, 204842.94it/s]\nProcessing Images: 100%|██████████| 45891/45891 [00:00<00:00, 569362.03it/s]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"def split_coco_and_check(coco_file, output_path, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n    # Carica il file COCO\n    with open(coco_file, 'r') as f:\n        coco_data = json.load(f)\n\n    images = coco_data['images']\n    annotations = coco_data['annotations']\n    categories = coco_data['categories']\n\n    total_images = len(images)\n    total_annotations = len(annotations)\n\n    # Shuffle delle immagini per garantire casualità\n    random.shuffle(images)\n\n    # Calcola i numeri di immagini per train, val e test\n    train_count = int(train_ratio * total_images)\n    val_count = int(val_ratio * total_images)\n    test_count = total_images - train_count - val_count\n\n    train_images = images[:train_count]\n    val_images = images[train_count:train_count + val_count]\n    test_images = images[train_count + val_count:]\n\n    # Crea set di ID immagini\n    train_ids = {img['id'] for img in train_images}\n    val_ids = {img['id'] for img in val_images}\n    test_ids = {img['id'] for img in test_images}\n\n    # Divide le annotazioni\n    train_annotations = [ann for ann in annotations if ann['image_id'] in train_ids]\n    val_annotations = [ann for ann in annotations if ann['image_id'] in val_ids]\n    test_annotations = [ann for ann in annotations if ann['image_id'] in test_ids]\n\n    train_bbox_count = len(train_annotations)\n    val_bbox_count = len(val_annotations)\n    test_bbox_count = len(test_annotations)\n\n    # Salva i file di output\n    def save_split(file_name, images_split):\n        with open(file_name, 'w') as f:\n            for img in images_split:\n                f.write(f\"{output_path}/{img['file_name']}\\n\")\n\n    save_split('train.txt', train_images)\n    save_split('val.txt', val_images)\n    save_split('test.txt', test_images)\n\n    # Controlla le proporzioni\n    check_split_proportions(\n        total_images, total_annotations,\n        len(train_images), len(val_images), len(test_images),\n        train_bbox_count, val_bbox_count, test_bbox_count,\n        train_ratio, val_ratio, test_ratio,\n        train_annotations, val_annotations, test_annotations,\n        categories\n    )\n\ndef check_split_proportions(total_images, total_annotations, train_count, val_count, test_count, \n                            train_bbox_count, val_bbox_count, test_bbox_count, \n                            train_ratio, val_ratio, test_ratio, \n                            train_annotations, val_annotations, test_annotations, categories):\n    # Percentuali per immagini\n    train_image_percentage = (train_count / total_images) * 100\n    val_image_percentage = (val_count / total_images) * 100\n    test_image_percentage = (test_count / total_images) * 100\n\n    # Percentuali per bbox\n    train_bbox_percentage = (train_bbox_count / total_annotations) * 100\n    val_bbox_percentage = (val_bbox_count / total_annotations) * 100\n    test_bbox_percentage = (test_bbox_count / total_annotations) * 100\n\n    print(f\"Totale immagini: {total_images}\")\n    print(f\"Totale annotazioni (bbox): {total_annotations}\")\n    print(f\"Train: {train_count} immagini ({train_image_percentage:.2f}%) ({train_bbox_count} bbox) ({train_bbox_percentage:.2f}%)\")\n    print(f\"Val: {val_count} immagini ({val_image_percentage:.2f}%) ({val_bbox_count} bbox) ({val_bbox_percentage:.2f}%)\")\n    print(f\"Test: {test_count} immagini ({test_image_percentage:.2f}%) ({test_bbox_count} bbox) ({test_bbox_percentage:.2f}%)\")\n\n    # Calcola il numero di annotazioni per categoria nei vari set\n    category_count_train = defaultdict(int)\n    category_count_val = defaultdict(int)\n    category_count_test = defaultdict(int)\n\n    for annotation in train_annotations:\n        category_count_train[annotation['category_id']] += 1\n    for annotation in val_annotations:\n        category_count_val[annotation['category_id']] += 1\n    for annotation in test_annotations:\n        category_count_test[annotation['category_id']] += 1\n\n    # Stampa le proporzioni per categoria\n    print(\"\\nProporzioni per categoria:\")\n    for category in categories:\n        category_id = category['id']\n        category_name = category['name']\n\n        # Conta il numero di annotazioni per categoria in ogni set\n        train_cat_count = category_count_train.get(category_id, 0)\n        val_cat_count = category_count_val.get(category_id, 0)\n        test_cat_count = category_count_test.get(category_id, 0)\n\n        # Calcola la percentuale di annotazioni per categoria\n        total_cat_annotations = train_cat_count + val_cat_count + test_cat_count\n        if total_cat_annotations > 0:\n            train_cat_percentage = (train_cat_count / total_cat_annotations) * 100\n            val_cat_percentage = (val_cat_count / total_cat_annotations) * 100\n            test_cat_percentage = (test_cat_count / total_cat_annotations) * 100\n        else:\n            train_cat_percentage = val_cat_percentage = test_cat_percentage = 0.0\n\n        print(f\"{category_name}:\")\n        print(f\"  Train: {train_cat_count} annotazioni ({train_cat_percentage:.2f}%)\")\n        print(f\"  Val: {val_cat_count} annotazioni ({val_cat_percentage:.2f}%)\")\n        print(f\"  Test: {test_cat_count} annotazioni ({test_cat_percentage:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:01:36.533357Z","iopub.execute_input":"2024-12-12T09:01:36.533813Z","iopub.status.idle":"2024-12-12T09:01:36.551750Z","shell.execute_reply.started":"2024-12-12T09:01:36.533764Z","shell.execute_reply":"2024-12-12T09:01:36.550612Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"split_coco_and_check(new_coco_json_path, img_dict, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:01:36.553096Z","iopub.execute_input":"2024-12-12T09:01:36.553460Z","iopub.status.idle":"2024-12-12T09:01:40.424459Z","shell.execute_reply.started":"2024-12-12T09:01:36.553423Z","shell.execute_reply":"2024-12-12T09:01:40.423418Z"}},"outputs":[{"name":"stdout","text":"Totale immagini: 45891\nTotale annotazioni (bbox): 683897\nTrain: 27534 immagini (60.00%) (415411 bbox) (60.74%)\nVal: 9178 immagini (20.00%) (137906 bbox) (20.16%)\nTest: 9179 immagini (20.00%) (132014 bbox) (19.30%)\n\nProporzioni per categoria:\nAircraft:\n  Train: 1038 annotazioni (60.77%)\n  Val: 311 annotazioni (18.21%)\n  Test: 359 annotazioni (21.02%)\nPassenger Vehicle:\n  Train: 138548 annotazioni (61.44%)\n  Val: 44053 annotazioni (19.54%)\n  Test: 42902 annotazioni (19.03%)\nTruck:\n  Train: 21198 annotazioni (61.47%)\n  Val: 6679 annotazioni (19.37%)\n  Test: 6608 annotazioni (19.16%)\nRailway Vehicle:\n  Train: 2696 annotazioni (63.69%)\n  Val: 621 annotazioni (14.67%)\n  Test: 916 annotazioni (21.64%)\nMaritime Vessel:\n  Train: 3853 annotazioni (60.88%)\n  Val: 1417 annotazioni (22.39%)\n  Test: 1059 annotazioni (16.73%)\nEngineering Vehicle:\n  Train: 3218 annotazioni (58.73%)\n  Val: 1226 annotazioni (22.38%)\n  Test: 1035 annotazioni (18.89%)\nBuilding:\n  Train: 231977 annotazioni (60.13%)\n  Val: 79185 annotazioni (20.53%)\n  Test: 74630 annotazioni (19.34%)\nHelipad:\n  Train: 101 annotazioni (66.45%)\n  Val: 16 annotazioni (10.53%)\n  Test: 35 annotazioni (23.03%)\nStorage Tank:\n  Train: 1230 annotazioni (60.50%)\n  Val: 437 annotazioni (21.50%)\n  Test: 366 annotazioni (18.00%)\nShipping Container:\n  Train: 3108 annotazioni (57.00%)\n  Val: 1108 annotazioni (20.32%)\n  Test: 1237 annotazioni (22.68%)\nPylon:\n  Train: 292 annotazioni (62.13%)\n  Val: 75 annotazioni (15.96%)\n  Test: 103 annotazioni (21.91%)\nbackground:\n  Train: 8152 annotazioni (59.53%)\n  Val: 2778 annotazioni (20.29%)\n  Test: 2764 annotazioni (20.18%)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"def parse_coco_annotation(annotation_data):\n    \"\"\"\n    Converti le annotazioni COCO in una struttura utile.\n    \"\"\"\n    boxes = []\n    labels = []\n\n    for ann in annotation_data:\n        category_id = ann['category_id']  # Usa direttamente il category_id come etichetta\n        bbox = ann['bbox']  # I bounding box sono già nel formato [xmin, ymin, xmax, ymax]\n        xmin, ymin, xmax, ymax = bbox\n\n        boxes.append([xmin, ymin, xmax, ymax])\n        labels.append(category_id)\n\n    return {'boxes': boxes, 'labels': labels}\n\ndef create_coco_data_lists(coco_file, splits_path, output_folder):\n    \"\"\"\n    Converte i dati COCO e split in liste per train, val e test.\n    \"\"\"\n    with open(coco_file, 'r') as f:\n        coco_data = json.load(f)\n\n    # Prepara mappature da immagini e annotazioni\n    images = {img['file_name']: img for img in coco_data['images']}  # Mappa file_name -> immagine\n    annotations_by_image = defaultdict(list)\n    for ann in coco_data['annotations']:\n        annotations_by_image[ann['image_id']].append(ann)\n\n    # Genera i dati per ciascuno split\n    for split in ['train', 'val', 'test']:\n        split_file = os.path.join(splits_path, f\"{split}.txt\")\n        with open(split_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n\n        image_list = []\n        objects_list = []\n\n        for image_file in image_files:\n            file_name = os.path.basename(image_file)  # Ottieni solo il nome del file\n            if file_name not in images:\n                continue\n\n            image_info = images[file_name]\n            image_id = image_info['id']\n            annotations = annotations_by_image[image_id]\n            objects = parse_coco_annotation(annotations)\n\n            if not objects['boxes']:\n                continue\n\n            image_list.append(image_file)\n            objects_list.append(objects)\n\n        # Salva i risultati\n        with open(os.path.join(output_folder, f\"{split.upper()}_images.json\"), 'w') as j:\n            json.dump(image_list, j)\n        with open(os.path.join(output_folder, f\"{split.upper()}_objects.json\"), 'w') as j:\n            json.dump(objects_list, j)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:06:58.863892Z","iopub.execute_input":"2024-12-12T09:06:58.864300Z","iopub.status.idle":"2024-12-12T09:06:58.876782Z","shell.execute_reply.started":"2024-12-12T09:06:58.864262Z","shell.execute_reply":"2024-12-12T09:06:58.875602Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"create_coco_data_lists(new_coco_json_path, output_folder, output_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:07:00.506554Z","iopub.execute_input":"2024-12-12T09:07:00.507555Z","iopub.status.idle":"2024-12-12T09:07:10.166528Z","shell.execute_reply.started":"2024-12-12T09:07:00.507513Z","shell.execute_reply":"2024-12-12T09:07:10.165264Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms.functional as FT\n\nclass CustomDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset class to be used in a DataLoader for batching.\n    \"\"\"\n\n    def __init__(self, path_image, path_bbox):\n        \"\"\"\n        :param path_image: Path to the JSON file containing image paths.\n        :param path_bbox: Path to the JSON file containing bounding boxes and labels.\n        \"\"\"\n        # Load data\n        with open(path_image, 'r') as j:\n            self.images = json.load(j)\n        with open(path_bbox, 'r') as j:\n            self.objects = json.load(j)\n\n        # Ensure the lengths match\n        assert len(self.images) == len(self.objects), \"Images and annotations must have the same length.\"\n\n    def __transform(self, image, boxes, labels):\n        \"\"\"\n        Apply transformations to the image, boxes, and labels.\n        :param image: A PIL Image.\n        :param boxes: Bounding boxes as a tensor of dimensions (n_objects, 4).\n        :param labels: Labels as a tensor of dimensions (n_objects).\n        :return: Transformed image, boxes, and labels.\n        \"\"\"\n        def resize(image, boxes, dims=(300, 300)):\n            # Resize image\n            new_image = FT.resize(image, dims)\n\n            # Normalize bounding boxes\n            old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n            new_boxes = boxes / old_dims  # Percent coordinates\n\n            return new_image, new_boxes\n\n        # ImageNet normalization values\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        # Resize image and normalize boxes\n        image, boxes = resize(image, boxes)\n\n        # Convert image to tensor\n        image = FT.to_tensor(image)\n\n        # Normalize image\n        image = FT.normalize(image, mean=mean, std=std)\n\n        return image, boxes, labels\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve an image and its corresponding objects.\n        :param idx: Index of the data point.\n        :return: Transformed image, bounding boxes, and labels.\n        \"\"\"\n        # Load image\n        image = Image.open(self.images[idx]).convert('RGB')\n\n        # Load objects\n        objects = self.objects[idx]\n        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n\n        # Apply transformations\n        image, boxes, labels = self.__transform(image, boxes, labels)\n\n        return image, boxes, labels\n\n    def __len__(self):\n        \"\"\"\n        Total number of data points.\n        :return: Length of the dataset.\n        \"\"\"\n        return len(self.images)\n\n    def collate_fn(self, batch):\n        \"\"\"\n        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n\n        This describes how to combine these tensors of different sizes. We use lists.\n\n        :param batch: an iterable of N sets from __getitem__()\n        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels.\n        \"\"\"\n\n        images = list()\n        boxes = list()\n        labels = list()\n\n        for b in batch:\n            images.append(b[0])\n            boxes.append(b[1])\n            labels.append(b[2])\n\n        images = torch.stack(images, dim=0)\n\n        return images, boxes, labels  # tensor (N, 3, 300, 300), 2 lists of N tensors each","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:24:54.566858Z","iopub.execute_input":"2024-12-12T09:24:54.567275Z","iopub.status.idle":"2024-12-12T09:24:54.580067Z","shell.execute_reply.started":"2024-12-12T09:24:54.567235Z","shell.execute_reply":"2024-12-12T09:24:54.578677Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"train_dataset = CustomDataset(train_image, train_bbox)\nval_dataset = CustomDataset(val_image, val_bbox)\ntest_dataset = CustomDataset(test_image, test_bbox)\n\n# Recupera un campione\nimage, boxes, labels = train_dataset[0]\n\n# Dimensioni\nprint(\"Image shape:\", image.shape)  # Torch tensor di dimensione (3, 300, 300)\nprint(\"Boxes:\", boxes)             # Bounding box normalizzati\nprint(\"Labels:\", labels)           # Etichette","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:24:56.033560Z","iopub.execute_input":"2024-12-12T09:24:56.033967Z","iopub.status.idle":"2024-12-12T09:24:57.521943Z","shell.execute_reply.started":"2024-12-12T09:24:56.033928Z","shell.execute_reply":"2024-12-12T09:24:57.520798Z"}},"outputs":[{"name":"stdout","text":"Image shape: torch.Size([3, 300, 300])\nBoxes: tensor([[0.2812, 0.9375, 0.3281, 0.9719],\n        [0.0000, -0.0000, 0.1562, 0.2031],\n        [0.7219, 0.9062, 0.9281, 1.0000],\n        [0.2250, 0.0000, 0.5000, 0.1344],\n        [0.5188, 0.8938, 0.6781, 1.0000],\n        [0.8875, 0.9250, 1.0031, 0.9969],\n        [-0.0000, 0.0000, 0.2656, 0.1094],\n        [0.5625, 0.1187, 0.5875, 0.1531],\n        [0.5125, 0.1094, 0.5281, 0.1406],\n        [0.6438, 0.9312, 0.7719, 1.0000],\n        [0.2531, 0.9750, 0.2781, 1.0000]])\nLabels: tensor([1, 6, 6, 6, 6, 6, 6, 2, 1, 6, 1])\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                               collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)\nval_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                             collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)\ntest_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                              collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:25:07.124932Z","iopub.execute_input":"2024-12-12T09:25:07.125390Z","iopub.status.idle":"2024-12-12T09:25:07.132076Z","shell.execute_reply.started":"2024-12-12T09:25:07.125352Z","shell.execute_reply":"2024-12-12T09:25:07.130806Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"fare l'import da GitHub https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def find_intersection(set_1, set_2):\n    \"\"\"\n    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\n\ndef find_jaccard_overlap(set_1, set_2):\n    \"\"\"\n    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # Find intersections\n    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def adjust_learning_rate(optimizer, scale):\n    \"\"\"\n    Scale learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param scale: factor to multiply learning rate with.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * scale\n    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n\n\ndef accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)\n\n\ndef save_checkpoint(epoch, model, optimizer):\n    \"\"\"\n    Save model checkpoint.\n\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model,\n             'optimizer': optimizer}\n    filename = 'checkpoint_ssd300.pth.tar'\n    torch.save(state, filename)\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_dataset, train_dataloader, criterion, optimizer, batch_size, num_workers, device, \n                 grad_clip=None, print_freq=10, iterations=120000, decay_lr_at=None, decay_lr_to=0.1, \n                 momentum=0.9, weight_decay=5e-4):\n        \"\"\"\n        Initialize the Trainer.\n        \n        :param model: SSD300 model instance\n        :param train_dataset: Dataset object\n        :param criterion: Loss function\n        :param optimizer: Optimizer\n        :param batch_size: Training batch size\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for training ('cuda' or 'cpu')\n        :param grad_clip: Gradient clipping value (default: None)\n        :param print_freq: Frequency of printing training progress\n        :param iterations: Total number of training iterations\n        :param decay_lr_at: Iterations to decay learning rate\n        :param decay_lr_to: Learning rate decay factor\n        :param momentum: Momentum for optimizer\n        :param weight_decay: Weight decay for optimizer\n        \"\"\"\n        self.model = model\n        self.train_dataset = train_dataset\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.grad_clip = grad_clip\n        self.print_freq = print_freq\n        self.iterations = iterations\n        self.decay_lr_at = decay_lr_at if decay_lr_at is not None else [80000, 100000]\n        self.decay_lr_to = decay_lr_to\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n\n        # Prepare dataloader\n        self.train_loader = train_dataloader\n\n        # Calculate epochs and decay epochs\n        self.epochs = iterations // (len(train_dataset) // 32)\n        self.decay_epochs = [it // (len(train_dataset) // 32) for it in self.decay_lr_at]\n\n    def adjust_learning_rate(self, epoch):\n        \"\"\"\n        Adjust the learning rate at specific epochs.\n        \"\"\"\n        if epoch in self.decay_epochs:\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = param_group['lr'] * self.decay_lr_to\n            print(f\"Learning rate adjusted to {param_group['lr']} at epoch {epoch}\")\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Perform one epoch of training.\n        \"\"\"\n        self.model.train()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n\n        start = time.time()\n\n        for i, (images, boxes, labels, _) in enumerate(self.train_loader):\n            data_time.update(time.time() - start)\n\n            # Move to device\n            images = images.to(self.device)\n            boxes = [b.to(self.device) for b in boxes]\n            labels = [l.to(self.device) for l in labels]\n\n            # Forward pass\n            predicted_locs, predicted_scores = self.model(images)\n\n            # Compute loss\n            loss = self.criterion(predicted_locs, predicted_scores, boxes, labels)\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n\n            # Gradient clipping\n            if self.grad_clip is not None:\n                clip_gradient(self.optimizer, self.grad_clip)\n\n            # Update model parameters\n            self.optimizer.step()\n\n            # Update metrics\n            losses.update(loss.item(), images.size(0))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            # Print status\n            if i % self.print_freq == 0:\n                print('Epoch: [{0}][{1}/{2}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(self.train_loader),\n                                                                      batch_time=batch_time,\n                                                                      data_time=data_time,\n                                                                      loss=losses))\n\n        del predicted_locs, predicted_scores, images, boxes, labels\n\n    def save_checkpoint(self, epoch):\n        \"\"\"\n        Save model checkpoint.\n        \"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model': self.model,\n            'optimizer': self.optimizer,\n        }, f'checkpoint_epoch_{epoch}.pth')\n        print(f\"Checkpoint saved for epoch {epoch}.\")\n\n    def train(self, start_epoch=0):\n        \"\"\"\n        Train the model across all epochs.\n        \"\"\"\n        for epoch in range(start_epoch, self.epochs):\n            self.adjust_learning_rate(epoch)\n            self.train_one_epoch(epoch)\n            self.save_checkpoint(epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.972157Z","iopub.status.idle":"2024-12-11T15:01:54.972706Z","shell.execute_reply.started":"2024-12-11T15:01:54.972438Z","shell.execute_reply":"2024-12-11T15:01:54.972468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model parameters\nn_classes = len(label_map())  # number of different types of objects\n\n# Learning parameters\ncheckpoint = None  # path to model checkpoint, None if none\nbatch_size = 8  # batch size\niterations = 10  # number of iterations to train\nworkers = 4  # number of workers for loading data in the DataLoader\nprint_freq = 200  # print training status every __ batches\nlr = 1e-3  # learning rate\ndecay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\ndecay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\nmomentum = 0.9  # momentum\nweight_decay = 5e-4  # weight decay\ngrad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.973936Z","iopub.status.idle":"2024-12-11T15:01:54.974489Z","shell.execute_reply.started":"2024-12-11T15:01:54.974222Z","shell.execute_reply":"2024-12-11T15:01:54.974251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n\n# Ottimizzatore\nbiases = [param for name, param in model.named_parameters() if param.requires_grad and name.endswith('.bias')]\nnot_biases = [param for name, param in model.named_parameters() if param.requires_grad and not name.endswith('.bias')]\noptimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n                            lr=lr, momentum=momentum, weight_decay=weight_decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.975753Z","iopub.status.idle":"2024-12-11T15:01:54.976168Z","shell.execute_reply.started":"2024-12-11T15:01:54.975954Z","shell.execute_reply":"2024-12-11T15:01:54.975971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione e avvio del trainer\ntrainer = Trainer(model, train_dataset, criterion, optimizer, batch_size, workers, device, grad_clip=grad_clip)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.977345Z","iopub.status.idle":"2024-12-11T15:01:54.977731Z","shell.execute_reply.started":"2024-12-11T15:01:54.977557Z","shell.execute_reply":"2024-12-11T15:01:54.977575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing sulle predizioni","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, test_dataset, batch_size, num_workers, device):\n        \"\"\"\n        Initialize the Evaluator.\n        \n        :param model: Trained SSD model to be evaluated\n        :param test_dataset: Dataset object for testing\n        :param batch_size: Batch size for evaluation\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for evaluation ('cuda' or 'cpu')\n        \"\"\"\n        self.model = model.to(device)\n        self.test_dataset = test_dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.pp = PrettyPrinter()  # For printing APs nicely\n\n        # Prepare dataloader\n        self.test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=test_dataset.collate_fn,\n            num_workers=num_workers,\n            pin_memory=True\n        )\n\n    def evaluate(self):\n        \"\"\"\n        Perform evaluation and compute mAP.\n        \"\"\"\n        self.model.eval()\n\n        # Lists to store detected and true boxes, labels, scores\n        det_boxes = list()\n        det_labels = list()\n        det_scores = list()\n        true_boxes = list()\n        true_labels = list()\n        true_difficulties = list()\n\n        with torch.no_grad():\n            for i, (images, boxes, labels, difficulties) in enumerate(tqdm(self.test_loader, desc='Evaluating')):\n                images = images.to(self.device)\n\n                # Forward pass\n                predicted_locs, predicted_scores = self.model(images)\n\n                # Detect objects\n                det_boxes_batch, det_labels_batch, det_scores_batch = self.model.detect_objects(\n                    predicted_locs, predicted_scores,\n                    min_score=0.01, max_overlap=0.45, top_k=200\n                )\n\n                # Store this batch's results\n                boxes = [b.to(self.device) for b in boxes]\n                labels = [l.to(self.device) for l in labels]\n                difficulties = [d.to(self.device) for d in difficulties]\n\n                det_boxes.extend(det_boxes_batch)\n                det_labels.extend(det_labels_batch)\n                det_scores.extend(det_scores_batch)\n                true_boxes.extend(boxes)\n                true_labels.extend(labels)\n                true_difficulties.extend(difficulties)\n\n        # Calculate mAP\n        APs, mAP = self.calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n\n        # Print AP for each class\n        self.pp.pprint(APs)\n        print('\\nMean Average Precision (mAP): %.3f' % mAP)\n\n    @staticmethod\n    def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n        \"\"\"\n        Calculate Mean Average Precision (mAP).\n        Placeholder for an actual implementation.\n        \n        :param det_boxes: Detected boxes\n        :param det_labels: Detected labels\n        :param det_scores: Detected scores\n        :param true_boxes: Ground truth boxes\n        :param true_labels: Ground truth labels\n        :param true_difficulties: Ground truth difficulties\n        :return: APs and mAP\n        \"\"\"\n        # Replace this with your actual mAP calculation logic\n        APs = {f'class_{i}': 0.0 for i in range(1, 21)}  # Dummy values for each class\n        mAP = 0.0  # Dummy value for mAP\n        return APs, mAP\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.979649Z","iopub.status.idle":"2024-12-11T15:01:54.980022Z","shell.execute_reply.started":"2024-12-11T15:01:54.979840Z","shell.execute_reply":"2024-12-11T15:01:54.979858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Caricamento del modello\ncheckpoint = torch.load(checkpoint_path)\nmodel = checkpoint['model']\n\n\n# Creazione e avvio del valutatore\nevaluator = Evaluator(model=model, test_dataset=test_dataset, batch_size=64, num_workers=4, device=device)\nevaluator.evaluate()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.981530Z","iopub.status.idle":"2024-12-11T15:01:54.981894Z","shell.execute_reply.started":"2024-12-11T15:01:54.981713Z","shell.execute_reply":"2024-12-11T15:01:54.981730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n    \"\"\"\n    Calculate the Mean Average Precision (mAP) of detected objects.\n\n    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n\n    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n    :param true_difficulties: list of tensors, one tensor for each image containing actual objects' difficulty (0 or 1)\n    :return: list of average precisions for all classes, mean average precision (mAP)\n    \"\"\"\n    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(true_labels)  # these are all lists of tensors of the same length, i.e. number of images\n    n_classes = len(label_map)\n\n    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n    true_images = list()\n    for i in range(len(true_labels)):\n        true_images.extend([i] * true_labels[i].size(0))\n    true_images = torch.LongTensor(true_images).to(\n        device)  # (n_objects), n_objects is the total no. of objects across all images\n    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n\n    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n\n    # Store all detections in a single continuous tensor while keeping track of the image it is from\n    det_images = list()\n    for i in range(len(det_labels)):\n        det_images.extend([i] * det_labels[i].size(0))\n    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n\n    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n\n    # Calculate APs for each class (except background)\n    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n    for c in range(1, n_classes):\n        # Extract only objects with this class\n        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n\n        # Keep track of which true objects with this class have already been 'detected'\n        # So far, none\n        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(\n            device)  # (n_class_objects)\n\n        # Extract only detections with this class\n        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n        n_class_detections = det_class_boxes.size(0)\n        if n_class_detections == 0:\n            continue\n\n        # Sort detections in decreasing order of confidence/scores\n        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n\n        # In the order of decreasing scores, check if true or false positive\n        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        for d in range(n_class_detections):\n            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n            this_image = det_class_images[d]  # (), scalar\n\n            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n            # If no such object in this image, then the detection is a false positive\n            if object_boxes.size(0) == 0:\n                false_positives[d] = 1\n                continue\n\n            # Find maximum overlap of this detection with objects in this image of this class\n            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n\n            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n            # We need 'original_ind' to update 'true_class_boxes_detected'\n\n            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n            if max_overlap.item() > 0.5:\n                # If this object has already not been detected, it's a true positive\n                if true_class_boxes_detected[original_ind] == 0:\n                    true_positives[d] = 1\n                    true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n                # Otherwise, it's a false positive (since this object is already accounted for)\n                else:\n                    false_positives[d] = 1\n            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n            else:\n                false_positives[d] = 1\n\n        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n        cumul_precision = cumul_true_positives / (\n                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n\n        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n        for i, t in enumerate(recall_thresholds):\n            recalls_above_t = cumul_recall >= t\n            if recalls_above_t.any():\n                precisions[i] = cumul_precision[recalls_above_t].max()\n            else:\n                precisions[i] = 0.\n        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n\n    # Calculate Mean Average Precision (mAP)\n    mean_average_precision = average_precisions.mean().item()\n\n    # Keep class-wise average precisions in a dictionary\n    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n\n    return average_precisions, mean_average_precision\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:01:54.983571Z","iopub.status.idle":"2024-12-11T15:01:54.983921Z","shell.execute_reply.started":"2024-12-11T15:01:54.983755Z","shell.execute_reply":"2024-12-11T15:01:54.983772Z"}},"outputs":[],"execution_count":null}]}