{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library import","metadata":{}},{"cell_type":"markdown","source":"### import librerie","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nimport json\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nimport ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:17.094820Z","iopub.execute_input":"2024-12-09T14:22:17.095072Z","iopub.status.idle":"2024-12-09T14:22:24.120201Z","shell.execute_reply.started":"2024-12-09T14:22:17.095047Z","shell.execute_reply":"2024-12-09T14:22:24.119406Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:24.121835Z","iopub.execute_input":"2024-12-09T14:22:24.122578Z","iopub.status.idle":"2024-12-09T14:22:41.218606Z","shell.execute_reply.started":"2024-12-09T14:22:24.122536Z","shell.execute_reply":"2024-12-09T14:22:41.217575Z"}},"outputs":[{"name":"stdout","text":"Collecting triton\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.15.1)\nDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\nSuccessfully installed triton-3.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Path","metadata":{}},{"cell_type":"code","source":"# file contenente i path delle immagini del dataset\ntxt_file = \"/kaggle/input/our-xview-dataset/xView_class_map.json\"\nimg_dir = \"/kaggle/input/our-xview-dataset/images\"\n\nannotation_file = \"/kaggle/input/our-xview-dataset/COCO_annotations_new.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:41.219916Z","iopub.execute_input":"2024-12-09T14:22:41.220209Z","iopub.status.idle":"2024-12-09T14:22:41.224486Z","shell.execute_reply.started":"2024-12-09T14:22:41.220183Z","shell.execute_reply":"2024-12-09T14:22:41.223679Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils', trust_repo=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:41.226349Z","iopub.execute_input":"2024-12-09T14:22:41.226593Z","iopub.status.idle":"2024-12-09T14:22:48.970837Z","shell.execute_reply.started":"2024-12-09T14:22:41.226570Z","shell.execute_reply":"2024-12-09T14:22:48.969943Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n  warnings.warn(\n/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, utils, aug=False):\n        \"\"\"\n        Args:\n            annotations_file (str): Path al file JSON delle annotazioni (es. formato COCO).\n            img_dir (str): Path alla directory delle immagini.\n            utils: Funzioni di utilità per il dataset.\n            aug (bool): Flag per attivare le trasformazioni di data augmentation.\n        \"\"\"\n        with open(annotations_file, 'r') as f:\n            self.annotations = json.load(f)\n        self.img_dir = img_dir\n        self.utils = utils\n        self.aug = aug\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),  # Converte in formato [C, H, W]\n            transforms.Resize((320, 320)),  # Ridimensiona l'immagine\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.annotations['images'])\n\n    def __getitem__(self, idx):\n        # Leggi i dettagli dell'immagine\n        img_info = self.annotations['images'][idx]\n        img_path = os.path.join(self.img_dir, img_info['file_name'])\n\n        # Caricamento immagine\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            raise ValueError(f\"Errore nel caricamento dell'immagine: {img_path}, errore: {e}\")\n\n        # Trasforma l'immagine\n        if self.aug:\n            #image = self.utils.augment(image)  # Applica data augmentation, se definita\n            image = self.transform(image)\n\n        # Check sul formato dell'immagine\n        if not isinstance(image, torch.Tensor):\n            raise ValueError(f\"L'immagine non è un tensore: {type(image)}\")\n        if image.shape[0] != 3:\n            raise ValueError(f\"L'immagine ha un numero di canali errato: {image.shape[0]} (atteso: 3)\")\n\n        # Leggi le annotazioni\n        img_id = img_info['id']\n        annotations = [ann for ann in self.annotations['annotations'] if ann['image_id'] == img_id]\n\n        # Bounding box e label\n        if len(annotations) == 0:\n            bboxes = np.zeros((0, 4), dtype=np.float32)\n            labels = np.zeros((0,), dtype=np.int64)\n        else:\n            try:\n                bboxes = np.array(\n                    [ast.literal_eval(ann['bbox']) if isinstance(ann['bbox'], str) else ann['bbox'] for ann in annotations],\n                    dtype=np.float32\n                )\n                labels = np.array([ann['category_id'] for ann in annotations], dtype=np.int64)\n                bboxes[:, 2:] += bboxes[:, :2]  # Converti nel formato (x_min, y_min, x_max, y_max)\n            except Exception as e:\n                raise ValueError(f\"Errore nel parsing delle annotazioni: {annotations}, errore: {e}\")\n\n        # Check sui bounding box\n        if bboxes.ndim != 2 or bboxes.shape[1] != 4:\n            raise ValueError(f\"Bounding box in formato errato: {bboxes.shape} (atteso: [N, 4])\")\n\n        # Check sulle label\n        if labels.ndim != 1:\n            raise ValueError(f\"Labels in formato errato: {labels.shape} (atteso: [N])\")\n        if not np.issubdtype(labels.dtype, np.integer):\n            raise ValueError(f\"Labels non sono interi: {labels.dtype}\")\n\n        # Costruzione del target\n        target = {\n            \"image\": image,\n            'boxes': torch.tensor(bboxes, dtype=torch.float32),\n            'labels': torch.tensor(labels, dtype=torch.int64),\n        }\n\n        return target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:48.972178Z","iopub.execute_input":"2024-12-09T14:22:48.972972Z","iopub.status.idle":"2024-12-09T14:22:49.108562Z","shell.execute_reply.started":"2024-12-09T14:22:48.972932Z","shell.execute_reply":"2024-12-09T14:22:49.107571Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def collate_fn(batch):\n    # Separate images and targets from the batch\n    images = [item['image'] for item in batch]\n    targets = [item for item in batch]\n    \n    # Stack images into a tensor of shape (batch_size, C, H, W)\n    images = torch.stack(images, dim=0)\n    \n    # Prepare boxes and labels lists\n    boxes_list = [target['boxes'] for target in targets]\n    labels_list = [target['labels'] for target in targets]\n    \n    # Return images and targets (without using stack)\n    return images, [{'boxes': boxes, 'labels': labels} for boxes, labels in zip(boxes_list, labels_list)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:49.109668Z","iopub.execute_input":"2024-12-09T14:22:49.110018Z","iopub.status.idle":"2024-12-09T14:22:49.145860Z","shell.execute_reply.started":"2024-12-09T14:22:49.109982Z","shell.execute_reply":"2024-12-09T14:22:49.145238Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(annotation_file, img_dir, utils, aug=True) \nvalid_dataset = CustomDataset(annotation_file, img_dir, utils, aug=False)  \ntest_dataset = CustomDataset(annotation_file, img_dir, utils, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:49.146777Z","iopub.execute_input":"2024-12-09T14:22:49.147038Z","iopub.status.idle":"2024-12-09T14:22:53.763492Z","shell.execute_reply.started":"2024-12-09T14:22:49.147014Z","shell.execute_reply":"2024-12-09T14:22:53.762789Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Network","metadata":{}},{"cell_type":"code","source":"class SSDModel(nn.Module):\n    def __init__(self, num_classes):\n        super(SSDModel, self).__init__()\n\n        ## Model -> per info https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD\n        self.ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd') # modello pre-addestrato su dataset COCO\n\n    def forward(self, images):\n\n        # Calcola le previsioni con il modello SSD\n        predictions = self.ssd_model(images)  # Output grezzo del modello SSD\n        \n        return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:53.764465Z","iopub.execute_input":"2024-12-09T14:22:53.764723Z","iopub.status.idle":"2024-12-09T14:22:53.770070Z","shell.execute_reply.started":"2024-12-09T14:22:53.764698Z","shell.execute_reply":"2024-12-09T14:22:53.769222Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class SSDLoss(nn.Module):\n    \n    def __init__(self):\n        super(SSDLoss, self).__init__()\n        self.smooth_l1 = nn.SmoothL1Loss(reduction='sum')  # Per bounding boxes\n        self.cross_entropy = nn.CrossEntropyLoss(reduction='sum')  # Per classificazione\n\n    def forward(self, predictions, targets):\n        loc_preds, conf_preds = predictions\n        loc_targets, conf_targets = targets\n        \n        # Calcola la perdita di regressione (bounding boxes)\n        loc_loss = self.smooth_l1(loc_preds, loc_targets)\n        \n        # Calcola la perdita di classificazione (confidence scores)\n        conf_loss = self.cross_entropy(conf_preds, conf_targets)\n        \n        # Restituisci la perdita combinata\n        return loc_loss + conf_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:53.771035Z","iopub.execute_input":"2024-12-09T14:22:53.771305Z","iopub.status.idle":"2024-12-09T14:22:53.795679Z","shell.execute_reply.started":"2024-12-09T14:22:53.771281Z","shell.execute_reply":"2024-12-09T14:22:53.794658Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_loader, val_loader, criterion, optimizer=None, device='cuda', checkpoint_dir='checkpoints'):\n        \"\"\"\n        Inizializza la classe Trainer.\n        \n        Args:\n            model: Il modello SSD.\n            train_loader: DataLoader per il training set.\n            val_loader: DataLoader per il validation set.\n            criterion: Funzione di perdita.\n            optimizer: Ottimizzatore (opzionale, di default Adam).\n            device: Dispositivo per il calcolo ('cuda' o 'cpu').\n            checkpoint_dir: Directory per salvare i checkpoint.\n        \"\"\"\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.criterion = criterion\n        self.optimizer = optimizer if optimizer else Adam(model.parameters(), lr=1e-4)\n        self.device = device\n        self.checkpoint_dir = checkpoint_dir\n        \n        if not os.path.exists(self.checkpoint_dir):\n            os.makedirs(self.checkpoint_dir)\n\n    def process_targets(self, raw_targets, anchor_boxes):\n        loc_targets = []\n        conf_targets = []\n    \n        for target in raw_targets:\n            gt_boxes = target[\"boxes\"]\n            gt_labels = target[\"labels\"]\n    \n            # Effettua il matching\n            matched_boxes = match_anchors_to_targets(anchor_boxes, gt_boxes)\n            matched_labels = match_labels_to_anchors(anchor_boxes, gt_boxes, gt_labels)\n    \n            loc_targets.append(matched_boxes)\n            conf_targets.append(matched_labels)\n    \n        # Concatenare per ottenere un tensore compatibile\n        loc_targets = torch.stack(loc_targets, dim=0)\n        conf_targets = torch.stack(conf_targets, dim=0)\n    \n        return loc_targets, conf_targets\n\n    def match_anchors_to_targets(anchor_boxes, gt_boxes, iou_threshold=0.5):\n        \"\"\"\n        Associa bounding box reali (gt_boxes) agli anchor box predefiniti.\n        \"\"\"\n        num_anchors = anchor_boxes.shape[0]\n        num_gt_boxes = gt_boxes.shape[0]\n    \n        # Calcola l'IoU tra ogni anchor e ogni gt_box\n        iou_matrix = calculate_iou(anchor_boxes, gt_boxes)\n    \n        # Trova il miglior match per ogni anchor\n        best_gt_idx = iou_matrix.argmax(dim=1)\n        best_anchors_idx = iou_matrix.argmax(dim=0)\n    \n        # Applica la soglia IoU per il matching\n        matched_gt_boxes = gt_boxes[best_gt_idx]\n        matched_gt_boxes[iou_matrix.max(dim=1).values < iou_threshold] = 0  # Imposta a 0 per match deboli\n    \n        return matched_gt_boxes\n\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        epoch_loss = 0\n        for images, targets in self.train_loader:\n            images = images.to(self.device)\n            \n            # Processa i targets (in questo caso non vengono 'stackati', sono una lista)\n            processed_targets = self.process_targets(targets)\n            \n            # Esegui il forward pass\n            predictions = self.model(images)\n            \n            # Calcola la perdita (utilizzando il formato richiesto dal modello)\n            loss = self.criterion(predictions, processed_targets)\n            \n            # Ottimizzazione\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        return epoch_loss\n\n\n\n    \n    def validate_one_epoch(self, epoch):\n        self.model.eval()\n        running_loss = 0.0\n        pbar = tqdm(self.val_loader, desc=f\"Validation Epoch {epoch}\")\n        \n        with torch.no_grad():\n            for images, targets in pbar:\n                images = images.to(self.device)\n                processed_targets = self.process_targets(targets)\n                \n                predictions = self.model(images)\n                loss = self.compute_loss(predictions, processed_targets)\n                \n                running_loss += loss.item()\n                pbar.set_postfix({\"val_loss\": running_loss / len(self.val_loader)})\n        \n        return running_loss / len(self.val_loader)\n\n    \n    def save_checkpoint(self, epoch, train_loss, val_loss, predictions=None, targets=None, best=False):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'predictions': predictions,\n            'targets': targets\n        }\n        filename = f\"best_model.pth\" if best else f\"checkpoint_epoch_{epoch}.pth\"\n        path = os.path.join(self.checkpoint_dir, filename)\n        torch.save(checkpoint, path)\n        print(f\"Checkpoint salvato: {path}\")\n\n    \n    def fit(self, epochs):\n        \"\"\"\n        Esegue l'addestramento e la validazione per un dato numero di epoche.\n        \"\"\"\n        train_losses = []\n        val_losses = []\n        best_val_loss = float('inf')\n        \n        for epoch in range(1, epochs + 1):\n            train_loss = self.train_one_epoch(epoch)\n            val_loss = self.validate_one_epoch(epoch)\n            \n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            \n            # Salvataggio del checkpoint per ogni epoca\n            self.save_checkpoint(epoch, train_loss, val_loss)\n            \n            # Salvataggio del miglior modello\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                self.save_checkpoint(epoch, train_loss, val_loss, best=True)\n            \n            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n        \n        return train_losses, val_losses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:53.798940Z","iopub.execute_input":"2024-12-09T14:22:53.799228Z","iopub.status.idle":"2024-12-09T14:22:53.820355Z","shell.execute_reply.started":"2024-12-09T14:22:53.799204Z","shell.execute_reply":"2024-12-09T14:22:53.819458Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"num_classes = 12\nssd_model = SSDModel(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:53.821691Z","iopub.execute_input":"2024-12-09T14:22:53.822245Z","iopub.status.idle":"2024-12-09T14:22:56.982292Z","shell.execute_reply.started":"2024-12-09T14:22:53.822208Z","shell.execute_reply":"2024-12-09T14:22:56.981556Z"}},"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 188MB/s] \nDownloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/ssd_pyt_ckpt_amp/versions/20.06.0/files/nvidia_ssdpyt_amp_200703.pt\n/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Detection/SSD/ssd/entrypoints.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(ckpt_file)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Addestra il modello, con validazione ad ogni epoca\nloss = SSDLoss()\n\n# Inizializza il trainer\ntrainer = Trainer(\n    model=ssd_model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=loss,\n    optimizer=Adam(ssd_model.parameters(), lr=1e-4),\n    device='cuda'\n)\n\n# Avvia il training\ntrain_losses, val_losses = trainer.fit(epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:22:56.983398Z","iopub.execute_input":"2024-12-09T14:22:56.983766Z","iopub.status.idle":"2024-12-09T14:22:59.190720Z","shell.execute_reply.started":"2024-12-09T14:22:56.983712Z","shell.execute_reply":"2024-12-09T14:22:59.189516Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mssd_model,\n\u001b[1;32m      7\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Avvia il training\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 139\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    136\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 139\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_one_epoch(epoch)\n\u001b[1;32m    142\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","Cell \u001b[0;32mIn[10], line 75\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     72\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Processa i targets (in questo caso non vengono 'stackati', sono una lista)\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m processed_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Esegui il forward pass\u001b[39;00m\n\u001b[1;32m     78\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n","\u001b[0;31mTypeError\u001b[0m: Trainer.process_targets() missing 1 required positional argument: 'anchor_boxes'"],"ename":"TypeError","evalue":"Trainer.process_targets() missing 1 required positional argument: 'anchor_boxes'","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}