{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def init(self, txt_file, img_dir, coco_json_file, aug=False):\n        \"\"\"\n        Inizializza il dataset personalizzato.\n        Args:\n        - txt_file: Il file di testo contenente i percorsi delle immagini.\n        - img_dir: La cartella delle immagini.\n        - coco_json_file: Il file JSON in formato COCO contenente le annotazioni.\n        - aug: Booleano per attivare o meno l'augmentazione.\n        \"\"\"\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n \n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n \n        self.img_dir = img_dir\n \n        # Carica il file JSON delle annotazioni COCO\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n \n        # Crea una struttura per le annotazioni\n        self.image_annotations = {}\n        self.image_bboxes = {}\n \n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox = annotation['bbox']  # Formato COCO [x_min, y_min, width, height]\n \n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n \n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n \n        # Mappa per associare ID immagine a file_name\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n \n        # Trasformazioni di base e di augmentation\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),   \n        ])\n \n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n \n        self.aug = aug\n \n    def len(self):\n        return len(self.image_paths)\n \n    def getitem(self, index):\n        # Estrai il nome dell'immagine e l'ID corrispondente\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        # Carica l'immagine\n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        # Applica le trasformazioni\n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        # Estrai le annotazioni e i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        if not bboxes:  # Immagini senza annotazioni\n            target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros((0,), dtype=torch.int64)\n            }\n        else:\n            # Converte da formato COCO [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            scale_x = 320 / original_width\n            scale_y = 320 / original_height\n            scaled_bboxes = [\n                torch.tensor([  # [x1, y1, x2, y2]\n                    bbox[0] * scale_x,               # x_min\n                    bbox[1] * scale_y,               # y_min\n                    (bbox[0] + bbox[2]) * scale_x,   # x_max\n                    (bbox[1] + bbox[3]) * scale_y    # y_max\n                ], dtype=torch.float32)\n                for bbox in bboxes\n            ]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di collation per il DataLoader, utile per il batching di immagini e annotazioni.\n    La funzione restituirà un batch di immagini e un batch di target, formattato correttamente per Faster R-CNN.\n    \n    Args:\n    - batch: lista di tuple (image, target)\n    \n    Returns:\n    - images: batch di immagini\n    - targets: lista di dizionari contenenti le annotazioni per ogni immagine\n    \"\"\"\n    # Separa immagini e target\n    images, targets = zip(*batch)\n\n    # Converte la lista di immagini in un batch di immagini\n    images = list(images)\n    \n    # Varia le dimensioni delle immagini per garantire che siano tutte della stessa dimensione\n    # La dimensione di uscita deve essere la stessa per tutte le immagini del batch\n    images = [F.resize(img, (320, 320)) for img in images]  # Assumendo che 320 sia la dimensione target\n\n    # Restituisci il batch\n    return images, list(targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset_frcc = CustomDataset(train_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=True)\nvalid_dataset_frcc = CustomDataset(val_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \ntest_dataset_frcc = CustomDataset(test_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader_frcc = DataLoader(train_dataset_frcc, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader_frcc = DataLoader(valid_dataset_frcc, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader_frcc = DataLoader(test_dataset_frcc, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader_frcc.dataset)\nval_size = len(val_loader_frcc.dataset)\ntest_size = len(test_loader_frcc.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader_frcc)\nval_batches = len(val_loader_frcc)\ntest_batches = len(test_loader_frcc)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_txt_vs_json(txt_paths, coco_data):\n    \"\"\"\n    Controlla se le immagini del JSON sono presenti in almeno uno dei file TXT.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n        coco_data (dict): Dati in formato COCO.\n    \"\"\"\n    # Estrai i nomi delle immagini dal JSON\n    image_names = [image['file_name'] for image in coco_data['images']]\n    \n    # Inizializza un set per contenere tutte le immagini presenti nei TXT\n    txt_image_names = set()\n    \n    # Leggi i nomi delle immagini da ciascun file TXT\n    for txt_path in txt_paths:\n        with open(txt_path, 'r') as f:\n            txt_image_names.update(os.path.basename(line.strip()) for line in f.readlines())\n    \n    # Trova le immagini presenti nel JSON ma non in nessuno dei TXT\n    missing_in_txts = [name for name in image_names if name not in txt_image_names]\n    \n    # Verifica e stampa i risultati\n    print(\"\\nControllo completato:\")\n    if missing_in_txts:\n        print(f\"Errore: le seguenti immagini non sono presenti in nessuno dei file TXT forniti:\\n{missing_in_txts}\")\n    else:\n        print(\"Tutte le immagini del JSON sono presenti in almeno uno dei file TXT.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_paths = [train_txt_pth, val_txt_pth, test_txt_pth]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_txt_vs_json(txt_paths, coco_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verifica il DataLoader\ndef check_dataloader(dataloader, num_batches=5):\n    for i, (images, targets) in enumerate(dataloader):\n        # Check numero di batch\n        if i >= num_batches:\n            break\n        \n        # Verifica che le immagini siano nel formato corretto [N, C, H, W]\n        print(f\"Batch {i + 1}:\")\n        print(f\"  Numero di immagini: {len(images)}\")\n        print(f\"  Dimensioni dell'immagine: {images[0].shape}\")  # Forma della prima immagine nel batch\n        \n        # Verifica che i target siano nel formato corretto\n        print(f\"  Numero di target: {len(targets)}\")\n        print(f\"  Bounding boxes: {targets[0]['boxes'].shape}\")  # Dimensione dei bounding boxes\n        print(f\"  Etichette: {targets[0]['labels'].shape}\")  # Dimensione delle etichette\n        \n        # Check che i bounding boxes abbiano il formato corretto\n        for target in targets:\n            assert target['boxes'].shape[1] == 4, \"Le dimensioni dei bounding box non sono corrette!\"\n            assert target['labels'].dtype == torch.int64, \"Le etichette non sono di tipo int64!\"\n        \n        print(\"  -----\")\n\ndef visualize_batch(images, targets, num_images=3):\n    for i in range(min(num_images, len(images))):\n        # Converti la prima immagine del batch in numpy (per matplotlib)\n        image = images[i].cpu().numpy().transpose(1, 2, 0)  # [C, H, W] -> [H, W, C]\n        image = np.clip(image, 0, 1)  # Assicurati che i valori siano nel range [0, 1]\n\n        # Ottieni i bounding boxes\n        boxes = targets[i]['boxes'].cpu().numpy()\n        labels = targets[i]['labels'].cpu().numpy()\n\n        # Visualizza l'immagine\n        plt.figure(figsize=(6, 6))\n        plt.imshow(image)\n        \n        # Aggiungi i bounding boxes\n        for box, label in zip(boxes, labels):\n            x_min, y_min, x_max, y_max = box\n            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n                                              linewidth=2, edgecolor='r', facecolor='none'))\n            plt.text(x_min, y_min, f\"Class: {label}\", color='red', fontsize=12)\n        \n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_dataloader(train_loader_frcc)\ncheck_dataloader(val_loader_frcc)\ncheck_dataloader(test_loader_frcc)\n\n# Visualizza alcune immagini con i bounding box\nfor images, targets in train_loader_frcc:\n    visualize_batch(images, targets)\n    break  # Visualizza solo il primo batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modello Faster R-CNN (Resnet50)","metadata":{}},{"cell_type":"code","source":"from torchvision import models\n\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\nfrom torchvision.models.detection import FasterRCNN\n\nfrom torch import optim\n \n# Carica il modello Faster R-CNN con ResNet50 e FPN\n\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n \nnum_classes = 12  # 11 classi + 1 per il background\n \n# Modifica il numero di classi in output\n\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n \n# Imposta il dispositivo (GPU o CPU)\n\ndevice = torch.device('cuda')\n \n# Definisci l'ottimizzatore (esempio con AdamW)\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n \n# Esegui un ciclo di addestramento\n\nnum_epochs = 10  # Modifica a seconda del tuo caso\n\nfor epoch in range(num_epochs):\n\n    model.train()  # Imposta il modello in modalità training\n\n    total_loss = 0  # Per monitorare la perdita totale per epoca\n \n    for images, targets in train_loader:\n\n        images = [image.to(device) for image in images]  # Trasferisci le immagini al dispositivo\n\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Trasferisci i target al dispositivo\n \n        # Ottimizza il modello\n\n        loss_dict = model(images, targets)  # Calcola le perdite\n\n        losses = sum(loss for loss in loss_dict.values())  # Somma le perdite\n \n        # Ottimizza\n\n        optimizer.zero_grad()\n\n        losses.backward()\n\n        optimizer.step()\n \n        total_loss += losses.item()\n \n    print(f\"Epoca {epoch + 1}/{num_epochs}, Perdita Totale: {total_loss}\")\n \n    # Valutazione dopo ogni epoca\n\n    model.eval()  # Imposta il modello in modalità valutazione\n\n    with torch.no_grad():\n\n        for images, targets in val_loader:\n\n            images = [image.to(device) for image in images]\n\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Calcola le predizioni per la valutazione\n\n            prediction = model(images)\n \n    # Salva il modello dopo ogni epoca (opzionale)\n\n    torch.save(model.state_dict(), f\"model_epoch_{epoch}.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}