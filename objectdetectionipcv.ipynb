{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2571636,"sourceType":"datasetVersion","datasetId":1561333}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Librerie**","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:23.803347Z","iopub.execute_input":"2024-11-29T07:40:23.803621Z","iopub.status.idle":"2024-11-29T07:40:35.216779Z","shell.execute_reply.started":"2024-11-29T07:40:23.803594Z","shell.execute_reply":"2024-11-29T07:40:35.215857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport imageio.v3 as imageio\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nimport cv2\nimport shutil\nimport json\nimport yaml\nimport random\nimport time\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\nimport concurrent.futures\nimport multiprocessing as mp\nfrom PIL import Image, ImageOps\nfrom collections import defaultdict, Counter\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TF\nimport torch.optim as optim\nimport re\nimport selectivesearch\nimport torch.optim as optim\nfrom torchvision import models\nfrom torchvision.models import AlexNet_Weights\nimport matplotlib.patches as mpatches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:17:32.526634Z","iopub.execute_input":"2024-11-29T11:17:32.527264Z","iopub.status.idle":"2024-11-29T11:17:32.533720Z","shell.execute_reply.started":"2024-11-29T11:17:32.527228Z","shell.execute_reply":"2024-11-29T11:17:32.532748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Dataset Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"Il nostro dataset è xView, un'analisi preliminare del dataset è presente nel [documento](https://medium.com/picterra/the-xview-dataset-and-baseline-results-5ab4a1d0f47f) linkato.\n\n\nQuesto dataset rappresentava il migliore per il benchmarking di visione artificiale satellitare. La documentazione tecnica spiega che i dati sono ottenuti dai satelliti **WorldView-3**, con una distanza di campionamento al suolo uniforme di 0,3 metri. Ciò conferisce una risoluzione più elevata e omogenea rispetto alla maggior parte degli altri dataset satellitari esistenti all'epoca, molti dei quali si basano invece su fotografie aeree. Quest'ultime, infatti, presentano differenze nella distorsione causate dall'angolo di ripresa, poiché sono scattate da velivoli a bassa quota. \n\nIl dataset **xView** offre una copertura geografica ampia e diversificata, includendo anche aree meno sviluppate e urbanizzate, fornendo quindi una maggiore varietà di scenari rispetto a dataset più convenzionali. ","metadata":{}},{"cell_type":"markdown","source":"### Elenco delle categorie del dataset:  \n\n#### 0-3: Velivoli  \n- 0: Fixed-wing Aircraft  \n- 1: Small Aircraft  \n- 2: Passenger/Cargo Plane  \n- 3: Helicopter  \n\n#### 4-15: Veicoli terrestri  \n- 4: Passenger Vehicle  \n- 5: Small Car  \n- 6: Bus  \n- 7: Pickup Truck  \n- 8: Utility Truck  \n- 9: Truck  \n- 10: Cargo Truck  \n- 11: Truck Tractor w/ Box Trailer  \n- 12: Truck Tractor  \n- 13: Trailer  \n- 14: Truck Tractor w/ Flatbed Trailer  \n- 15: Truck Tractor w/ Liquid Tank  \n\n#### 16: Veicoli speciali  \n- 16: Crane Truck  \n\n#### 17-22: Veicoli ferroviari  \n- 17: Railway Vehicle  \n- 18: Passenger Car  \n- 19: Cargo/Container Car  \n- 20: Flat Car  \n- 21: Tank car  \n- 22: Locomotive  \n\n#### 23-32: Imbarcazioni  \n- 23: Maritime Vessel  \n- 24: Motorboat  \n- 25: Sailboat  \n- 26: Tugboat  \n- 27: Barge  \n- 28: Fishing Vessel  \n- 29: Ferry  \n- 30: Yacht  \n- 31: Container Ship  \n- 32: Oil Tanker  \n\n#### 33-45: Veicoli da costruzione/industriali  \n- 33: Engineering Vehicle  \n- 34: Tower crane  \n- 35: Container Crane  \n- 36: Reach Stacker  \n- 37: Straddle Carrier  \n- 38: Mobile Crane  \n- 39: Dump Truck  \n- 40: Haul Truck  \n- 41: Scraper/Tractor  \n- 42: Front loader/Bulldozer  \n- 43: Excavator  \n- 44: Cement Mixer  \n- 45: Ground Grader  \n\n#### 46-51: Edifici e strutture  \n- 46: Hut/Tent  \n- 47: Shed  \n- 48: Building  \n- 49: Aircraft Hangar  \n- 50: Damaged Building  \n- 51: Facility  \n\n#### 52-59: Altre infrastrutture  \n- 52: Construction Site  \n- 53: Vehicle Lot  \n- 54: Helipad  \n- 55: Storage Tank  \n- 56: Shipping container lot  \n- 57: Shipping Container  \n- 58: Pylon  \n- 59: Tower  ","metadata":{}},{"cell_type":"markdown","source":"Utilizziamo un processo di preprocessing per il dataset **xView** seguendo i passi presenti in questo [Notebook (Preprocessing)](https://www.kaggle.com/code/ollypowell/xview-dataset-to-yolo-and-coco-format). L'obiettivo è:\n\n1. **Pulizia e riformattazione**: Si parte dal dataset grezzo e lo si prepara per l'addestramento di modelli di intelligenza artificiale per il rilevamento di oggetti.\n2. **Suddivisione in chunck**: Poiché le immagini satellitari sono molto grandi, vengono suddivise in \"pezzi\" più piccoli (chunck) per facilitare l'elaborazione. I bounding box, che definiscono le posizioni degli oggetti nelle immagini, vengono ridimensionati in modo che corrispondano ai nuovi pezzi.\n3. **Ottimizzazione del formato**: Le immagini TIFF, che occupano molto spazio, vengono convertite in formato JPG, molto più leggero.\n4. **Personalizzazione**: È possibile scegliere la dimensione delle immagini finali e come suddividere i dati per l'addestramento.\n\nQuesto processo riduce significativamente la dimensione del dataset, rendendolo più gestibile, senza sacrificare le informazioni necessarie per l'addestramento di modelli come **YOLOv5**.\n","metadata":{}},{"cell_type":"markdown","source":"Dopo aver analizzato le considerazioni riportate durante l'analisi esplorativa del dataset nel seguente [Notebook (Exploratory Data Analysis)](https://www.kaggle.com/code/ollypowell/xview-1-dataset-eda?scriptVersionId=157247786), abbiamo deciso di apportare alcune modifiche al preprocessing originale descritto nel documento linkato.\n\nIn particolare, abbiamo osservato che circa il 41% delle porzioni (chunk) di immagini con dimensioni comprese tra 600x600 e 640x640 risultavano prive di label. Questo è un problema, poiché il dataset originale è stato costruito raccogliendo immagini contenenti almeno una label. Per migliorare la qualità del dataset, abbiamo scelto di suddividere le immagini originali in chunk più piccoli, di dimensioni 320x320, ed eliminare il 66% delle immagini prive di label. Questo approccio ci ha permesso di ottenere un dataset composto per circa il 70% da immagini contenenti oggetti da identificare e per il 30% da immagini vuote. L'obiettivo è ottenere un training della rete più coerente ed efficace.\n\nInoltre, abbiamo corretto alcuni errori nel preprocessing originale, che impedivano di escludere i chunk con dimensioni inferiori a una certa soglia, errori nei path della working space ed altri.\n\nLa scelta di utilizzare chunk più piccoli si basa sulla considerazione che suddividere le immagini in parti di dimensioni ridotte consente di massimizzare la probabilità di includere tutte le label presenti nel dataset originale. Inoltre, ciò facilita la visualizzazione degli oggetti da individuare, sia per l'algoritmo che per un osservatore umano.\n\nAbbiamo infine deciso di escludere i chunk con dimensioni comprese nell'intervallo [300x300, 320x320), poiché rappresentavano una quantità trascurabile rispetto al totale. Gestirli avrebbe richiesto ulteriori scelte progettuali, come l'applicazione di tecniche di stretching sia sulle immagini sia sulle coordinate geometriche delle label, con un impatto complessivo marginale sull'efficacia del modello.\n\nNel seguente codice sono inoltre presenti molte operazioni di debug e di check per evitare che errori nella fase di preprocessing si ripercuotano sul successivo training.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"#Data sources\nDATA_FLDR_NM = 'Data'\nIN_DATASET_NM = 'xview-dataset'\nIMAGE_FLDR_NM = 'train_images'\nIN_LABELS_FLDR_NM = 'train_labels'\nLABELS_XML_NM = 'xView_train.geojson'\n\n#Output folders and file names\nOUT_DATASET_NM = 'xview-yolo-dataset'\nCLASS_MAP_JSON_NM = 'xView_class_map.json'\nOUT_COCO_JSON_NM = 'COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nOUT_DATAFRAME_NM = 'xview_labels.parquet'\nYAML_NM = 'xview_yolo.yaml'\nCHUNK_WIDTH = 320  # width of the images being created\nCHUNK_HEIGHT = 320\nMIN_CHUNK_HEIGHT = 320 # no images will be kept if the image chunk is smaller than this\nMIN_CHUNK_WIDTH = 320\nIMAGE_WRITING = True #True to re-perform image cropping, False just to regenerated other data\nTEST_FRACTION = 0.1\nJPEG_COMPRESSION = 95 # For the saved files\nVAL_FRACTION = 0.1\nRANDOM_SEED = 2023\nDEBUG = False\n\nin_dataset_pth = Path('/kaggle/input/xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nfuture_ds_img_fldr = Path(f'/kaggle/working/{OUT_IMAGE_FLDR_NM}')\nfuture_ds_cfg_fldr = Path(f'/kaggle/working/{OUT_CFG_FLDR_NM}')\n\nlabels_json_pth = in_dataset_pth / IN_LABELS_FLDR_NM / LABELS_XML_NM\nimg_fldr_pth = in_dataset_pth / IMAGE_FLDR_NM / IMAGE_FLDR_NM\nsave_images_fldr_pth = out_dataset_pth / OUT_IMAGE_FLDR_NM \nout_data_parquet_pth = out_dataset_pth / OUT_DATAFRAME_NM\nout_json_map_pth = out_dataset_pth / CLASS_MAP_JSON_NM \nclass_map_pth = out_dataset_pth / CLASS_MAP_JSON_NM\ncfg_fldr_pth = out_dataset_pth / OUT_CFG_FLDR_NM\ncoco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nyolo_yaml_pth = cfg_fldr_pth / YAML_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\ndef make_empty_dir(directory):\n    if directory.is_dir():\n        shutil.rmtree(directory)\n    os.makedirs(directory)\n\nmake_empty_dir(cfg_fldr_pth)\nif IMAGE_WRITING:\n    make_empty_dir(save_images_fldr_pth)\n\nrandom.seed(RANDOM_SEED)\n\nprint(f'The input images are found at {cfg_fldr_pth}')\nprint(f'The input labels are found at  {labels_json_pth}')\nprint(f'Configuration files will be saved to {cfg_fldr_pth}')\nprint(f'YOLO image files will be saved to {save_images_fldr_pth}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:40.333532Z","iopub.execute_input":"2024-11-29T07:40:40.334276Z","iopub.status.idle":"2024-11-29T07:40:40.343887Z","shell.execute_reply.started":"2024-11-29T07:40:40.334235Z","shell.execute_reply":"2024-11-29T07:40:40.343092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"# Estrae i bounding box da un DataFrame, eventualmente filtrandoli per una lista di classi\ndef get_boxes(in_df, class_lst=[]):\n    if class_lst:\n        in_df = in_df[in_df['TYPE_ID'].isin(class_lst)]     # Filtra il DataFrame per le classi specificate\n    unique_images = in_df.IMAGE_ID.unique().tolist()        # Ottiene una lista unica di ID immagine\n    boxs = {}\n\n    for image in tqdm_notebook(unique_images):\n        mask = in_df['IMAGE_ID'] == image                                  # Seleziona solo le righe relative a una specifica immagine\n        masked = in_df[mask][['TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX']]  # Estrae solo le colonne richieste\n        boxs[image] = masked.values.tolist()                               # Converte i valori in lista e li salva nel dizionario\n    return boxs\n\n\n# Stampa le prime n righe di un file di testo\ndef print_first_n_lines(file_path, n):\n    try:\n        with open(file_path, 'r') as file:\n            for line_num, line in enumerate(file, 1):\n                if line_num > n:                  \n                    break\n                print(line.strip()) \n    except FileNotFoundError:\n        print('Unable to open file')        \n\n\n# Carica un'immagine per la visualizzazione (convertita da BGR a RGB)\ndef load_image(file_pth):\n    image_obj = cv2.imread(file_pth)                        \n    image_obj = cv2.cvtColor(image_obj, cv2.COLOR_BGR2RGB)  \n    return image_obj\n\n\n# Carica un'immagine per la sola elaborazione (BGR)\ndef load_bgr_image(file_pth):\n    image_obj = cv2.imread(file_pth)  \n    return image_obj\n\n\n# Mostra una lista di immagini con i bounding box disegnati sopra\ndef display_images(image_lst, boxes_dictionary, image_fldr, max_images=6, no_cols=1, text=False, class_map={}):\n    total_ims = len(image_lst)\n    display_ims = min(max_images, total_ims)\n    no_rows = display_ims // no_cols + (display_ims % no_cols > 0)                   # Calcola il numero di righe necessarie\n    fig, axs = plt.subplots(no_rows, no_cols, figsize=(10, 10*no_rows/no_cols*3/4))  # Crea il layout della griglia\n    axs = axs.flatten()                                                              # Appiattisce l'array degli assi\n\n    for k, img_nm in enumerate(image_lst[:display_ims]):\n        image_path = str(image_fldr / img_nm)  \n        img = load_image(image_path)  \n\n        # Disegna i bounding box sull'immagine\n        if img_nm in boxes_dictionary:\n            for box in boxes_dictionary[img_nm]:\n                box_id, x_min, y_min, x_max, y_max = box\n                x_min, y_min, x_max, y_max = int(x_min), int(y_max), int(x_max), int(y_min)\n                cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0,255,0), 3)      # Disegna il rettangolo\n                if text:\n                    if class_map:\n                        box_label = class_map[box_id]                                 # Mappa la classe\n                    else:\n                        box_label = str(box_id)\n                    cv2.putText(img, box_label, (x_min, y_max-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (36,255,12), 4)  # Scrive il testo\n\n        axs[k].set_title(f\"Image {img_nm}\", fontsize=12)\n        axs[k].imshow(img)\n        axs[k].set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n    return\n\n\n# Converte le coordinate YOLO in coordinate per OpenCV ((left, top), (right, bottom))\ndef get_corners(x_cen, y_cen, an_width, an_height, im_width, im_height):\n    x_cen, y_cen, an_width, an_height = float(x_cen), float(y_cen), float(an_width), float(an_height)\n    # Calcolo dei margini\n    left = (x_cen - an_width/2)*im_width            \n    top = (y_cen - an_height/2)*im_height           \n    right = (x_cen + an_width/2)*im_width           \n    bottom = (y_cen + an_height/2)*im_height        \n    return int(left), int(top), int(right), int(bottom)\n\n\n# Mostra immagini con bounding box definiti in formato YOLO\ndef display_yolo_images(image_lst, image_fldr, max_images=6, no_cols=1, text=False, class_map={}):\n    total_ims = len(image_lst)\n    display_ims = min(max_images, total_ims)\n    no_rows = display_ims // no_cols + (display_ims % no_cols > 0)\n    _, axs = plt.subplots(no_rows, no_cols, figsize=(10, 10*no_rows/no_cols*3/4))\n    axs = axs.flatten()\n\n    for k, img_nm in enumerate(image_lst[:display_ims]):\n        image_path = image_fldr / img_nm\n        text_fn = image_path.stem + '.txt'     # Costruisce il nome del file dei bounding box\n        boxes_path = image_fldr / text_fn\n        img = load_image(str(image_path))  \n        im_h, im_w, _ = img.shape  \n        with open(boxes_path) as text_file:\n            annotations = [line.rstrip().split() for line in text_file]  \n\n        # Disegna i bounding box\n        for ann in annotations:\n            class_id = ann[0]\n            x_centre, y_centre, w, h = ann[1], ann[2], ann[3], ann[4]\n            x_min, y_min, x_max, y_max = get_corners(x_centre, y_centre, w, h, im_w, im_h)\n            cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0,255,0), 3)\n            if text:\n                if class_map:\n                    box_label = class_map[int(class_id)]\n                else:\n                    box_label = str(class_id)\n                cv2.putText(img, box_label, (x_min, y_max-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (36,255,12), 4)\n\n        axs[k].set_title(f\"Image {img_nm}\", fontsize=12)\n        axs[k].imshow(img)\n        axs[k].set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n    return\n\n\n# Trova i bounding box contenuti in una sezione dell'immagine e li restituisce in formato YOLO\ndef match_boxes(box_list, chnk_lims):\n    boxes_lists = []\n    le, to = chnk_lims[0], chnk_lims[1]                    # Limiti del chunk (left, top)\n    w, h = chnk_lims[2], chnk_lims[3]                      # Larghezza e altezza del chunk\n    for box in box_list:\n        o_left, o_top, o_right, o_bottom = box[1], box[2], box[3], box[4]\n        left, right = (o_left - le)/w, (o_right - le)/w    # Normalizza rispetto ai limiti del chunk\n        top, bottom = (o_top - to)/h, (o_bottom - to)/h\n\n        # Verifica se il bounding box è contenuto nella sezione\n        h_match = (0 <= left < 1) or (0 < right <= 1)\n        v_match = (0 <= top < 1) or (0 < bottom <= 1)\n\n        if v_match and h_match:\n            clipped = np.clip([left, top, right, bottom], a_min=0, a_max=1)  # Clippa i valori tra 0 e 1\n            l, t, r, b = clipped[0], clipped[1], clipped[2], clipped[3]\n            bounding_box = [str(box[0]),\n                            str(round((l + r)/2, 5)),\n                            str(round((t + b)/2, 5)),\n                            str(round(r - l, 5)),\n                            str(round(b - t, 5))]       # Formatta in YOLO\n            boxes_lists.append(bounding_box)\n    return boxes_lists","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:40.345529Z","iopub.execute_input":"2024-11-29T07:40:40.345786Z","iopub.status.idle":"2024-11-29T07:40:40.367295Z","shell.execute_reply.started":"2024-11-29T07:40:40.345762Z","shell.execute_reply":"2024-11-29T07:40:40.366466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Extraction and Cleaning","metadata":{}},{"cell_type":"code","source":"# Apre un file JSON contenente annotazioni e carica i dati\nwith open(labels_json_pth, 'r') as infile:\n    data = json.load(infile)                        # Carica il file JSON in un dizionario\n    keys = list(data.keys())                        # Estrazioni delle chiavi del dizionario\n\n# Estrae la lista delle annotazioni (features) dal JSON\nfeature_list = data['features']\n\n# Definisce le colonne per il DataFrame\nCOLUMNS = ['IMAGE_ID', 'TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX', 'LONG', 'LAT']\n\ndata = []\nfor feature in tqdm_notebook(feature_list):  \n    properties = feature['properties']               # Estrae le proprietà del feature (dizionario)\n    img_id = properties['image_id']                  # Identificativo immagine\n    type_id = properties['type_id']                  # Tipo di oggetto annotato\n    bbox = properties['bounds_imcoords'].split(\",\")  # Coordinate del bounding box\n\n    geometry = feature['geometry']  \n    coords = geometry['coordinates'][0]              # Lista di coordinate (lista di liste)\n\n    # Calcola il punto centrale geografico \n    long = coords[0][0] / 2 + coords[2][0] / 2       # Media delle longitudini dei vertici opposti\n    lat = coords[0][1] / 2 + coords[1][1] / 2        # Media delle latitudini dei vertici opposti\n\n    # Crea una riga con tutte le informazioni rilevanti\n    one_row = [img_id, type_id, bbox[0], bbox[1], bbox[2], bbox[3], long, lat]\n    data.append(one_row)  \n\n# Conta il numero di istanze totali nel dataset\ninstances = len(data)\nprint(f'Ci sono {instances} istanze degli ogetti nel dataset originale')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:40.368288Z","iopub.execute_input":"2024-11-29T07:40:40.368540Z","iopub.status.idle":"2024-11-29T07:40:56.103878Z","shell.execute_reply.started":"2024-11-29T07:40:40.368517Z","shell.execute_reply":"2024-11-29T07:40:56.102968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Estrazione delle colonne di interesse:","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data, columns = COLUMNS)\ndf[['XMIN', 'YMIN', 'XMAX', 'YMAX']] = df[['XMIN', 'YMIN', 'XMAX', 'YMAX']].apply(pd.to_numeric)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:56.105014Z","iopub.execute_input":"2024-11-29T07:40:56.105292Z","iopub.status.idle":"2024-11-29T07:40:58.039295Z","shell.execute_reply.started":"2024-11-29T07:40:56.105267Z","shell.execute_reply":"2024-11-29T07:40:58.038234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Rimozione di 2 annotation errate per le class labels ([EDA](https://www.kaggle.com/code/ollypowell/xview-dataset-eda))","metadata":{}},{"cell_type":"code","source":"df = df[(df.TYPE_ID != 75) & (df.TYPE_ID != 82)]   # removing erroneous labels\nprint(f'{instances - len(df)} rows removed, leaving {len(df)} rows')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:58.040535Z","iopub.execute_input":"2024-11-29T07:40:58.040891Z","iopub.status.idle":"2024-11-29T07:40:58.113301Z","shell.execute_reply.started":"2024-11-29T07:40:58.040856Z","shell.execute_reply":"2024-11-29T07:40:58.112384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:58.114571Z","iopub.execute_input":"2024-11-29T07:40:58.114920Z","iopub.status.idle":"2024-11-29T07:40:58.125306Z","shell.execute_reply.started":"2024-11-29T07:40:58.114881Z","shell.execute_reply":"2024-11-29T07:40:58.124423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Rimozione dell'immagine 1395, poiché inesistente","metadata":{}},{"cell_type":"code","source":"old_length = len(df)\ndf = df[df.IMAGE_ID != '1395.tif']\nprint(f'{old_length - len(df)} rows removed, leaving {len(df)}')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:58.126243Z","iopub.execute_input":"2024-11-29T07:40:58.126583Z","iopub.status.idle":"2024-11-29T07:40:58.281413Z","shell.execute_reply.started":"2024-11-29T07:40:58.126543Z","shell.execute_reply":"2024-11-29T07:40:58.280591Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Inoltre, è utile convertire gli ID dei tipi in una sequenza continua da 0 a 59 per le 60 categorie. Le label originali della competizione non erano organizzate in questo modo. Il dizionario riportato di seguito rappresenta la mappatura originale:","metadata":{}},{"cell_type":"code","source":"old_dict = {\n    11:'Fixed-wing Aircraft', 12:'Small Aircraft', 13:'Passenger/Cargo Plane', 15:'Helicopter',\n    17:'Passenger Vehicle', 18:'Small Car', 19:'Bus', 20:'Pickup Truck', 21:'Utility Truck',\n    23:'Truck', 24:'Cargo Truck', 25:'Truck Tractor w/ Box Trailer', 26:'Truck Tractor',27:'Trailer',\n    28:'Truck Tractor w/ Flatbed Trailer', 29:'Truck Tractor w/ Liquid Tank', 32:'Crane Truck',\n    33:'Railway Vehicle', 34:'Passenger Car', 35:'Cargo/Container Car', 36:'Flat Car', 37:'Tank car',\n    38:'Locomotive', 40:'Maritime Vessel', 41:'Motorboat', 42:'Sailboat', 44:'Tugboat', 45:'Barge',\n    47:'Fishing Vessel', 49:'Ferry', 50:'Yacht', 51:'Container Ship', 52:'Oil Tanker',\n    53:'Engineering Vehicle', 54:'Tower crane', 55:'Container Crane', 56:'Reach Stacker',\n    57:'Straddle Carrier', 59:'Mobile Crane', 60:'Dump Truck', 61:'Haul Truck', 62:'Scraper/Tractor',\n    63:'Front loader/Bulldozer', 64:'Excavator', 65:'Cement Mixer', 66:'Ground Grader', 71:'Hut/Tent',\n    72:'Shed', 73:'Building', 74:'Aircraft Hangar', 76:'Damaged Building', 77:'Facility', 79:'Construction Site',\n    83:'Vehicle Lot', 84:'Helipad', 86:'Storage Tank', 89:'Shipping container lot', 91:'Shipping Container',\n    93:'Pylon', 94:'Tower'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:58.285042Z","iopub.execute_input":"2024-11-29T07:40:58.285349Z","iopub.status.idle":"2024-11-29T07:40:58.294284Z","shell.execute_reply.started":"2024-11-29T07:40:58.285308Z","shell.execute_reply":"2024-11-29T07:40:58.293221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Di seguito si aggiorna la mappatura delle label","metadata":{}},{"cell_type":"code","source":"old_keys = sorted(list(old_dict.keys()))\nnew_dict = {old_dict[x]:y for y, x in enumerate(old_keys)}\nclass_map_dict = {y:old_dict[x] for y, x in enumerate(old_keys)}\nwith open(out_json_map_pth, \"w\") as json_file:\n    json.dump(class_map_dict, json_file)\nprint(class_map_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:58.295274Z","iopub.execute_input":"2024-11-29T07:40:58.295663Z","iopub.status.idle":"2024-11-29T07:40:58.310098Z","shell.execute_reply.started":"2024-11-29T07:40:58.295627Z","shell.execute_reply":"2024-11-29T07:40:58.309253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ora possiamo convertire i vecchi **TYPE_ID** del dataframe nei loro nuovi valori","metadata":{}},{"cell_type":"code","source":"df['TYPE_ID'] = df['TYPE_ID'].apply(lambda x: new_dict[old_dict[x]])\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:58.311091Z","iopub.execute_input":"2024-11-29T07:40:58.311338Z","iopub.status.idle":"2024-11-29T07:40:58.543067Z","shell.execute_reply.started":"2024-11-29T07:40:58.311296Z","shell.execute_reply":"2024-11-29T07:40:58.542091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Per verificare che i dati vengano caricati correttamente, visualizziamo alcune immagini con eventuali annotazioni relative ai trasporti","metadata":{}},{"cell_type":"code","source":"all_classes = list(class_map_dict.keys())\ntransport_only = [x for x in all_classes if x < 48]\n\nboxes = get_boxes(df, transport_only)\nimages_for_display = random.choices(list(boxes.keys()), k=2)\ndisplay_images(images_for_display, boxes, img_fldr_pth, max_images=2, no_cols=2, text=True, class_map=class_map_dict) #adjust as desired","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:40:58.544275Z","iopub.execute_input":"2024-11-29T07:40:58.544667Z","iopub.status.idle":"2024-11-29T07:41:24.613403Z","shell.execute_reply.started":"2024-11-29T07:40:58.544633Z","shell.execute_reply":"2024-11-29T07:41:24.612430Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Chunk Processing\n- Suddividere i file TIFF di grandi dimensioni in chunk  \n- Salvare questi chunk come file JPG  \n- Verificare se i chunk contengono annotazioni  \n- Riformattare le annotazioni nel formato YOLO: x_center, y_center, width, height (tutto normalizzato)  \n- Scrivere tutte le annotazioni in formato YOLO in un dizionario, utilizzando il nome del file come chiave  ","metadata":{}},{"cell_type":"code","source":"boxes_dict = get_boxes(df)  # Dizionario: {filename:[['TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX'],[..],[..],..]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:41:24.614869Z","iopub.execute_input":"2024-11-29T07:41:24.615184Z","iopub.status.idle":"2024-11-29T07:42:26.301363Z","shell.execute_reply.started":"2024-11-29T07:41:24.615157Z","shell.execute_reply":"2024-11-29T07:42:26.300465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_image(img_fn, \n                  dir_pth=img_fldr_pth, \n                  boxes=boxes_dict, \n                  out_dir=save_images_fldr_pth, \n                  c_height=CHUNK_HEIGHT, \n                  c_width=CHUNK_WIDTH,  \n                  jpg_q=JPEG_COMPRESSION,\n                  min_h=MIN_CHUNK_HEIGHT,\n                  min_w=MIN_CHUNK_WIDTH,\n                  writing=IMAGE_WRITING\n                 ):\n    \"\"\"\n    Suddivide un'immagine in piccoli chunk, salva i chunk in formato JPEG e converte le annotazioni in formato YOLO.\n\n    Args:\n        img_fn (str): Nome del file immagine da processare.\n        dir_pth (Path): Percorso della directory contenente le immagini di input.\n        boxes (dict): Dizionario che associa ogni immagine a una lista di annotazioni.\n        out_dir (Path): Percorso della directory dove salvare i chunk generati.\n        c_height (int): Altezza dei chunk da creare.\n        c_width (int): Larghezza dei chunk da creare.\n        jpg_q (int): Qualità di compressione JPEG (valore da 0 a 100).\n        min_h (int): Altezza minima per considerare un chunk valido.\n        min_w (int): Larghezza minima per considerare un chunk valido.\n        writing (bool): Se True, salva i chunk generati come file JPEG.\n\n    Returns:\n        tuple: Una tupla contenente:\n            - f_names (list): Lista dei nomi dei file dei chunk generati.\n            - widths (list): Lista delle larghezze dei chunk generati.\n            - heights (list): Lista delle altezze dei chunk generati.\n            - y_boxes (dict): Dizionario che associa i file dei chunk alle annotazioni in formato YOLO.\n    \"\"\"\n    \n    labels_list = boxes[img_fn]\n    img_pth = str(dir_pth / img_fn)\n    im = load_bgr_image(img_pth)\n    full_h, full_w, _ = im.shape\n    y_boxes= {}\n    f_names, widths, heights = [], [], []\n    \n    for r in range(0, full_h, c_height):\n        for c in range(0, full_w, c_width):\n            stem = img_fn.split('.')[0]\n            fn = str(f\"img_{stem}_{r}_{c}.jpg\")\n            out_pth = str(out_dir / fn)\n            width = c_width\n            height = c_height\n            if r + height > full_h:\n                height = full_h - r\n            if c + width > full_w:\n                width = full_w - c\n            big_enough = (height >= min_h) and (width >= min_w)\n            if big_enough:\n                if writing:\n                    cv2.imwrite(out_pth, im[r:r+height, c:c+height,:],  [int(cv2.IMWRITE_JPEG_QUALITY), jpg_q])\n                # Find any boxes occurring in the chunk, and convert to YOLO format\n                chunk_limits = [c, r, width, height]\n                y_boxes[fn] = match_boxes(labels_list, chunk_limits)\n                f_names.append(fn)\n                widths.append(width)\n                heights.append(height)\n    return f_names, widths, heights, y_boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:42:26.302485Z","iopub.execute_input":"2024-11-29T07:42:26.302726Z","iopub.status.idle":"2024-11-29T07:42:26.311765Z","shell.execute_reply.started":"2024-11-29T07:42:26.302703Z","shell.execute_reply":"2024-11-29T07:42:26.310931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_fns = df.IMAGE_ID.unique().tolist()\nif DEBUG:\n    img_fns = img_fns[:len(img_fns)//120]\n    df = df[df['IMAGE_ID'].isin(img_fns)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:42:26.312998Z","iopub.execute_input":"2024-11-29T07:42:26.313911Z","iopub.status.idle":"2024-11-29T07:42:26.423848Z","shell.execute_reply.started":"2024-11-29T07:42:26.313874Z","shell.execute_reply":"2024-11-29T07:42:26.423195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nnum_threads = mp.cpu_count() \noverall_progress = tqdm_notebook(total=len(img_fns), desc=\"Creating and saving image tiles\")\nyolo_boxes= {}\nfile_names, widths, heights = [], [], []\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n    for f_names, c_widths, c_heights, y_boxes in executor.map(process_image, img_fns):\n        file_names.extend(f_names)\n        widths.extend(c_widths)\n        heights.extend(c_heights)\n        yolo_boxes.update(y_boxes)\n        overall_progress.update(1)\noverall_progress.close()\n\nimage_data = {file_names[i]: [widths[i], heights[i]] for i in range(len(file_names))}\ntime_taken=time.time() - start_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:42:26.424616Z","iopub.execute_input":"2024-11-29T07:42:26.424869Z","iopub.status.idle":"2024-11-29T07:44:14.420008Z","shell.execute_reply.started":"2024-11-29T07:42:26.424831Z","shell.execute_reply":"2024-11-29T07:44:14.419270Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YOLO text files\nIterare attraverso il dizionario, creando un file di testo per ogni immagine con il formato: **classe x y larghezza altezza**, quindi salvare il file di testo completato nella stessa posizione, con lo stesso nome base dell'immagine.","metadata":{}},{"cell_type":"code","source":"all_image_files = os.listdir(save_images_fldr_pth)\nfor image_fn in tqdm_notebook(all_image_files):\n    stem = image_fn.split('.')[0]\n    fn = str (stem) + '.txt'\n    txt_pth = str(save_images_fldr_pth / fn)\n    seperator = ' '\n    with open(txt_pth, 'a') as f:\n        if image_fn in yolo_boxes:\n            for bbox in yolo_boxes[image_fn]:\n                txt = seperator.join(bbox) + '\\n'\n                f.write(txt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:44:14.420948Z","iopub.execute_input":"2024-11-29T07:44:14.421278Z","iopub.status.idle":"2024-11-29T07:44:18.643410Z","shell.execute_reply.started":"2024-11-29T07:44:14.421183Z","shell.execute_reply":"2024-11-29T07:44:18.642594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_files = [file for file in os.listdir(save_images_fldr_pth) if file.endswith('.txt')]\nnum_txt_files = len(txt_files)\nprint(f\"Numero di file .txt: {num_txt_files}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:44:18.644722Z","iopub.execute_input":"2024-11-29T07:44:18.645401Z","iopub.status.idle":"2024-11-29T07:44:18.748392Z","shell.execute_reply.started":"2024-11-29T07:44:18.645358Z","shell.execute_reply":"2024-11-29T07:44:18.747571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_with_boxes = [image_fn for image_fn in all_image_files if image_fn in yolo_boxes]\nprint(f\"Numero di immagini con dati in yolo_boxes: {len(images_with_boxes)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:44:18.749373Z","iopub.execute_input":"2024-11-29T07:44:18.749617Z","iopub.status.idle":"2024-11-29T07:44:18.772968Z","shell.execute_reply.started":"2024-11-29T07:44:18.749592Z","shell.execute_reply":"2024-11-29T07:44:18.772131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_paths = [save_images_fldr_pth / x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".txt\")]\ncolumn_names = ['Class_ID', 'x_center', 'y_center', 'width', 'height']\ndata = []\nfor file_path in text_paths:\n    with open(file_path, 'r') as file:\n        for line in file:\n            values = line.strip().split(' ')\n            row_data = {col: val for col, val in zip(column_names, values)}\n            row_data['File_Name'] = file_path.name\n            data.append(row_data)\n\nout_df = pd.DataFrame(data)\nout_df['Class_ID']=out_df['Class_ID'].astype(int)\nout_df['Class_Name'] = out_df['Class_ID'].map(class_map_dict).fillna('unknown')\nout_df = out_df[['File_Name', 'Class_Name', 'Class_ID', 'x_center', 'y_center', 'width', 'height']]\nout_df.to_parquet(out_data_parquet_pth, index=False)\nout_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:44:18.774109Z","iopub.execute_input":"2024-11-29T07:44:18.774437Z","iopub.status.idle":"2024-11-29T07:44:24.092112Z","shell.execute_reply.started":"2024-11-29T07:44:18.774400Z","shell.execute_reply":"2024-11-29T07:44:24.091152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Rimozione del 66% delle immagini senza label","metadata":{}},{"cell_type":"code","source":"def remove_empty(image_folder, yolo_boxes, image_data):\n    \"\"\"\n    Rimuove il 66% dei file di annotazione vuoti (.txt) e le immagini corrispondenti.\n    Aggiorna anche i dizionari yolo_boxes e image_data per rimuovere le voci corrispondenti.\n    \n    Args:\n        image_folder (str): Percorso della cartella contenente immagini e file .txt.\n        yolo_boxes (dict): Dizionario con annotazioni YOLO.\n        image_data (dict): Dizionario con metadati delle immagini.\n        file_names (list): Lista di nomi file immagine.\n        widths (list): Lista di larghezze delle immagini.\n        heights (list): Lista di altezze delle immagini.\n    \n    Returns:\n        tuple: Dizionari aggiornati (yolo_boxes, image_data, file_names, widths, heights).\n    \"\"\"\n    all_image_files = set(os.listdir(image_folder))  # Set per confronti più veloci\n    empty_files = [] \n\n    # Controlla i file .txt per annotazioni vuote\n    for txt_file in all_image_files:\n        if txt_file.endswith('.txt'):\n            txt_path = os.path.join(image_folder, txt_file)\n            # Controlla se il file è vuoto o contiene solo spazi\n            with open(txt_path, 'r') as file:\n                content = file.read().strip()\n            if not content:  \n                # Determina il file immagine corrispondente\n                image_file = txt_file.replace('.txt', '.jpg')  \n                empty_files.append(image_file)  # Aggiungi immagine alla lista dei file vuoti\n\n    # Seleziona il 66% dei file vuoti da rimuovere\n    num_to_remove = int(len(empty_files) * 0.66)\n    files_to_remove = random.sample(empty_files, num_to_remove)\n\n    # Rimuovi i file .txt e immagini corrispondenti\n    for image_file in files_to_remove:\n        txt_file = image_file.replace('.jpg', '.txt')  \n        txt_path = os.path.join(image_folder, txt_file)\n        image_path = os.path.join(image_folder, image_file)\n\n        if os.path.exists(txt_path):\n            os.remove(txt_path)\n        if os.path.exists(image_path):\n            os.remove(image_path)\n\n    # Aggiorna yolo_boxes eliminando i file rimossi\n    yolo_boxes = {key: value for key, value in yolo_boxes.items() if key not in files_to_remove}\n\n    # Aggiorna image_data eliminando i file rimossi\n    image_data = {key: value for key, value in image_data.items() if key not in files_to_remove}\n\n    # Filtra le liste per escludere i file da rimuovere\n    filtered_file_names = [name for name in file_names if name not in files_to_remove]\n    filtered_widths = [widths[i] for i in range(len(file_names)) if file_names[i] not in files_to_remove]\n    filtered_heights = [heights[i] for i in range(len(file_names)) if file_names[i] not in files_to_remove]\n\n    return yolo_boxes, image_data, filtered_file_names, filtered_widths, filtered_heights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:44:24.093238Z","iopub.execute_input":"2024-11-29T07:44:24.093484Z","iopub.status.idle":"2024-11-29T07:44:24.102568Z","shell.execute_reply.started":"2024-11-29T07:44:24.093460Z","shell.execute_reply":"2024-11-29T07:44:24.101604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo_boxes, image_data, filtered_file_names, filtered_widths, filtered_heights = remove_empty(save_images_fldr_pth, yolo_boxes, image_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:44:24.103584Z","iopub.execute_input":"2024-11-29T07:44:24.103904Z","iopub.status.idle":"2024-11-29T07:47:01.428488Z","shell.execute_reply.started":"2024-11-29T07:44:24.103859Z","shell.execute_reply":"2024-11-29T07:47:01.427693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verifica il numero di file .txt nel folder\ntxt_files = [f for f in os.listdir(save_images_fldr_pth) if f.endswith('.txt')]\nprint(f\"Numero di file .txt nel folder: {len(txt_files)}\")\n# Conta il numero di file immagine (per esempio file .jpg)\nimage_files = [f for f in os.listdir(save_images_fldr_pth) if f.endswith('.jpg')]\nnum_images = len(image_files)\nprint(f\"Numero di immagini nel folder: {num_images}\")\nimages_with_boxes = [image_fn for image_fn in all_image_files if image_fn in yolo_boxes]\nprint(f\"Numero di immagini con dati in yolo_boxes: {len(images_with_boxes)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:01.429477Z","iopub.execute_input":"2024-11-29T07:47:01.429710Z","iopub.status.idle":"2024-11-29T07:47:01.590298Z","shell.execute_reply.started":"2024-11-29T07:47:01.429686Z","shell.execute_reply":"2024-11-29T07:47:01.589450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import islice\n\n# Stampa i primi 5 elementi\nfor key, value in islice(image_data.items(), 5):\n    print(f\"{key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:01.591465Z","iopub.execute_input":"2024-11-29T07:47:01.592149Z","iopub.status.idle":"2024-11-29T07:47:01.596779Z","shell.execute_reply.started":"2024-11-29T07:47:01.592106Z","shell.execute_reply":"2024-11-29T07:47:01.595989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_file_names = len(image_data) # Il numero di chiavi corrisponde al numero di file_names (per ora)\nnum_widths = len([value[0] for value in image_data.values()])  # Conta tutte le larghezze\nnum_heights = len([value[1] for value in image_data.values()])  # Conta tutte le altezze\n\nprint(f\"Il numero di immagini uniche è: {num_file_names}\")\nprint(f\"Il numero di larghezze registrate è: {num_widths}\")\nprint(f\"Il numero di altezze registrate è: {num_heights}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:01.597762Z","iopub.execute_input":"2024-11-29T07:47:01.598060Z","iopub.status.idle":"2024-11-29T07:47:01.616972Z","shell.execute_reply.started":"2024-11-29T07:47:01.598036Z","shell.execute_reply":"2024-11-29T07:47:01.616289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filenames = [x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".jpg\")]\nimage_list = random.choices(filenames, k=4)\ndisplay_yolo_images(image_list, save_images_fldr_pth, max_images=4, no_cols=2, text=True,  class_map=class_map_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:01.617856Z","iopub.execute_input":"2024-11-29T07:47:01.618102Z","iopub.status.idle":"2024-11-29T07:47:02.332603Z","shell.execute_reply.started":"2024-11-29T07:47:01.618079Z","shell.execute_reply":"2024-11-29T07:47:02.331714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#without text labels\nfilenames = [x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".jpg\")]\nimage_list = random.choices(filenames, k=4)\ndisplay_yolo_images(image_list, save_images_fldr_pth, max_images=4, no_cols=2, text=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:02.338045Z","iopub.execute_input":"2024-11-29T07:47:02.338317Z","iopub.status.idle":"2024-11-29T07:47:03.061672Z","shell.execute_reply.started":"2024-11-29T07:47:02.338291Z","shell.execute_reply":"2024-11-29T07:47:03.060877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting","metadata":{}},{"cell_type":"code","source":"total_images = len(filenames)\nindices = list(range(total_images))\nrandom.shuffle(indices)\n\ntrain_fraction = 1 - TEST_FRACTION - VAL_FRACTION\ntrain_sp = int(np.floor(train_fraction * len(indices))) # The training-validation split\nvalid_sp = int(np.floor(VAL_FRACTION * len(indices))) + train_sp # The validation-test split\ntrain_idx, val_idx, test_idx = indices[:train_sp], indices[train_sp:valid_sp], indices[valid_sp:]\n\nprint(' Training set size: \\t', len(train_idx))\nprint(' Validation set size: \\t', len(val_idx))\nprint(' Test set size: \\t', len(test_idx))\nprint(' Total dataset: \\t', total_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.062777Z","iopub.execute_input":"2024-11-29T07:47:03.063102Z","iopub.status.idle":"2024-11-29T07:47:03.105653Z","shell.execute_reply.started":"2024-11-29T07:47:03.063067Z","shell.execute_reply":"2024-11-29T07:47:03.104858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Write 3 text files into the Data folder with the file paths: train.txt, val.txt, test.txt  These are lists of absolute filepaths to the images, one line each path.  They can reside anywhere just so long as the relative paths in xview_yolo.yaml points to them.","metadata":{}},{"cell_type":"code","source":"files = ['train.txt', 'val.txt', 'test.txt']\nsplits = [train_idx, val_idx, test_idx]\n\nfor fn, split in zip(files, splits):\n    txt_pth = cfg_fldr_pth / fn\n    with open(txt_pth, 'a') as f:\n        for ind in split:\n            f.write(str(future_ds_img_fldr / filenames[ind]) + '\\n')\n        print(f'{fn[:-4]} file written to {txt_pth}, with {len(split) } samples')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.106710Z","iopub.execute_input":"2024-11-29T07:47:03.107066Z","iopub.status.idle":"2024-11-29T07:47:03.317043Z","shell.execute_reply.started":"2024-11-29T07:47:03.107027Z","shell.execute_reply":"2024-11-29T07:47:03.316221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YAML File\nWrite a .yaml file pointing to the text file locations, and determining class names, number of categories location.\nThis is good practice, it means I don't need to move all the image files around just to change the training splits.\nAlso the .yml file gets updated automatically if anybody changes something like the number of classes.","metadata":{}},{"cell_type":"code","source":"config = {'train': str(future_ds_cfg_fldr / files[0]),\n          'val': str(future_ds_cfg_fldr / files[1]),\n          'test': str(future_ds_cfg_fldr / files[2]),\n          'nc': len(class_map_dict),\n          'names': class_map_dict\n          }\n\nwith open(yolo_yaml_pth, \"w\") as file:\n    yaml.dump(config, file, default_style=None, default_flow_style=False, sort_keys=False)\nprint(f'yaml file written to {yolo_yaml_pth}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.318115Z","iopub.execute_input":"2024-11-29T07:47:03.318357Z","iopub.status.idle":"2024-11-29T07:47:03.328569Z","shell.execute_reply.started":"2024-11-29T07:47:03.318333Z","shell.execute_reply":"2024-11-29T07:47:03.327733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting Check\nJust checking the first few lines of the train.txt file","metadata":{}},{"cell_type":"code","source":"for split in ['train', 'val', 'test']:\n    print(f'{split} text file')\n    print_first_n_lines(cfg_fldr_pth / f'{split}.txt', 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.329578Z","iopub.execute_input":"2024-11-29T07:47:03.329912Z","iopub.status.idle":"2024-11-29T07:47:03.341770Z","shell.execute_reply.started":"2024-11-29T07:47:03.329887Z","shell.execute_reply":"2024-11-29T07:47:03.341096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And the .yaml file","metadata":{}},{"cell_type":"code","source":"print_first_n_lines(yolo_yaml_pth, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.342616Z","iopub.execute_input":"2024-11-29T07:47:03.342909Z","iopub.status.idle":"2024-11-29T07:47:03.352055Z","shell.execute_reply.started":"2024-11-29T07:47:03.342858Z","shell.execute_reply":"2024-11-29T07:47:03.351218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And a couple of annotations files","metadata":{}},{"cell_type":"code","source":"txt_fnames = [save_images_fldr_pth / x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".txt\")]\ntext_list = random.choices(txt_fnames, k=2)\nprint(text_list)\nfor text_f in text_list:\n    print(f'Reading {text_f}')\n    print_first_n_lines(text_f, 3)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.353111Z","iopub.execute_input":"2024-11-29T07:47:03.353891Z","iopub.status.idle":"2024-11-29T07:47:03.570213Z","shell.execute_reply.started":"2024-11-29T07:47:03.353864Z","shell.execute_reply":"2024-11-29T07:47:03.569337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(text_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.571252Z","iopub.execute_input":"2024-11-29T07:47:03.571527Z","iopub.status.idle":"2024-11-29T07:47:03.575371Z","shell.execute_reply.started":"2024-11-29T07:47:03.571501Z","shell.execute_reply":"2024-11-29T07:47:03.574603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And the csv file","metadata":{}},{"cell_type":"code","source":"out_data = pd.read_parquet(out_data_parquet_pth)\nout_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.576457Z","iopub.execute_input":"2024-11-29T07:47:03.576775Z","iopub.status.idle":"2024-11-29T07:47:03.845717Z","shell.execute_reply.started":"2024-11-29T07:47:03.576738Z","shell.execute_reply":"2024-11-29T07:47:03.844858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta il numero di immagini con nomi unici\nnum_immagini_uniche = out_data['File_Name'].nunique()\nprint(f\"Il numero di immagini uniche è: {num_immagini_uniche}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.846676Z","iopub.execute_input":"2024-11-29T07:47:03.846946Z","iopub.status.idle":"2024-11-29T07:47:03.895323Z","shell.execute_reply.started":"2024-11-29T07:47:03.846922Z","shell.execute_reply":"2024-11-29T07:47:03.894499Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And the `.json`","metadata":{}},{"cell_type":"code","source":"with open(out_json_map_pth, \"r\") as json_file:\n    loaded_dict = json.load(json_file)\nprint(loaded_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.896298Z","iopub.execute_input":"2024-11-29T07:47:03.896586Z","iopub.status.idle":"2024-11-29T07:47:03.907341Z","shell.execute_reply.started":"2024-11-29T07:47:03.896562Z","shell.execute_reply":"2024-11-29T07:47:03.906530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YOLO to COCO\n\nPoiché non richiede molto sforzo in questo momento, riformatterò il dataframe nella geometria del formato COCO e scriverò un file `.json` in formato COCO per coloro che necessitano di tale formato. Il formato COCO è spiegato [qui](https://cocodataset.org/#format-data). A livello superiore, abbiamo principalmente bisogno di questi tre oggetti:\n\n- **images:**  \n  `{\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, }`  \n\n- **annotations:**  \n  `{\"id\": int, \"image_id\": int, \"category_id\": int, \"area\": float, \"bbox\": [x, y, width, height]}`  \n\n- **categories:**  \n  `[{\"id\": int, \"name\": str}]`","metadata":{}},{"cell_type":"markdown","source":"Copiamo il DataFrame YOLO, per estrarre le larghezze delle immagini e creare la categoria BBox","metadata":{}},{"cell_type":"code","source":"num_file_names = len(image_data) # Il numero di chiavi corrisponde al numero di file_names (per ora)\nnum_widths = len([value[0] for value in image_data.values()])  # Conta tutte le larghezze\nnum_heights = len([value[1] for value in image_data.values()])  # Conta tutte le altezze\n\nprint(f\"Il numero di immagini uniche è: {num_file_names}\")\nprint(f\"Il numero di larghezze registrate è: {num_widths}\")\nprint(f\"Il numero di altezze registrate è: {num_heights}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.908419Z","iopub.execute_input":"2024-11-29T07:47:03.908671Z","iopub.status.idle":"2024-11-29T07:47:03.918353Z","shell.execute_reply.started":"2024-11-29T07:47:03.908648Z","shell.execute_reply":"2024-11-29T07:47:03.917360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_data = {'width': filtered_widths, 'height': filtered_heights, 'file_name': filtered_file_names} # Ricreo il dizionario con 3 chiavi ma utilizzando le liste filtraten dopo l'eliminazione fatta prima\nim_df = pd.DataFrame(image_data)\nim_df['id'] = im_df['file_name'].str.replace(r'\\D', '', regex=True).astype(int)\nim_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:03.919187Z","iopub.execute_input":"2024-11-29T07:47:03.919442Z","iopub.status.idle":"2024-11-29T07:47:04.054609Z","shell.execute_reply.started":"2024-11-29T07:47:03.919414Z","shell.execute_reply":"2024-11-29T07:47:04.053766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_file_names = len(image_data['file_name'])  # Numero di file rimanenti\nnum_widths = len(image_data['width'])         # Numero di larghezze rimanenti\nnum_heights = len(image_data['height'])       # Numero di altezze rimanenti\n\nprint(f\"Il numero di immagini uniche è: {num_file_names}\")\nprint(f\"Il numero di larghezze registrate è: {num_widths}\")\nprint(f\"Il numero di altezze registrate è: {num_heights}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:04.055772Z","iopub.execute_input":"2024-11-29T07:47:04.056146Z","iopub.status.idle":"2024-11-29T07:47:04.062050Z","shell.execute_reply.started":"2024-11-29T07:47:04.056105Z","shell.execute_reply":"2024-11-29T07:47:04.061171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_images_in_im_df = len(im_df)\nprint(f\"Numero di immagini in im_df: {num_images_in_im_df}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:04.063151Z","iopub.execute_input":"2024-11-29T07:47:04.063755Z","iopub.status.idle":"2024-11-29T07:47:04.079632Z","shell.execute_reply.started":"2024-11-29T07:47:04.063719Z","shell.execute_reply":"2024-11-29T07:47:04.078744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def row_to_dict(row):\n    return {\n        'id': row['id'],\n        'width': row['width'],\n        'height':row['height'],\n        'file_name':row['file_name']\n    }\n\nim_list = im_df.apply(lambda row: row_to_dict(row), axis=1).tolist()\n[print(val) for val in im_list[:4]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:04.080590Z","iopub.execute_input":"2024-11-29T07:47:04.080858Z","iopub.status.idle":"2024-11-29T07:47:04.610852Z","shell.execute_reply.started":"2024-11-29T07:47:04.080820Z","shell.execute_reply":"2024-11-29T07:47:04.609994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Merge the images dataframe with the annotations to work out the absolute pixel values, plus a bit more re-organising.","metadata":{}},{"cell_type":"code","source":"annotations_df = out_data.copy()\nannotations_df['image_id'] = annotations_df['File_Name'].str.replace(r'\\D', '', regex=True).astype(int)\nannotations_df= annotations_df.rename(columns={'height': 'h', 'width': 'w'})\nan_df = annotations_df.merge(im_df, left_on='image_id', right_on='id', how='left')\nan_df['x_center']= (an_df['x_center'].astype(np.float64)*an_df['width']).round(decimals=0)\nan_df['y_center']= (an_df['y_center'].astype(np.float64)*an_df['height']).round(decimals=0)\nan_df['w']= (an_df['w'].astype(np.float64)*an_df['width']).round(decimals=0)\nan_df['h']= (an_df['h'].astype(np.float64)*an_df['height']).round(decimals=0)\nan_df['Class_ID']= an_df['Class_ID'].astype(int)\nan_df = an_df.drop(columns=['File_Name', 'file_name', 'width', 'height', 'id'])\nan_df['left'] = (an_df['x_center'] - an_df['w']/2).round(decimals=0)\nan_df['top'] =  (an_df['y_center'] - an_df['h']/2).round(decimals=0)\nan_df['bbox'] = ('[' + an_df['left'].astype(str) + ', ' \n              + an_df['top'].astype(str) + ', ' \n              + an_df['w'].astype(str) + ', '\n              + an_df['h'].astype(str) + ']')\nan_df['area'] = an_df['w'] * an_df['h']\nan_df = an_df.drop(columns=['x_center', 'y_center', 'w', 'h', 'left', 'top', 'Class_Name'])\nan_df.reset_index(inplace=True)\nan_df.rename(columns={'index': 'id'}, inplace=True)\nan_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:04.612007Z","iopub.execute_input":"2024-11-29T07:47:04.612666Z","iopub.status.idle":"2024-11-29T07:47:08.473686Z","shell.execute_reply.started":"2024-11-29T07:47:04.612636Z","shell.execute_reply":"2024-11-29T07:47:08.472792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def row_to_dict(row):\n    return {\n        'id': row['id'],\n        'image_id' : row['image_id'],\n        'category_id': row['Class_ID'],\n        'area':row['area'],\n        'bbox':row['bbox']\n    }\n\nan_list = an_df.apply(lambda row: row_to_dict(row), axis=1).tolist()\nprint(an_list[:4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:08.474942Z","iopub.execute_input":"2024-11-29T07:47:08.475411Z","iopub.status.idle":"2024-11-29T07:47:17.474796Z","shell.execute_reply.started":"2024-11-29T07:47:08.475361Z","shell.execute_reply":"2024-11-29T07:47:17.473856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The category mapping is just about in a convenient format already","metadata":{}},{"cell_type":"code","source":"cat_list = [{key:val} for key,val in class_map_dict.items()]\nprint(cat_list[:4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:17.475885Z","iopub.execute_input":"2024-11-29T07:47:17.476143Z","iopub.status.idle":"2024-11-29T07:47:17.481330Z","shell.execute_reply.started":"2024-11-29T07:47:17.476119Z","shell.execute_reply":"2024-11-29T07:47:17.480473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now I just need to combine the top level objects, and save to `.json`","metadata":{}},{"cell_type":"code","source":"out_json_data = {'images': im_list, 'annotations': an_list, 'categories': cat_list}\nwith open(coco_json_pth, 'w') as json_file:\n    json.dump(out_json_data, json_file, indent=4)\n    \nfor key, value in out_json_data.items():\n    print(key, value[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:17.482277Z","iopub.execute_input":"2024-11-29T07:47:17.482616Z","iopub.status.idle":"2024-11-29T07:47:23.438241Z","shell.execute_reply.started":"2024-11-29T07:47:17.482576Z","shell.execute_reply":"2024-11-29T07:47:23.437344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Image Sizes","metadata":{}},{"cell_type":"code","source":"def check_image_sizes(directory_path):\n    size_counts = defaultdict(int)\n\n    # Ottieni la lista dei file nella cartella\n    files = [f for f in os.listdir(directory_path) if f.endswith(('.jpg'))]\n\n    # Aggiungi una barra di progresso per iterare sui file\n    for filename in tqdm(files, desc=\"Processing images\"):\n        img_path = os.path.join(directory_path, filename)\n        with Image.open(img_path) as img:\n            size = img.size  # (width, height)\n            size_counts[size] += 1\n\n    # Crea gruppi per le dimensioni richieste\n    size_groups = {\n        'Smaller than 320x320': [],\n        '320x320': [],\n        'Larger than 320x320': [],\n    }\n\n    # Aggiungi le dimensioni agli appropriate gruppi\n    for size, count in size_counts.items():\n        width, height = size\n        if width < 320 and height < 320:\n            size_groups['Smaller than 320x320'].append((size, count))\n        elif width == 320 and height == 320:\n            size_groups['320x320'].append((size, count))\n        elif width > 320 and height > 320:\n            size_groups['Larger than 320x320'].append((size, count))\n\n    # Ordina le dimensioni per area (larghezza * altezza) in ordine decrescente\n    for group, items in size_groups.items():\n        sorted_items = sorted(items, key=lambda x: x[0][0] * x[0][1], reverse=True)  # ordina per area\n        size_groups[group] = sorted_items\n\n    # Stampa i gruppi con il numero di immagini per dimensione e il totale per intervallo\n    for group, items in size_groups.items():\n        total = sum(count for _, count in items)\n        print(f\"{group} (Totale: {total} immagini):\")\n        for size, count in items:\n            print(f\"  Dimensione {size}: {count} immagini\")\n        print()\n\n# Esegui la funzione con il percorso della cartella\ncheck_image_sizes(save_images_fldr_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:23.439388Z","iopub.execute_input":"2024-11-29T07:47:23.439747Z","iopub.status.idle":"2024-11-29T07:47:31.256493Z","shell.execute_reply.started":"2024-11-29T07:47:23.439705Z","shell.execute_reply":"2024-11-29T07:47:31.255622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(coco_json_pth, 'r') as f:\n    coco_data = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:31.257427Z","iopub.execute_input":"2024-11-29T07:47:31.257673Z","iopub.status.idle":"2024-11-29T07:47:32.605777Z","shell.execute_reply.started":"2024-11-29T07:47:31.257648Z","shell.execute_reply":"2024-11-29T07:47:32.605093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_image_size(image, img_id, coco_data):\n    # Trova le dimensioni dell'immagine nel JSON\n    image_info = next((img for img in coco_data['images'] if img['id'] == img_id), None)\n    if image_info:\n        width, height = image_info['width'], image_info['height']\n        img_width, img_height = image.size\n        # Lancia un'eccezione solo se le dimensioni non corrispondono\n        assert img_width == width and img_height == height, (\n            f\"Dimensioni errate: JSON ({width}, {height}), immagine ({img_width}, {img_height})\"\n        )\n    else:\n        raise ValueError(f\"Immagine con ID {img_id} non trovata nel JSON.\")\n\ndef check_all_images(folder_path, coco_data):\n    folder_path = Path(folder_path)\n    errors_found = False  # Flag per tenere traccia degli errori\n    check_count = 0  # Conta il numero di immagini verificate\n    \n    # Itera attraverso tutti i file nella cartella\n    for image_path in folder_path.iterdir():\n        if image_path.is_file() and image_path.suffix in ['.jpg']:  # Controlla solo file immagine\n            check_count += 1  # Incrementa il contatore delle immagini\n            # Trova l'ID corrispondente basato sul nome file\n            img_id = int(''.join(filter(str.isdigit, image_path.stem)))  # Estrae i numeri dal nome\n            try:\n                with Image.open(image_path) as img:\n                    check_image_size(img, img_id, coco_data)\n            except (AssertionError, ValueError) as e:\n                errors_found = True\n                print(f\"Errore per immagine {image_path.name}: {e}\")\n            except Exception as e:\n                errors_found = True\n                print(f\"Errore generico per immagine {image_path.name}: {e}\")\n\n    # Stampa il risultato finale\n    if not errors_found:\n        print(f\"Check completato, nessun errore trovato. Numero di immagini verificate: {check_count}\")\n    else:\n        print(f\"Check completato con errori. Numero di immagini verificate: {check_count}\")\n\n# Percorso alla directory delle immagini\ncheck_all_images(save_images_fldr_pth, coco_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:47:32.606756Z","iopub.execute_input":"2024-11-29T07:47:32.607013Z","iopub.status.idle":"2024-11-29T07:48:39.217399Z","shell.execute_reply.started":"2024-11-29T07:47:32.606989Z","shell.execute_reply":"2024-11-29T07:48:39.216417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Labels","metadata":{}},{"cell_type":"code","source":"def check_empty_txt_files(directory_path):\n    # Ottieni tutti i file .txt nella cartella\n    txt_files = [f for f in os.listdir(directory_path) if f.endswith(\".txt\")]\n    \n    empty_files_count = 0\n    total_lines = 0\n    non_empty_files_count = 0\n    \n    # Controlla se ogni file è vuoto\n    for txt_file in txt_files:\n        file_path = os.path.join(directory_path, txt_file)\n        if os.path.getsize(file_path) == 0:\n            empty_files_count += 1\n        else:\n            non_empty_files_count += 1\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n                total_lines += len(lines)\n    \n    # Calcola la media delle righe per i file non vuoti\n    avg_lines = total_lines / non_empty_files_count if non_empty_files_count > 0 else 0\n    \n    # Stampa il numero di file vuoti e la media delle righe nei file non vuoti\n    print(f\"Numero di file .txt vuoti: {empty_files_count}\")\n    print(f\"Numero di file .txt non vuoti: {non_empty_files_count}\")\n    print(f\"Media delle righe per file non vuoto: {avg_lines:.2f}\")\n\ncheck_empty_txt_files(save_images_fldr_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:48:39.218425Z","iopub.execute_input":"2024-11-29T07:48:39.218710Z","iopub.status.idle":"2024-11-29T07:48:40.287760Z","shell.execute_reply.started":"2024-11-29T07:48:39.218683Z","shell.execute_reply":"2024-11-29T07:48:40.286966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Category Distribution","metadata":{}},{"cell_type":"code","source":"with open(coco_json_pth, 'r') as f:\n    coco_data = json.load(f)\n\n# Crea un dizionario per mappare category_id a categoria\ncategory_mapping = {str(index): list(category.values())[0] for index, category in enumerate(coco_data['categories'])}\n\n# Estrai i category_id dalle annotazioni\ncategory_ids = [annotation['category_id'] for annotation in coco_data['annotations']]\n\n# Conta le occorrenze di ogni category_id\ncategory_counts = Counter(category_ids)\n\n# Gruppi delle categorie\ncategory_groups = {\n    \"Aircraft\": [0, 1, 2, 3],\n    \"Passenger Vehicle\": [4, 5, 6, 53],\n    \"Truck\": [7, 8, 9, 10, 11, 12, 13, 14, 15],\n    \"Railway Vehicle\": [17, 18, 19, 20, 21, 22],\n    \"Maritime Vessel\": [23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n    \"Engineering Vehicle\": [16, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n    \"Building\": [46, 47, 48, 49, 50, 51, 52, 59],\n    \"Helipad\": [54],\n    \"Storage Tank\": [55],\n    \"Shipping Container\": [56, 57],\n    \"Pylon\": [58],\n}\n\n# Crea un dizionario per le categorie aggregate con le loro occorrenze\ngrouped_occurrences = {}\nfor group_name, category_ids in category_groups.items():\n    # Somma le occorrenze per le categorie di ogni gruppo\n    grouped_occurrences[group_name] = sum(category_counts[cat_id] for cat_id in category_ids)\n\n# Ordina le categorie raggruppate per occorrenze in ordine decrescente\ngrouped_occurrences = dict(sorted(grouped_occurrences.items(), key=lambda item: item[1], reverse=True))\n\n# Prepara i dati per il grafico\ngroups = list(grouped_occurrences.keys())\noccurrences = list(grouped_occurrences.values())\n\n\n# Crea il grafico a barre\nplt.figure(figsize=(10, 6))\nplt.barh(groups, occurrences, color='skyblue')\nplt.xlabel('Numero di Occorrenze')\nplt.ylabel('Gruppi di Categoria')\nplt.title('Distribuzione delle Occorrenze per Gruppo di Categoria')\nplt.tight_layout()\n\n# Mostra il grafico\nplt.show()\n\n# Stampare le occorrenze per ciascun group e i relativi ID\nsorted_groups = sorted(category_groups.items(), key=lambda item: sum(category_counts.get(cat_id, 0) for cat_id in item[1]))\n\nfor group, category_ids in sorted_groups:\n    # Calcola il numero totale di occorrenze per il gruppo\n    total_count = sum(category_counts.get(cat_id, 0) for cat_id in category_ids)\n    \n    print(f\"Gruppo: {group}, Occorrenze Totali: {total_count}\")\n    \n    for cat_id in category_ids:\n        category_name = category_mapping[str(cat_id)]\n        count = category_counts.get(cat_id, 0)\n        print(f\"  category_id: {cat_id}, Categoria: {category_name}, Occorrenze: {count}\")\n    \n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:48:40.288712Z","iopub.execute_input":"2024-11-29T07:48:40.288987Z","iopub.status.idle":"2024-11-29T07:48:41.990623Z","shell.execute_reply.started":"2024-11-29T07:48:40.288960Z","shell.execute_reply":"2024-11-29T07:48:41.989752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_coco_json_pth = out_dataset_pth / 'COCO_annotations_new.json'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:48:41.991491Z","iopub.execute_input":"2024-11-29T07:48:41.991725Z","iopub.status.idle":"2024-11-29T07:48:41.995738Z","shell.execute_reply.started":"2024-11-29T07:48:41.991701Z","shell.execute_reply":"2024-11-29T07:48:41.994858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica il file COCO originale\nwith open(coco_json_pth, 'r') as f:\n    coco_data = json.load(f)\n\n# Creare nuove categorie nel formato richiesto\nnew_categories = [{str(idx): group} for idx, group in enumerate(category_groups.keys())]\n\n# Mappatura inversa dei gruppi (vecchio category_id -> nuovo group_id)\ngroup_mapping = {}\nfor group_id, (group_name, category_ids) in enumerate(category_groups.items(), start=0):\n    for cat_id in category_ids:\n        group_mapping[cat_id] = group_id\n\n# Aggiorna i category_id nelle annotazioni\nfor annotation in coco_data[\"annotations\"]:\n    original_id = annotation[\"category_id\"]\n    annotation[\"category_id\"] = group_mapping.get(original_id, -1)  # Usa -1 per eventuali category_id non mappati\n\n# Verifica annotazioni non mappate\nunmapped = [ann for ann in coco_data[\"annotations\"] if ann[\"category_id\"] == -1]\nif unmapped:\n    print(f\"Attenzione: {len(unmapped)} annotazioni con category_id non mappati.\")\n\n# Aggiorna il dizionario delle categorie\ncoco_data[\"categories\"] = new_categories\n\n# Salva il file COCO modificato\nnew_coco_json_pth = Path(out_dataset_pth) / 'COCO_annotations_new.json'\nwith open(new_coco_json_pth, \"w\") as f:\n    json.dump(coco_data, f, indent=4)\n\nprint(f\"File COCO modificato salvato in {new_coco_json_pth}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:48:41.996961Z","iopub.execute_input":"2024-11-29T07:48:41.997286Z","iopub.status.idle":"2024-11-29T07:48:49.340961Z","shell.execute_reply.started":"2024-11-29T07:48:41.997250Z","shell.execute_reply":"2024-11-29T07:48:49.340087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica il file COCO modificato\nwith open(new_coco_json_pth, 'r') as f:\n    coco_data = json.load(f)\n\n# Crea un dizionario per mappare category_id a nome della categoria dal nuovo formato\ncategory_mapping = {\n    int(list(category.keys())[0]): list(category.values())[0]\n    for category in coco_data[\"categories\"]\n}\n\n# Estrai i category_id dalle annotazioni\ncategory_ids = [annotation[\"category_id\"] for annotation in coco_data[\"annotations\"]]\n\n# Conta le occorrenze di ogni category_id\ncategory_counts = Counter(category_ids)\n\n# Associa i nomi delle categorie alle loro occorrenze\ncategory_occurrences = {\n    cat_id: (category_mapping[cat_id], count)\n    for cat_id, count in category_counts.items()\n    if cat_id in category_mapping\n}\n\n# Ordina le categorie per occorrenze in ordine decrescente\ncategory_occurrences = dict(sorted(category_occurrences.items(), key=lambda item: item[1][1], reverse=True))\n\n# Prepara i dati per il grafico\ncategories = [f\"{cat_id} - {name}\" for cat_id, (name, _) in category_occurrences.items()]\noccurrences = [count for _, (_, count) in category_occurrences.items()]\n\n# Crea il grafico a barre\nplt.figure(figsize=(10, 8))\nplt.barh(categories, occurrences, color=\"skyblue\")\nplt.xlabel(\"Numero di Occorrenze\")\nplt.ylabel(\"Categorie\")\nplt.title(\"Distribuzione delle Occorrenze per Categoria\")\nplt.tight_layout()\n\n# Mostra il grafico\nplt.show()\n\n# Stampa le occorrenze per ogni categoria con ID\nfor cat_id, (name, count) in category_occurrences.items():\n    print(f\"Categoria ID: {cat_id}, Nome: {name}, Occorrenze: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:48:49.342134Z","iopub.execute_input":"2024-11-29T07:48:49.342990Z","iopub.status.idle":"2024-11-29T07:48:51.160264Z","shell.execute_reply.started":"2024-11-29T07:48:49.342949Z","shell.execute_reply":"2024-11-29T07:48:51.159197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IoU Loss","metadata":{}},{"cell_type":"code","source":"def assign_proposals_to_gt(proposals, gt_bboxes, iou_threshold=0.5):\n    assigned_labels = []\n    assigned_boxes = []\n\n    # Verifica che proposals e gt_bboxes siano tensori PyTorch\n    if not isinstance(proposals, torch.Tensor):\n        proposals = torch.tensor(proposals, dtype=torch.float32)\n    if not isinstance(gt_bboxes, torch.Tensor):\n        gt_bboxes = torch.tensor(gt_bboxes, dtype=torch.float32)\n\n    for proposal in proposals:\n        try:\n            iou_scores = [calculate_iou(proposal, gt_box) for gt_box in gt_bboxes]\n            max_iou = max(iou_scores)\n\n            if max_iou >= iou_threshold:\n                assigned_labels.append(1)  # Oggetto presente\n                assigned_boxes.append(gt_bboxes[iou_scores.index(max_iou)])\n            else:\n                assigned_labels.append(0)  # Background\n                assigned_boxes.append(proposal)\n        except Exception as e:\n            print(f\"Errore nel processamento della proposal {proposal}: {e}\")\n\n    return assigned_labels, assigned_boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:10:16.281498Z","iopub.execute_input":"2024-11-29T08:10:16.281757Z","iopub.status.idle":"2024-11-29T08:10:16.295268Z","shell.execute_reply.started":"2024-11-29T08:10:16.281733Z","shell.execute_reply":"2024-11-29T08:10:16.294411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_iou(box1, box2):\n    x1, y1, x2, y2 = box1\n    x1g, y1g, x2g, y2g = box2\n\n    # Assicurati che le coordinate siano valide\n    if x1 >= x2 or y1 >= y2 or x1g >= x2g or y1g >= y2g:\n        return 0.0\n\n    # Intersezione\n    inter_x1 = max(x1, x1g)\n    inter_y1 = max(y1, y1g)\n    inter_x2 = min(x2, x2g)\n    inter_y2 = min(y2, y2g)\n    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n\n    # Unione\n    area_box1 = (x2 - x1) * (y2 - y1)\n    area_box2 = (x2g - x1g) * (y2g - y1g)\n    union_area = area_box1 + area_box2 - inter_area\n\n    # Evita divisione per zero\n    if union_area == 0:\n        return 0.0\n\n    return inter_area / union_area\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:10:16.297044Z","iopub.execute_input":"2024-11-29T08:10:16.297626Z","iopub.status.idle":"2024-11-29T08:10:16.309336Z","shell.execute_reply.started":"2024-11-29T08:10:16.297588Z","shell.execute_reply":"2024-11-29T08:10:16.308469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou_loss(pred_bboxes, gt_bboxes):\n    \"\"\"\n    Calcola la perdita IoU media tra i bounding box predetti e quelli di ground truth.\n    \"\"\"\n    if len(pred_bboxes) != len(gt_bboxes):\n        raise ValueError(\"Il numero di pred_bboxes e gt_bboxes deve essere lo stesso.\")\n    \n    iou_scores = torch.tensor([calculate_iou(pred, gt) for pred, gt in zip(pred_bboxes, gt_bboxes)])\n    \n    return 1 - iou_scores.mean()  # Più bassa è la IoU, maggiore è la perdita","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:10:16.310527Z","iopub.execute_input":"2024-11-29T08:10:16.310868Z","iopub.status.idle":"2024-11-29T08:10:16.319110Z","shell.execute_reply.started":"2024-11-29T08:10:16.310831Z","shell.execute_reply":"2024-11-29T08:10:16.318357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cross_entropy_loss(pred_class_logits, labels):\n    \"\"\"\n    Calcola la Cross-Entropy Loss per la classificazione degli oggetti.\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    return criterion(pred_class_logits, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:10:16.384130Z","iopub.execute_input":"2024-11-29T08:10:16.384371Z","iopub.status.idle":"2024-11-29T08:10:16.388078Z","shell.execute_reply.started":"2024-11-29T08:10:16.384348Z","shell.execute_reply":"2024-11-29T08:10:16.387251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_loss(proposals, gt_bboxes, pred_bboxes, pred_class_logits, labels, iou_threshold=0.5):\n    # Assicurati che proposals e gt_bboxes siano tensori\n    if not isinstance(proposals, torch.Tensor):\n        proposals = torch.tensor(proposals, dtype=torch.float32)\n    if not isinstance(gt_bboxes, torch.Tensor):\n        gt_bboxes = torch.tensor(gt_bboxes, dtype=torch.float32)\n\n    # Assegna le proposte ai ground truth\n    assigned_labels, assigned_boxes = assign_proposals_to_gt(proposals, gt_bboxes, iou_threshold)\n\n    # Indici per le proposte positive e negative\n    pos_indices = [i for i, label in enumerate(assigned_labels) if label == 1]\n    neg_indices = [i for i, label in enumerate(assigned_labels) if label == 0]\n\n    if not pos_indices:\n        raise ValueError(\"Nessuna proposta positiva trovata.\")\n    \n    # Per le proposte positive, calcola la IoU Loss\n    pos_pred_bboxes = pred_bboxes[pos_indices]\n    pos_gt_bboxes = torch.tensor([assigned_boxes[i] for i in pos_indices], dtype=torch.float32)\n    \n    loss = iou_loss(pos_pred_bboxes, pos_gt_bboxes)\n\n    # Calcola la Cross-Entropy Loss per la classificazione\n    ce_loss = cross_entropy_loss(pred_class_logits, labels)\n    \n    # Combina la IoU Loss e la Cross-Entropy Loss\n    total_loss = loss + ce_loss\n\n    return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:10:16.389704Z","iopub.execute_input":"2024-11-29T08:10:16.390318Z","iopub.status.idle":"2024-11-29T08:10:16.399983Z","shell.execute_reply.started":"2024-11-29T08:10:16.390280Z","shell.execute_reply":"2024-11-29T08:10:16.399132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Modelli**\nModelli scelti:\n* [R-CNN con backbone AlexNet](https://medium.com/@selfouly/r-cnn-3a9beddfd55a)\n* [SPPNet con backbone ZF-5](https://arxiv.org/pdf/1406.4729)\n* [Faster R-CNN con backbone VGG16](https://medium.com/@fractal.ai/guide-to-build-faster-rcnn-in-pytorch-42d47cb0ecd3)","metadata":{}},{"cell_type":"markdown","source":"# R-CNN","metadata":{}},{"cell_type":"markdown","source":"## **DataLoader R-CNN**","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n\n        self.img_dir = img_dir\n\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n\n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox_str = annotation['bbox']\n            bbox = list(map(float, bbox_str.strip('[]').split(', ')))\n\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug = aug\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n    \n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        \n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Ridimensiona i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        categories = [c for c in categories if isinstance(c, int)]\n        if not categories:\n            categories = [-1]  # Etichetta speciale per immagini senza annotazioni\n        \n        scale_x = 320 / original_width\n        scale_y = 320 / original_height\n        scaled_bboxes = [\n            torch.tensor([\n                bbox[0] * scale_x,  # x_min\n                bbox[1] * scale_y,  # y_min\n                bbox[2] * scale_x,  # x_max\n                bbox[3] * scale_y   # y_max\n            ], dtype=torch.float32)\n            for bbox in bboxes\n        ] if bboxes else [torch.zeros(4, dtype=torch.float32)]\n        \n        labels = torch.tensor(categories, dtype=torch.int64)\n        proposals_tensor = self._generate_region_proposals(image)\n        processed_proposals = self._process_proposals(image_tensor, proposals_tensor)\n        \n        return {\n            \"image\": image_tensor,\n            \"labels\": labels,\n            \"bboxes\": scaled_bboxes,\n            \"regions\": processed_proposals  # Lista di tensori, ogni proposta è un tensore\n        }\n\n\n    def _generate_region_proposals(self, image):\n        img_np = np.array(image)\n        \n        if len(img_np.shape) == 3 and img_np.shape[0] == 3:\n            img_np = np.transpose(img_np, (1, 2, 0))  # Da [C, H, W] a [H, W, C]\n        elif len(img_np.shape) == 2:\n            img_np = np.stack([img_np] * 3, axis=-1)  # Da [H, W] a [H, W, 3]\n        elif img_np.shape[2] < 3:\n            img_np = np.repeat(img_np, 3, axis=2)  # Da [H, W, 1] a [H, W, 3]\n        elif img_np.shape[2] != 3:\n            raise ValueError(f\"L'immagine ha una forma non valida: {img_np.shape}\")\n        \n        _, regions = selectivesearch.selective_search(img_np, scale=500, sigma=0.9, min_size=10)\n        if len(regions) == 0:\n            print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n        \n        proposals = []\n        for region in regions:\n            x, y, w, h = region['rect']\n            if w > 0 and h > 0 and w >= 10 and h >= 10:\n                x_max, y_max = min(x + w, img_np.shape[1]), min(y + h, img_np.shape[0])\n                proposals.append([x, y, x_max, y_max])\n        \n        filtered_proposals = self._filter_proposals(proposals, img_np.shape[1], img_np.shape[0])\n        \n        return filtered_proposals  # Restituisce una lista di coordinate (non ancora tensorizzate)\n\n    def _filter_proposals(self, proposals, img_width, img_height, min_area=100, max_area_ratio=0.8):\n        unique_proposals = set(tuple(p) for p in proposals)\n        filtered = []\n        for x_min, y_min, x_max, y_max in unique_proposals:\n            width = x_max - x_min\n            height = y_max - y_min\n            area = width * height\n            if area >= min_area and area <= max_area_ratio * (img_width * img_height):\n                filtered.append((x_min, y_min, x_max, y_max))\n        return filtered\n\n    def _process_proposals(self, image_tensor, proposals, output_size=(227, 227)):\n        processed_proposals = []\n        for proposal in proposals:\n            try:\n                _, H, W = image_tensor.shape\n                x_min, y_min, x_max, y_max = map(int, proposal)\n                x_min, y_min = max(0, x_min), max(0, y_min)\n                x_max, y_max = min(W, x_max), min(H, y_max)\n    \n                # Controlla se la proposal è valida\n                if x_min < x_max and y_min < y_max:\n                    cropped_region = image_tensor[:, y_min:y_max, x_min:x_max]  # Ritaglio\n                    \n                    # Controlla che il ritaglio non sia vuoto\n                    if cropped_region.numel() == 0:\n                        print(f\"Ritaglio vuoto per proposal: {proposal}. Salto.\")\n                        continue\n                    \n                    # Controlla che il tensor sia 3D (C, H, W)\n                    if cropped_region.ndim != 3:\n                        print(f\"Proposal non valida per il ridimensionamento: {proposal}. Salto.\")\n                        continue\n                    \n                    resized_region = torch.nn.functional.interpolate(\n                        cropped_region.unsqueeze(0), size=output_size, mode='bilinear', align_corners=False\n                    ).squeeze(0)  # Ridimensiona\n                    \n                    processed_proposals.append(resized_region)\n            except Exception as e:\n                print(f\"Errore durante il processamento della proposal: {proposal}. Errore: {e}\")\n    \n        return processed_proposals  # Lista di tensori delle region proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:49:53.730412Z","iopub.execute_input":"2024-11-29T11:49:53.730769Z","iopub.status.idle":"2024-11-29T11:49:53.755122Z","shell.execute_reply.started":"2024-11-29T11:49:53.730738Z","shell.execute_reply":"2024-11-29T11:49:53.754451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione per creare un batch in cui i tensori di dimensioni variabili (come region proposals e bounding boxes)\n    siano gestiti correttamente senza causare errori di stack.\n    \"\"\"\n    # Creiamo delle liste separate per le immagini, etichette, bounding boxes, e region proposals\n    images = []\n    labels = []\n    bboxes = []\n    regions = []\n\n    # Iteriamo sul batch\n    for data in batch:\n        images.append(data['image'])\n        labels.append(data['labels'])\n        bboxes.append(data['bboxes'])\n        regions.append(data['regions'])\n\n    # Stack delle immagini, labels e bounding boxes (questi hanno dimensioni fisse per ogni immagine)\n    images = torch.stack(images, dim=0)\n    labels = torch.cat(labels, dim=0)  # Uniamo tutte le etichette in un unico tensor\n    bboxes = [torch.stack(b, dim=0) if len(b) > 0 else torch.zeros(1, 4) for b in bboxes]  # Gestiamo le bounding boxes\n\n    # Poiché le region proposals sono di dimensioni variabili, creiamo una lista di tensori\n    regions = [torch.stack(region, dim=0) if len(region) > 0 else torch.zeros(1, 227, 227) for region in regions]\n\n    return {\n        'image': images,\n        'labels': labels,\n        'bboxes': bboxes,\n        'regions': regions\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:49:53.756886Z","iopub.execute_input":"2024-11-29T11:49:53.757539Z","iopub.status.idle":"2024-11-29T11:49:53.772397Z","shell.execute_reply.started":"2024-11-29T11:49:53.757508Z","shell.execute_reply":"2024-11-29T11:49:53.771739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=True)\nvalid_dataset = CustomDataset(val_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \ntest_dataset = CustomDataset(test_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:49:53.773296Z","iopub.execute_input":"2024-11-29T11:49:53.773621Z","iopub.status.idle":"2024-11-29T11:50:01.260545Z","shell.execute_reply.started":"2024-11-29T11:49:53.773584Z","shell.execute_reply":"2024-11-29T11:50:01.259828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:50:01.262914Z","iopub.execute_input":"2024-11-29T11:50:01.263544Z","iopub.status.idle":"2024-11-29T11:50:01.269504Z","shell.execute_reply.started":"2024-11-29T11:50:01.263501Z","shell.execute_reply":"2024-11-29T11:50:01.268612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_txt_vs_json(txt_paths, coco_data):\n    \"\"\"\n    Controlla se le immagini del JSON sono presenti in almeno uno dei file TXT.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n        coco_data (dict): Dati in formato COCO.\n    \"\"\"\n    # Estrai i nomi delle immagini dal JSON\n    image_names = [image['file_name'] for image in coco_data['images']]\n    \n    # Inizializza un set per contenere tutte le immagini presenti nei TXT\n    txt_image_names = set()\n    \n    # Leggi i nomi delle immagini da ciascun file TXT\n    for txt_path in txt_paths:\n        with open(txt_path, 'r') as f:\n            txt_image_names.update(os.path.basename(line.strip()) for line in f.readlines())\n    \n    # Trova le immagini presenti nel JSON ma non in nessuno dei TXT\n    missing_in_txts = [name for name in image_names if name not in txt_image_names]\n    \n    # Verifica e stampa i risultati\n    print(\"\\nControllo completato:\")\n    if missing_in_txts:\n        print(f\"Errore: le seguenti immagini non sono presenti in nessuno dei file TXT forniti:\\n{missing_in_txts}\")\n    else:\n        print(\"Tutte le immagini del JSON sono presenti in almeno uno dei file TXT.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:50:01.270528Z","iopub.execute_input":"2024-11-29T11:50:01.270763Z","iopub.status.idle":"2024-11-29T11:50:01.284574Z","shell.execute_reply.started":"2024-11-29T11:50:01.270739Z","shell.execute_reply":"2024-11-29T11:50:01.283718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta il numero di immagini nel JSON\nnum_images = len(coco_data['images'])\nprint(f\"Numero di immagini nel JSON: {num_images}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:50:01.285582Z","iopub.execute_input":"2024-11-29T11:50:01.285885Z","iopub.status.idle":"2024-11-29T11:50:01.297257Z","shell.execute_reply.started":"2024-11-29T11:50:01.285857Z","shell.execute_reply":"2024-11-29T11:50:01.296522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estrai i nomi delle immagini dal JSON\nimage_names = [image['file_name'] for image in coco_data['images']]\n\n# Stampa i primi 5 nomi delle immagini nel JSON\nprint(f\"Primi 5 nomi delle immagini nel JSON: {image_names[:5]}\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:50:01.298193Z","iopub.execute_input":"2024-11-29T11:50:01.298422Z","iopub.status.idle":"2024-11-29T11:50:01.311982Z","shell.execute_reply.started":"2024-11-29T11:50:01.298397Z","shell.execute_reply":"2024-11-29T11:50:01.311089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_lines_in_txt(txt_paths):\n    \"\"\"\n    Conta il numero di righe in ciascun file TXT dato il percorso e calcola la somma totale delle righe.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n    \"\"\"\n    total_lines = 0  # Variabile per accumulare il numero totale di righe\n    \n    for txt_path in txt_paths:\n        try:\n            # Apri il file e conta le righe\n            with open(txt_path, 'r') as f:\n                num_lines = sum(1 for line in f)\n            total_lines += num_lines  # Aggiungi il numero di righe del file al totale\n            print(f\"Numero di righe nel file {txt_path}: {num_lines}\")\n        except FileNotFoundError:\n            print(f\"Errore: il file {txt_path} non è stato trovato.\")\n        except Exception as e:\n            print(f\"Errore nel leggere il file {txt_path}: {e}\")\n    \n    # Stampa la somma totale delle righe\n    print(f\"\\nSomma totale delle righe in tutti i file: {total_lines}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:50:01.314930Z","iopub.execute_input":"2024-11-29T11:50:01.315296Z","iopub.status.idle":"2024-11-29T11:50:01.325932Z","shell.execute_reply.started":"2024-11-29T11:50:01.315270Z","shell.execute_reply":"2024-11-29T11:50:01.325103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_paths = [train_txt_pth, val_txt_pth, test_txt_pth]\ncount_lines_in_txt(txt_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:50:01.326772Z","iopub.execute_input":"2024-11-29T11:50:01.327047Z","iopub.status.idle":"2024-11-29T11:50:01.346665Z","shell.execute_reply.started":"2024-11-29T11:50:01.327023Z","shell.execute_reply":"2024-11-29T11:50:01.345853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_txt_vs_json(txt_paths, coco_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:50:01.347674Z","iopub.execute_input":"2024-11-29T11:50:01.347927Z","iopub.status.idle":"2024-11-29T11:50:01.405551Z","shell.execute_reply.started":"2024-11-29T11:50:01.347903Z","shell.execute_reply":"2024-11-29T11:50:01.404678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_dataloader(loader):\n    \"\"\"\n    Funzione per controllare il comportamento di un DataLoader.\n    Visualizza alcune immagini insieme alle loro annotazioni per verificare il corretto funzionamento.\n\n    Args:\n        loader (DataLoader): Il DataLoader da verificare.\n    \"\"\"\n    for batch_idx, batch in enumerate(loader):\n        print(f\"Batch {batch_idx + 1}:\")\n        \n        # Estrai i dati dal dizionario\n        images = batch[\"image\"]\n        labels = batch[\"labels\"]\n        bboxes = batch[\"bboxes\"]\n        regions = batch[\"regions\"]\n\n        # Stampa le shape e le dimensioni dei dati\n        print(f\"  Shape delle immagini (batch): {images.shape}\")\n        print(f\"  Numero di etichette nel batch: {len(labels)}\")\n\n        # Controlla se il batch è vuoto\n        if images.size(0) == 0:\n            print(\"Batch vuoto. Procedo con il batch successivo.\")\n            continue\n\n        # Scegli un'immagine casuale dal batch\n        random_index = random.randint(0, images.size(0) - 1)\n        image = images[random_index].permute(1, 2, 0).numpy()  # Da [C, H, W] a [H, W, C]\n\n        # Visualizza l'immagine\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.imshow(image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Immagine nel batch {batch_idx + 1}, indice {random_index}\")\n\n        # Etichette\n        label = labels[random_index]\n        label_info = label.tolist() if isinstance(label, torch.Tensor) else label\n        print(f\"  Etichette: {label_info}\")\n\n        # Bounding boxes\n        bbox = bboxes[random_index]\n        for box in bbox:\n            if isinstance(box, torch.Tensor):\n                box = box.tolist()  # Converte il bounding box in lista\n    \n            # Converti da [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            x_min, y_min, width, height = box\n            x_max = x_min + width\n            y_max = y_min + height\n    \n            # Opzionale: Verifica dei limiti dell'immagine\n            H, W, _ = image.shape\n            x_min, y_min = max(0, x_min), max(0, y_min)\n            x_max, y_max = min(W, x_max), min(H, y_max)\n    \n            # Disegna il bounding box\n            rect = plt.Rectangle((x_min, y_min), width, height,\n                                  edgecolor='green', facecolor='none', linewidth=1.5)\n            ax.add_patch(rect)\n\n        # Visualizza alcune region proposals come immagini separate\n        region_proposals = regions[random_index]\n        print(f\"  Numero di region proposals: {len(region_proposals)}\")\n\n        fig, axs = plt.subplots(1, min(5, len(region_proposals)), figsize=(15, 5))\n        for i, proposal in enumerate(region_proposals[:5]):\n            proposal_image = proposal.permute(1, 2, 0).numpy()  # Da [C, H, W] a [H, W, C]\n            axs[i].imshow(proposal_image)\n            axs[i].axis(\"off\")\n            axs[i].set_title(f\"Region Proposal {i + 1}\")\n        plt.show()\n\n        break  # Mostra solo il primo batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:53:57.520194Z","iopub.execute_input":"2024-11-29T11:53:57.520544Z","iopub.status.idle":"2024-11-29T11:53:57.531330Z","shell.execute_reply.started":"2024-11-29T11:53:57.520515Z","shell.execute_reply":"2024-11-29T11:53:57.530418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui il controllo per i dataloader\ncheck_dataloader(train_loader)\ncheck_dataloader(val_loader)\ncheck_dataloader(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:57:26.594415Z","iopub.execute_input":"2024-11-29T11:57:26.595252Z","iopub.status.idle":"2024-11-29T11:58:25.946365Z","shell.execute_reply.started":"2024-11-29T11:57:26.595217Z","shell.execute_reply":"2024-11-29T11:58:25.945478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modello R-CNN (AlexNet)","metadata":{}},{"cell_type":"code","source":"class RCNNModel(nn.Module):\n    def __init__(self, base_model='alexnet', num_classes=12):  # 11 classi + sfondo\n        super(RCNNModel, self).__init__()\n        \n        # Carica AlexNet con pesi pre-addestrati usando la nuova sintassi\n        alexnet = models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)  # O usa AlexNet_Weights.DEFAULT\n        self.backbone = alexnet.features  # Prendo solo la parte di estrazione delle feature del modello\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Pooling adattivo\n\n        # Classificatore per multi-classe\n        self.classifier = nn.Sequential(\n            nn.Linear(alexnet.classifier[1].in_features, 512),  # Numero di feature dal penultimo layer\n            nn.ReLU(),\n            nn.Linear(256 * 9 * 9, num_classes)  # Numero di classi\n        )\n        #self.classifier = nn.Linear(256 * 9 * 9, num_classes)\n\n        #parte per trovare il numero di feature prodotte da AlexNet\n        dummy_input = torch.randn(1, 3, 224, 224)  # Input fittizio\n        dummy_backbone_out = self.backbone(dummy_input)  # Uscita dal backbone\n        C = dummy_backbone_out.size(1)  # Numero di canali\n        self.in_features = C * 7 * 7  # 7x7 è la dimensione fissa dopo il RoI Pooling\n\n        # Opzionale: Bounding Box Regressor\n        self.bbox_regressor = nn.Sequential(\n            nn.Linear(self.in_features, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4)  # 4 coordinate per il bounding box (x1, y1, x2, y2)\n        )\n        self.bbox_regressor = nn.Sequential(\n            nn.Linear(self.in_features, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4)\n        )\n\n    def forward(self, x, region_proposals):\n        # Estrai feature dalla backbone\n        x = self.backbone(x)\n        print(f\"Shape after backbone: {x.shape}\")\n    \n        # Lista per raccogliere i risultati del RoI Pooling per ogni immagine nel batch\n        rois_pooled = []\n\n        \n        for i, proposal in enumerate(region_proposals):\n            #CONTROLLO SULLA DIMENSIONE DELLE PROPOSAL\n            if proposal.size(0) == 0:\n                print(f\"Image {i} ha zero region proposals, saltata.\")\n                continue\n            assert proposal.dim() == 2 and proposal.size(1) == 4, \\\n                f\"Proposal shape error: got {proposal.shape}, expected [num_rois, 4]\" #IL PROBLEMA è QUI -> torch.Size([1, 3, 227, 227])\n\n            print(f\"Shape of PROPOSAL: {proposal.shape}\") \n            # Verifica che ogni proposta sia di forma [num_rois, 4]\n            # Aggiungi l'indice batch (i) alla proposta, creando la forma [num_rois, 5]\n            batch_indices = torch.full((proposal.size(0), 1), i, dtype=torch.float32, device=proposal.device)\n            proposal = proposal.float()  # Assicurati che sia in float32\n            #faccio un check sulle dimensioni di proposal\n            print(f\"Shape of PROPOSAL: {rois.shape}\") \n            rois = torch.cat([batch_indices, proposal], dim=1)  # Forma finale: [num_rois, 5]\n\n            # Verifica la forma finale delle RoIs\n            print(f\"Shape of rois: {rois.shape}\")  # Questo ti darà un'indicazione del numero totale di RoI nel batch\n            \n            # Applica RoI Pooling per l'immagine corrente\n            pooled_rois = torchvision.ops.roi_pool(x[i:i+1], rois, output_size=(7, 7), spatial_scale=1.0)\n            \n            # Aggiungi il risultato del RoI Pooling per questa immagine\n            rois_pooled.append(pooled_rois)\n\n            if not rois_pooled:\n                raise ValueError(\"Nessuna region proposal valida nel batch.\")\n        \n        # Concatenazione dei risultati di RoI Pooling per tutte le immagini\n        x = torch.cat(rois_pooled, dim=0)  # Forma finale: [tot_num_rois, C, 7, 7]\n        print(f\"Shape of roi_pooled: {x.shape}\")  # Debug\n    \n        # Applica RoI Pooling per ottenere regioni fisse (7x7 per ogni RoI)\n        #x = torchvision.ops.roi_pool(x, rois, output_size=(7, 7), spatial_scale=1.0)  # Output: [num_rois, C, 7, 7]\n    \n        # Flattening\n        x = torch.flatten(x, start_dim=1)\n        print(f\"Shape of x after flattening: {x.shape}\")\n    \n        # Classificazione multi-classe\n        class_logits = self.classifier(x)\n    \n        # Regressione del Bounding Box (opzionale)\n        bbox_preds = self.bbox_regressor(x)\n    \n        return class_logits, bbox_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.178598Z","iopub.status.idle":"2024-11-29T11:51:18.179064Z","shell.execute_reply.started":"2024-11-29T11:51:18.178831Z","shell.execute_reply":"2024-11-29T11:51:18.178856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_loader, val_loader, num_classes, device='cuda'):\n        \"\"\"\n        Classe per il training e la validazione di un modello RCNN.\n        Args:\n            model (nn.Module): il modello da addestrare.\n            train_loader (DataLoader): dataloader per il set di training.\n            val_loader (DataLoader): dataloader per il set di validazione.\n            num_classes (int): numero di classi (incluso lo sfondo).\n            device (str): dispositivo per l'addestramento ('cuda' o 'cpu').\n        \"\"\"\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.num_classes = num_classes\n\n        # Ottimizzatore\n        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n\n        # Per tenere traccia della storia di training e validazione\n        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\n    def train_one_epoch(self):\n        self.model.train()\n        total_loss = 0.0\n    \n        for batch in self.train_loader:\n            # Sposta i dati sulla device\n            images = batch['image'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            bboxes = [bbox.to(self.device) for bbox in batch['bboxes']]\n            #PROBLEMA NELLA GESTIONE DELLE REGION PROPOSALS    \n            proposals = [region.to(self.device) for region in batch['regions']]\n            if not any(len(p) > 0 for p in proposals):\n                print(f\"Batch {index} non ha region proposals valide. Salto il batch.\")\n                continue  # Salta il batch senza proposals\n\n            \n            # Forward pass: Passiamo sia le immagini che le regioni proposte al modello\n            class_logits, bbox_preds = self.model(images, proposals)  # Passiamo entrambe le variabili\n    \n            # Calcola la perdita\n            loss = compute_loss(\n                proposals=proposals,\n                gt_bboxes=bboxes,\n                pred_bboxes=bbox_preds,  # Usa bbox_preds invece di outputs['bboxes']\n                pred_class_logits=class_logits,  # Usa class_logits invece di outputs['class_logits']\n                labels=labels\n            )\n    \n            total_loss += loss.item()\n\n            # Backpropagation\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n    \n        return total_loss / len(self.train_loader)\n\n    def validate(self):\n        #s += loss.item() * images.size(0)\n        running_loss += loss.item() * images.size(0)\n    \n                # Calcolo accura\"\"\"Esegue la validazione sul set di validazione.\"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n    \n        with torch.no_grad():\n            for batch in self.val_loader:\n                # Sposta i dati sulla device\n                images = batch['image'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                bboxes = [bbox.to(self.device) for bbox in batch['bboxes']]\n                proposals = [region.to(self.device) for region in batch['regions']]\n    \n                # Forward pass: Passiamo sia le immagini che le regioni proposte al modello\n                class_logits, bbox_preds = self.model(images, proposals)  # Passiamo entrambe le variabili\n    \n                # Calcola la perdita\n                loss = compute_loss(\n                    proposals=proposals,\n                    gt_bboxes=bboxes,\n                    pred_bboxes=bbox_preds,  # Usa bbox_preds invece di outputs['bboxes']\n                    pred_class_logits=class_logits,  # Usa class_logits invece di outputs['class_logits']\n                    labels=labels\n                )\n                running_lostezza\n                _, preds = torch.max(class_logits, 1)  # Classifica usando class_logits\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n    \n        # Calcola la perdita media e accuratezza\n        #epoch_loss = running_loss / len(self.val_loader.dataset)\n        epoch_loss = running_loss / total\n        accuracy = correct / total\n        return epoch_loss, accuracy\n\n    def train(self, num_epochs):\n        \"\"\"Esegue il training e la validazione per un numero di epoche.\"\"\"\n        for epoch in range(num_epochs):\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n            # Training\n            train_loss = self.train_one_epoch()\n            print(f\"Train Loss: {train_loss:.4f}\")\n\n            # Validazione\n            val_loss, val_acc = self.validate()\n            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n\n            # Salva i risultati\n            self.history['train_loss'].append(train_loss)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n\n    def save_model(self, path):\n        \"\"\"Salva il modello addestrato.\"\"\"\n        torch.save(self.model.state_dict(), path)\n        print(f\"Modello salvato in {path}\")\n\n    def load_model(self, path):\n        \"\"\"Carica un modello addestrato.\"\"\"\n        self.model.load_state_dict(torch.load(path))\n        self.model.to(self.device)\n        print(f\"Modello caricato da {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.180989Z","iopub.status.idle":"2024-11-29T11:51:18.181420Z","shell.execute_reply.started":"2024-11-29T11:51:18.181193Z","shell.execute_reply":"2024-11-29T11:51:18.181215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(test_image_path, model, top_k=2000):\n    # Carica l'immagine\n    image = cv2.imread(test_image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Genera region proposals con Selective Search\n    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n    ss.setBaseImage(image)\n    ss.switchToSelectiveSearchFast()\n    ss_results = ss.process()[:top_k]  # Prendi le prime 2000 proposte\n    \n    # Trasforma region proposals in tensor\n    transform = transforms.Compose([\n        transforms.Resize((227, 227)),\n        transforms.ToTensor()\n    ])\n    proposals = []\n    for region in ss_results:\n        x, y, w, h = region\n        cropped = image_rgb[y:y + h, x:x + w]\n        resized = cv2.resize(cropped, (227, 227), interpolation=cv2.INTER_AREA)\n        proposals.append(transform(Image.fromarray(resized)))\n    proposals = torch.stack(proposals)  # Shape: (top_k, 3, 224, 224)\n    \n    # Predizioni del modello\n    model.eval()\n    with torch.no_grad():\n        class_logits = model(proposals)\n        predictions = torch.argmax(class_logits, dim=1).cpu().numpy()  # 0 o 1\n    \n    # Filtra region proposals positive\n    positive_regions = [ss_results[i] for i in range(len(predictions)) if predictions[i] == 1]\n    \n    return positive_regions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.182659Z","iopub.status.idle":"2024-11-29T11:51:18.183830Z","shell.execute_reply.started":"2024-11-29T11:51:18.183569Z","shell.execute_reply":"2024-11-29T11:51:18.183593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Version:\", torch.version.cuda)\nprint(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.185028Z","iopub.status.idle":"2024-11-29T11:51:18.185467Z","shell.execute_reply.started":"2024-11-29T11:51:18.185237Z","shell.execute_reply":"2024-11-29T11:51:18.185259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inizializzazione del modello\nmodel = RCNNModel(base_model='alexnet', num_classes=12)  # 12 classi = 11 + sfondo, do in input anche le region proposals\n\n# Inizializzazione del Trainer\ntrainer = Trainer(model=model, \n                  train_loader=train_loader, \n                  val_loader=val_loader, \n                  num_classes=12,\n                  device=device)  # Usa 'cuda' per l'addestramento su GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.186709Z","iopub.status.idle":"2024-11-29T11:51:18.187144Z","shell.execute_reply.started":"2024-11-29T11:51:18.186923Z","shell.execute_reply":"2024-11-29T11:51:18.186946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui l'addestramento per 10 epoche\ntrainer.train(num_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.188707Z","iopub.status.idle":"2024-11-29T11:51:18.189136Z","shell.execute_reply.started":"2024-11-29T11:51:18.188919Z","shell.execute_reply":"2024-11-29T11:51:18.188941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model('rcnn_model.pth')  # Salva il modello addestrato","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.190474Z","iopub.status.idle":"2024-11-29T11:51:18.190910Z","shell.execute_reply.started":"2024-11-29T11:51:18.190675Z","shell.execute_reply":"2024-11-29T11:51:18.190697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.load_model('rcnn_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:51:18.192335Z","iopub.status.idle":"2024-11-29T11:51:18.192618Z","shell.execute_reply.started":"2024-11-29T11:51:18.192484Z","shell.execute_reply":"2024-11-29T11:51:18.192498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Faster R-CNN (VGG16)","metadata":{}},{"cell_type":"markdown","source":"## **DataLoader Faster R-CNN**","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n\n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n\n        self.img_dir = img_dir\n\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n\n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox_str = annotation['bbox']\n            bbox = list(map(float, bbox_str.strip('[]').split(', ')))\n\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug = aug\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n    \n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        \n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Ridimensiona i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        categories = [c for c in categories if isinstance(c, int)]\n        if not categories:\n            categories = [-1]  # Etichetta speciale per immagini senza annotazioni\n        \n        scale_x = 320 / original_width\n        scale_y = 320 / original_height\n        scaled_bboxes = [\n            torch.tensor([\n                bbox[0] * scale_x,  # x_min\n                bbox[1] * scale_y,  # y_min\n                bbox[2] * scale_x,  # x_max\n                bbox[3] * scale_y   # y_max\n            ], dtype=torch.float32)\n            for bbox in bboxes\n        ] if bboxes else [torch.zeros(4, dtype=torch.float32)]\n        \n        labels = torch.tensor(categories, dtype=torch.int64)\n        \n        return {\n            \"image\": image_tensor,\n            \"labels\": labels,\n            \"bboxes\": scaled_bboxes,\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione per creare un batch in cui i tensori di dimensioni variabili (come region proposals e bounding boxes)\n    siano gestiti correttamente senza causare errori di stack.\n    \"\"\"\n    # Creiamo delle liste separate per le immagini, etichette, bounding boxes, e region proposals\n    images = []\n    labels = []\n    bboxes = []\n\n    # Iteriamo sul batch\n    for data in batch:\n        images.append(data['image'])\n        labels.append(data['labels'])\n        bboxes.append(data['bboxes'])\n\n    # Stack delle immagini, labels e bounding boxes (questi hanno dimensioni fisse per ogni immagine)\n    images = torch.stack(images, dim=0)\n    labels = torch.cat(labels, dim=0)  # Uniamo tutte le etichette in un unico tensor\n    bboxes = [torch.stack(b, dim=0) if len(b) > 0 else torch.zeros(1, 4) for b in bboxes]  # Gestiamo le bounding boxes\n\n    return {\n        'image': images,\n        'labels': labels,\n        'bboxes': bboxes\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=True)\nvalid_dataset = CustomDataset(val_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \ntest_dataset = CustomDataset(test_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Check DataLoader Faster R-CNN**","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_txt_vs_json(txt_paths, coco_data):\n    \"\"\"\n    Controlla se le immagini del JSON sono presenti in almeno uno dei file TXT.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n        coco_data (dict): Dati in formato COCO.\n    \"\"\"\n    # Estrai i nomi delle immagini dal JSON\n    image_names = [image['file_name'] for image in coco_data['images']]\n    \n    # Inizializza un set per contenere tutte le immagini presenti nei TXT\n    txt_image_names = set()\n    \n    # Leggi i nomi delle immagini da ciascun file TXT\n    for txt_path in txt_paths:\n        with open(txt_path, 'r') as f:\n            txt_image_names.update(os.path.basename(line.strip()) for line in f.readlines())\n    \n    # Trova le immagini presenti nel JSON ma non in nessuno dei TXT\n    missing_in_txts = [name for name in image_names if name not in txt_image_names]\n    \n    # Verifica e stampa i risultati\n    print(\"\\nControllo completato:\")\n    if missing_in_txts:\n        print(f\"Errore: le seguenti immagini non sono presenti in nessuno dei file TXT forniti:\\n{missing_in_txts}\")\n    else:\n        print(\"Tutte le immagini del JSON sono presenti in almeno uno dei file TXT.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta il numero di immagini nel JSON\nnum_images = len(coco_data['images'])\nprint(f\"Numero di immagini nel JSON: {num_images}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estrai i nomi delle immagini dal JSON\nimage_names = [image['file_name'] for image in coco_data['images']]\n\n# Stampa i primi 5 nomi delle immagini nel JSON\nprint(f\"Primi 5 nomi delle immagini nel JSON: {image_names[:5]}\")   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_lines_in_txt(txt_paths):\n    \"\"\"\n    Conta il numero di righe in ciascun file TXT dato il percorso e calcola la somma totale delle righe.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n    \"\"\"\n    total_lines = 0  # Variabile per accumulare il numero totale di righe\n    \n    for txt_path in txt_paths:\n        try:\n            # Apri il file e conta le righe\n            with open(txt_path, 'r') as f:\n                num_lines = sum(1 for line in f)\n            total_lines += num_lines  # Aggiungi il numero di righe del file al totale\n            print(f\"Numero di righe nel file {txt_path}: {num_lines}\")\n        except FileNotFoundError:\n            print(f\"Errore: il file {txt_path} non è stato trovato.\")\n        except Exception as e:\n            print(f\"Errore nel leggere il file {txt_path}: {e}\")\n    \n    # Stampa la somma totale delle righe\n    print(f\"\\nSomma totale delle righe in tutti i file: {total_lines}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_paths = [train_txt_pth, val_txt_pth, test_txt_pth]\ncount_lines_in_txt(txt_paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_txt_vs_json(txt_paths, coco_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_dataloader(loader):\n    \"\"\"\n    Funzione per controllare il comportamento di un DataLoader.\n    Visualizza alcune immagini insieme alle loro annotazioni per verificare il corretto funzionamento.\n\n    Args:\n        loader (DataLoader): Il DataLoader da verificare.\n    \"\"\"\n    for batch_idx, batch in enumerate(loader):\n        print(f\"Batch {batch_idx + 1}:\")\n        \n        # Estrai i dati dal dizionario\n        images = batch[\"image\"]\n        labels = batch[\"labels\"]\n        bboxes = batch[\"bboxes\"]\n\n        # Stampa le shape e le dimensioni dei dati\n        print(f\"  Shape delle immagini (batch): {images.shape}\")\n        print(f\"  Numero di etichette nel batch: {len(labels)}\")\n\n        # Controlla se il batch è vuoto\n        if images.size(0) == 0:\n            print(\"Batch vuoto. Procedo con il batch successivo.\")\n            continue\n\n        # Scegli un'immagine casuale dal batch\n        random_index = random.randint(0, images.size(0) - 1)\n        image = images[random_index].permute(1, 2, 0).numpy()  # Da [C, H, W] a [H, W, C]\n\n        # Visualizza l'immagine\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.imshow(image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Immagine nel batch {batch_idx + 1}, indice {random_index}\")\n\n        # Etichette\n        label = labels[random_index]\n        label_info = label.tolist() if isinstance(label, torch.Tensor) else label\n        print(f\"  Etichette: {label_info}\")\n\n        # Bounding boxes\n        bbox = bboxes[random_index]\n        for box in bbox:\n            if isinstance(box, torch.Tensor):\n                box = box.tolist()  # Converte il bounding box in lista\n    \n            # Converti da [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            x_min, y_min, width, height = box\n            x_max = x_min + width\n            y_max = y_min + height\n    \n            # Opzionale: Verifica dei limiti dell'immagine\n            H, W, _ = image.shape\n            x_min, y_min = max(0, x_min), max(0, y_min)\n            x_max, y_max = min(W, x_max), min(H, y_max)\n    \n            # Disegna il bounding box\n            rect = plt.Rectangle((x_min, y_min), width, height,\n                                  edgecolor='green', facecolor='none', linewidth=1.5)\n            ax.add_patch(rect)\n\n        break  # Mostra solo il primo batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui il controllo per i dataloader\ncheck_dataloader(train_loader)\ncheck_dataloader(val_loader)\ncheck_dataloader(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Modello Faster R-CNN (VGG16)**","metadata":{}},{"cell_type":"code","source":"'''\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\n# Usa il backbone VGG16\nvgg16_backbone = models.vgg16(pretrained=True).features\n\n# Carica Faster R-CNN con backbone personalizzato\nfaster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\nfaster_rcnn_model.backbone = vgg16_backbone\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SPPNET","metadata":{}},{"cell_type":"markdown","source":"## DataLoader SPPNet","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Check DataLoader SPPNet**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modello SPPNet (ZF-5)","metadata":{}},{"cell_type":"code","source":"# Define the SPP Layer\nclass SpatialPyramidPooling(nn.Module):\n    def __init__(self, output_sizes):\n        \"\"\"\n        :param output_sizes: List of output sizes for pyramid levels (e.g., [1, 2, 4]).\n        \"\"\"\n        super(SpatialPyramidPooling, self).__init__()\n        self.output_sizes = output_sizes\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        pooled_outputs = []\n        for output_size in self.output_sizes:\n            kernel_size = (height // output_size, width // output_size)\n            stride = kernel_size\n            padding = (height % output_size // 2, width % output_size // 2)\n            pooled = F.adaptive_max_pool2d(x, output_size)\n            pooled_outputs.append(pooled.view(batch_size, -1))  # Flatten each level\n        return torch.cat(pooled_outputs, dim=1)  # Concatenate pyramid levels\n\n# Define the ZFNet Backbone with SPP Layer\nclass SPPNetZF5(nn.Module):\n    def __init__(self, num_classes=1000, spp_output_sizes=[1, 2, 4]):\n        super(SPPNetZF5, self).__init__()\n        # ZFNet convolutional layers\n        self.conv1 = nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=2, padding=2)\n        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n\n        self.spp = SpatialPyramidPooling(output_sizes=spp_output_sizes)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(self._calculate_fc_input_size(spp_output_sizes), 4096)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, num_classes)\n\n    def _calculate_fc_input_size(self, spp_output_sizes):\n        \"\"\"\n        Calculate the total size of the output vector from the SPP layer.\n        \"\"\"\n        return sum([size * size for size in spp_output_sizes]) * 256\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = self.spp(x)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.fc3(x)\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, device, optimizer, criterion, train_loader, val_loader):\n        \"\"\"\n        :param model: The SPPNet model.\n        :param device: The device (CPU or GPU).\n        :param optimizer: Optimizer for training.\n        :param criterion: Loss function.\n        :param train_loader: DataLoader for training.\n        :param val_loader: DataLoader for validation.\n        \"\"\"\n        self.model = model.to(device)\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n\n            # Forward\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n\n            # Backward\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # Metrics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            if batch_idx % 10 == 0:\n                print(\n                    f'Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, '\n                    f'Loss: {loss.item():.4f}, Accuracy: {100. * correct / total:.2f}%'\n                )\n\n        train_loss = running_loss / len(self.train_loader)\n        train_accuracy = 100. * correct / total\n        return train_loss, train_accuracy\n\n    def validate(self):\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, targets in self.val_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n\n                # Forward\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n\n                # Metrics\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n        val_loss = running_loss / len(self.val_loader)\n        val_accuracy = 100. * correct / total\n        return val_loss, val_accuracy\n\n    def fit(self, epochs):\n        for epoch in range(epochs):\n            train_loss, train_accuracy = self.train_one_epoch(epoch)\n            val_loss, val_accuracy = self.validate()\n\n            print(\n                f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n                f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%'\n            )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}