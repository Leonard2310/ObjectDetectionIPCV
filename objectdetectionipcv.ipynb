{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2571636,"sourceType":"datasetVersion","datasetId":1561333}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Librerie**","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:39:57.141558Z","iopub.execute_input":"2024-11-22T13:39:57.142080Z","iopub.status.idle":"2024-11-22T13:40:07.908801Z","shell.execute_reply.started":"2024-11-22T13:39:57.142020Z","shell.execute_reply":"2024-11-22T13:40:07.907874Z"}},"outputs":[{"name":"stdout","text":"Collecting selectivesearch\n  Downloading selectivesearch-0.4.tar.gz (3.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (1.26.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from selectivesearch) (0.23.2)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (1.14.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->selectivesearch) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->selectivesearch) (3.1.2)\nBuilding wheels for collected packages: selectivesearch\n  Building wheel for selectivesearch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for selectivesearch: filename=selectivesearch-0.4-py3-none-any.whl size=4335 sha256=213f5e4d35f9194a84ac3ca110a2ee85d7c024e9dc0d57aff93f2e60ea80f7ff\n  Stored in directory: /root/.cache/pip/wheels/0e/49/95/01447a4e0f48a135ac91fbdb1dd2a1c0523e40e29957b383a3\nSuccessfully built selectivesearch\nInstalling collected packages: selectivesearch\nSuccessfully installed selectivesearch-0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport imageio.v3 as imageio\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nimport cv2\nimport shutil\nimport json\nimport yaml\nimport random\nimport time\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\nimport concurrent.futures\nimport multiprocessing as mp\nfrom PIL import Image, ImageOps\nfrom collections import defaultdict, Counter\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TF\nimport torch.optim as optim\nimport re\nimport selectivesearch\nimport torch.optim as optim\nfrom torchvision import models\nfrom torchvision.models import AlexNet_Weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:07.910623Z","iopub.execute_input":"2024-11-22T13:40:07.910940Z","iopub.status.idle":"2024-11-22T13:40:13.029629Z","shell.execute_reply.started":"2024-11-22T13:40:07.910912Z","shell.execute_reply":"2024-11-22T13:40:13.028961Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **Dataset Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"Il nostro dataset è xView, un'analisi preliminare del dataset è presente nel [documento](https://medium.com/picterra/the-xview-dataset-and-baseline-results-5ab4a1d0f47f) linkato.\n\n\nQuesto dataset rappresentava il migliore per il benchmarking di visione artificiale satellitare. La documentazione tecnica spiega che i dati sono ottenuti dai satelliti **WorldView-3**, con una distanza di campionamento al suolo uniforme di 0,3 metri. Ciò conferisce una risoluzione più elevata e omogenea rispetto alla maggior parte degli altri dataset satellitari esistenti all'epoca, molti dei quali si basano invece su fotografie aeree. Quest'ultime, infatti, presentano differenze nella distorsione causate dall'angolo di ripresa, poiché sono scattate da velivoli a bassa quota. \n\nIl dataset **xView** offre una copertura geografica ampia e diversificata, includendo anche aree meno sviluppate e urbanizzate, fornendo quindi una maggiore varietà di scenari rispetto a dataset più convenzionali. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Elenco delle categorie del dataset:  \n\n#### 0-3: Velivoli  \n- 0: Fixed-wing Aircraft  \n- 1: Small Aircraft  \n- 2: Passenger/Cargo Plane  \n- 3: Helicopter  \n\n#### 4-15: Veicoli terrestri  \n- 4: Passenger Vehicle  \n- 5: Small Car  \n- 6: Bus  \n- 7: Pickup Truck  \n- 8: Utility Truck  \n- 9: Truck  \n- 10: Cargo Truck  \n- 11: Truck Tractor w/ Box Trailer  \n- 12: Truck Tractor  \n- 13: Trailer  \n- 14: Truck Tractor w/ Flatbed Trailer  \n- 15: Truck Tractor w/ Liquid Tank  \n\n#### 16: Veicoli speciali  \n- 16: Crane Truck  \n\n#### 17-22: Veicoli ferroviari  \n- 17: Railway Vehicle  \n- 18: Passenger Car  \n- 19: Cargo/Container Car  \n- 20: Flat Car  \n- 21: Tank car  \n- 22: Locomotive  \n\n#### 23-32: Imbarcazioni  \n- 23: Maritime Vessel  \n- 24: Motorboat  \n- 25: Sailboat  \n- 26: Tugboat  \n- 27: Barge  \n- 28: Fishing Vessel  \n- 29: Ferry  \n- 30: Yacht  \n- 31: Container Ship  \n- 32: Oil Tanker  \n\n#### 33-45: Veicoli da costruzione/industriali  \n- 33: Engineering Vehicle  \n- 34: Tower crane  \n- 35: Container Crane  \n- 36: Reach Stacker  \n- 37: Straddle Carrier  \n- 38: Mobile Crane  \n- 39: Dump Truck  \n- 40: Haul Truck  \n- 41: Scraper/Tractor  \n- 42: Front loader/Bulldozer  \n- 43: Excavator  \n- 44: Cement Mixer  \n- 45: Ground Grader  \n\n#### 46-51: Edifici e strutture  \n- 46: Hut/Tent  \n- 47: Shed  \n- 48: Building  \n- 49: Aircraft Hangar  \n- 50: Damaged Building  \n- 51: Facility  \n\n#### 52-59: Altre infrastrutture  \n- 52: Construction Site  \n- 53: Vehicle Lot  \n- 54: Helipad  \n- 55: Storage Tank  \n- 56: Shipping container lot  \n- 57: Shipping Container  \n- 58: Pylon  \n- 59: Tower  ","metadata":{}},{"cell_type":"markdown","source":"Utilizziamo un processo di preprocessing per il dataset **xView** seguendo i passi presenti in questo [Notebook (Preprocessing)](https://www.kaggle.com/code/ollypowell/xview-dataset-to-yolo-and-coco-format). L'obiettivo è:\n\n1. **Pulizia e riformattazione**: Si parte dal dataset grezzo e lo si prepara per l'addestramento di modelli di intelligenza artificiale per il rilevamento di oggetti.\n2. **Suddivisione in chunck**: Poiché le immagini satellitari sono molto grandi, vengono suddivise in \"pezzi\" più piccoli (chunck) per facilitare l'elaborazione. I bounding box, che definiscono le posizioni degli oggetti nelle immagini, vengono ridimensionati in modo che corrispondano ai nuovi pezzi.\n3. **Ottimizzazione del formato**: Le immagini TIFF, che occupano molto spazio, vengono convertite in formato JPG, molto più leggero.\n4. **Personalizzazione**: È possibile scegliere la dimensione delle immagini finali e come suddividere i dati per l'addestramento.\n\nQuesto processo riduce significativamente la dimensione del dataset, rendendolo più gestibile, senza sacrificare le informazioni necessarie per l'addestramento di modelli come **YOLOv5**.\n","metadata":{}},{"cell_type":"markdown","source":"Dopo aver analizzato le considerazioni riportate durante l'analisi esplorativa del dataset nel seguente [Notebook (Exploratory Data Analysis)](https://www.kaggle.com/code/ollypowell/xview-1-dataset-eda?scriptVersionId=157247786), abbiamo deciso di apportare alcune modifiche al preprocessing originale descritto nel documento linkato.\n\nIn particolare, abbiamo osservato che circa il 41% delle porzioni (chunk) di immagini con dimensioni comprese tra 600x600 e 640x640 risultavano prive di label. Questo è un problema, poiché il dataset originale è stato costruito raccogliendo immagini contenenti almeno una label. Per migliorare la qualità del dataset, abbiamo scelto di suddividere le immagini originali in chunk più piccoli, di dimensioni 320x320, ed eliminare il 66% delle immagini prive di label. Questo approccio ci ha permesso di ottenere un dataset composto per circa il 70% da immagini contenenti oggetti da identificare e per il 30% da immagini vuote. L'obiettivo è ottenere un training della rete più coerente ed efficace.\n\nInoltre, abbiamo corretto alcuni errori nel preprocessing originale, che impedivano di escludere i chunk con dimensioni inferiori a una certa soglia, errori nei path della working space ed altri.\n\nLa scelta di utilizzare chunk più piccoli si basa sulla considerazione che suddividere le immagini in parti di dimensioni ridotte consente di massimizzare la probabilità di includere tutte le label presenti nel dataset originale. Inoltre, ciò facilita la visualizzazione degli oggetti da individuare, sia per l'algoritmo che per un osservatore umano.\n\nAbbiamo infine deciso di escludere i chunk con dimensioni comprese nell'intervallo [300x300, 320x320), poiché rappresentavano una quantità trascurabile rispetto al totale. Gestirli avrebbe richiesto ulteriori scelte progettuali, come l'applicazione di tecniche di stretching sia sulle immagini sia sulle coordinate geometriche delle label, con un impatto complessivo marginale sull'efficacia del modello.\n\nNel seguente codice sono inoltre presenti molte operazioni di debug e di check per evitare che errori nella fase di preprocessing si ripercuotano sul successivo training.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"#Data sources\nDATA_FLDR_NM = 'Data'\nIN_DATASET_NM = 'xview-dataset'\nIMAGE_FLDR_NM = 'train_images'\nIN_LABELS_FLDR_NM = 'train_labels'\nLABELS_XML_NM = 'xView_train.geojson'\n\n#Output folders and file names\nOUT_DATASET_NM = 'xview-yolo-dataset'\nCLASS_MAP_JSON_NM = 'xView_class_map.json'\nOUT_COCO_JSON_NM = 'COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nOUT_DATAFRAME_NM = 'xview_labels.parquet'\nYAML_NM = 'xview_yolo.yaml'\nCHUNK_WIDTH = 320  # width of the images being created\nCHUNK_HEIGHT = 320\nMIN_CHUNK_HEIGHT = 320 # no images will be kept if the image chunk is smaller than this\nMIN_CHUNK_WIDTH = 320\nIMAGE_WRITING = True #True to re-perform image cropping, False just to regenerated other data\nTEST_FRACTION = 0.1\nJPEG_COMPRESSION = 95 # For the saved files\nVAL_FRACTION = 0.1\nRANDOM_SEED = 2023\nDEBUG = False\n\nin_dataset_pth = Path('/kaggle/input/xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nfuture_ds_img_fldr = Path(f'/kaggle/working/{OUT_IMAGE_FLDR_NM}')\nfuture_ds_cfg_fldr = Path(f'/kaggle/working/{OUT_CFG_FLDR_NM}')\n\nlabels_json_pth = in_dataset_pth / IN_LABELS_FLDR_NM / LABELS_XML_NM\nimg_fldr_pth = in_dataset_pth / IMAGE_FLDR_NM / IMAGE_FLDR_NM\nsave_images_fldr_pth = out_dataset_pth / OUT_IMAGE_FLDR_NM \nout_data_parquet_pth = out_dataset_pth / OUT_DATAFRAME_NM\nout_json_map_pth = out_dataset_pth / CLASS_MAP_JSON_NM \nclass_map_pth = out_dataset_pth / CLASS_MAP_JSON_NM\ncfg_fldr_pth = out_dataset_pth / OUT_CFG_FLDR_NM\ncoco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nyolo_yaml_pth = cfg_fldr_pth / YAML_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\ndef make_empty_dir(directory):\n    if directory.is_dir():\n        shutil.rmtree(directory)\n    os.makedirs(directory)\n\nmake_empty_dir(cfg_fldr_pth)\nif IMAGE_WRITING:\n    make_empty_dir(save_images_fldr_pth)\n\nrandom.seed(RANDOM_SEED)\n\nprint(f'The input images are found at {cfg_fldr_pth}')\nprint(f'The input labels are found at  {labels_json_pth}')\nprint(f'Configuration files will be saved to {cfg_fldr_pth}')\nprint(f'YOLO image files will be saved to {save_images_fldr_pth}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:13.030559Z","iopub.execute_input":"2024-11-22T13:40:13.030921Z","iopub.status.idle":"2024-11-22T13:40:13.040324Z","shell.execute_reply.started":"2024-11-22T13:40:13.030896Z","shell.execute_reply":"2024-11-22T13:40:13.039496Z"}},"outputs":[{"name":"stdout","text":"The input images are found at /kaggle/working/YOLO_cfg\nThe input labels are found at  /kaggle/input/xview-dataset/train_labels/xView_train.geojson\nConfiguration files will be saved to /kaggle/working/YOLO_cfg\nYOLO image files will be saved to /kaggle/working/images\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"# Estrae i bounding box da un DataFrame, eventualmente filtrandoli per una lista di classi\ndef get_boxes(in_df, class_lst=[]):\n    if class_lst:\n        in_df = in_df[in_df['TYPE_ID'].isin(class_lst)]     # Filtra il DataFrame per le classi specificate\n    unique_images = in_df.IMAGE_ID.unique().tolist()        # Ottiene una lista unica di ID immagine\n    boxs = {}\n\n    for image in tqdm_notebook(unique_images):\n        mask = in_df['IMAGE_ID'] == image                                  # Seleziona solo le righe relative a una specifica immagine\n        masked = in_df[mask][['TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX']]  # Estrae solo le colonne richieste\n        boxs[image] = masked.values.tolist()                               # Converte i valori in lista e li salva nel dizionario\n    return boxs\n\n\n# Stampa le prime n righe di un file di testo\ndef print_first_n_lines(file_path, n):\n    try:\n        with open(file_path, 'r') as file:\n            for line_num, line in enumerate(file, 1):\n                if line_num > n:                  \n                    break\n                print(line.strip()) \n    except FileNotFoundError:\n        print('Unable to open file')        \n\n\n# Carica un'immagine per la visualizzazione (convertita da BGR a RGB)\ndef load_image(file_pth):\n    image_obj = cv2.imread(file_pth)                        \n    image_obj = cv2.cvtColor(image_obj, cv2.COLOR_BGR2RGB)  \n    return image_obj\n\n\n# Carica un'immagine per la sola elaborazione (BGR)\ndef load_bgr_image(file_pth):\n    image_obj = cv2.imread(file_pth)  \n    return image_obj\n\n\n# Mostra una lista di immagini con i bounding box disegnati sopra\ndef display_images(image_lst, boxes_dictionary, image_fldr, max_images=6, no_cols=1, text=False, class_map={}):\n    total_ims = len(image_lst)\n    display_ims = min(max_images, total_ims)\n    no_rows = display_ims // no_cols + (display_ims % no_cols > 0)                   # Calcola il numero di righe necessarie\n    fig, axs = plt.subplots(no_rows, no_cols, figsize=(10, 10*no_rows/no_cols*3/4))  # Crea il layout della griglia\n    axs = axs.flatten()                                                              # Appiattisce l'array degli assi\n\n    for k, img_nm in enumerate(image_lst[:display_ims]):\n        image_path = str(image_fldr / img_nm)  \n        img = load_image(image_path)  \n\n        # Disegna i bounding box sull'immagine\n        if img_nm in boxes_dictionary:\n            for box in boxes_dictionary[img_nm]:\n                box_id, x_min, y_min, x_max, y_max = box\n                x_min, y_min, x_max, y_max = int(x_min), int(y_max), int(x_max), int(y_min)\n                cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0,255,0), 3)      # Disegna il rettangolo\n                if text:\n                    if class_map:\n                        box_label = class_map[box_id]                                 # Mappa la classe\n                    else:\n                        box_label = str(box_id)\n                    cv2.putText(img, box_label, (x_min, y_max-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (36,255,12), 4)  # Scrive il testo\n\n        axs[k].set_title(f\"Image {img_nm}\", fontsize=12)\n        axs[k].imshow(img)\n        axs[k].set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n    return\n\n\n# Converte le coordinate YOLO in coordinate per OpenCV ((left, top), (right, bottom))\ndef get_corners(x_cen, y_cen, an_width, an_height, im_width, im_height):\n    x_cen, y_cen, an_width, an_height = float(x_cen), float(y_cen), float(an_width), float(an_height)\n    # Calcolo dei margini\n    left = (x_cen - an_width/2)*im_width            \n    top = (y_cen - an_height/2)*im_height           \n    right = (x_cen + an_width/2)*im_width           \n    bottom = (y_cen + an_height/2)*im_height        \n    return int(left), int(top), int(right), int(bottom)\n\n\n# Mostra immagini con bounding box definiti in formato YOLO\ndef display_yolo_images(image_lst, image_fldr, max_images=6, no_cols=1, text=False, class_map={}):\n    total_ims = len(image_lst)\n    display_ims = min(max_images, total_ims)\n    no_rows = display_ims // no_cols + (display_ims % no_cols > 0)\n    _, axs = plt.subplots(no_rows, no_cols, figsize=(10, 10*no_rows/no_cols*3/4))\n    axs = axs.flatten()\n\n    for k, img_nm in enumerate(image_lst[:display_ims]):\n        image_path = image_fldr / img_nm\n        text_fn = image_path.stem + '.txt'     # Costruisce il nome del file dei bounding box\n        boxes_path = image_fldr / text_fn\n        img = load_image(str(image_path))  \n        im_h, im_w, _ = img.shape  \n        with open(boxes_path) as text_file:\n            annotations = [line.rstrip().split() for line in text_file]  \n\n        # Disegna i bounding box\n        for ann in annotations:\n            class_id = ann[0]\n            x_centre, y_centre, w, h = ann[1], ann[2], ann[3], ann[4]\n            x_min, y_min, x_max, y_max = get_corners(x_centre, y_centre, w, h, im_w, im_h)\n            cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0,255,0), 3)\n            if text:\n                if class_map:\n                    box_label = class_map[int(class_id)]\n                else:\n                    box_label = str(class_id)\n                cv2.putText(img, box_label, (x_min, y_max-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (36,255,12), 4)\n\n        axs[k].set_title(f\"Image {img_nm}\", fontsize=12)\n        axs[k].imshow(img)\n        axs[k].set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n    return\n\n\n# Trova i bounding box contenuti in una sezione dell'immagine e li restituisce in formato YOLO\ndef match_boxes(box_list, chnk_lims):\n    boxes_lists = []\n    le, to = chnk_lims[0], chnk_lims[1]                    # Limiti del chunk (left, top)\n    w, h = chnk_lims[2], chnk_lims[3]                      # Larghezza e altezza del chunk\n    for box in box_list:\n        o_left, o_top, o_right, o_bottom = box[1], box[2], box[3], box[4]\n        left, right = (o_left - le)/w, (o_right - le)/w    # Normalizza rispetto ai limiti del chunk\n        top, bottom = (o_top - to)/h, (o_bottom - to)/h\n\n        # Verifica se il bounding box è contenuto nella sezione\n        h_match = (0 <= left < 1) or (0 < right <= 1)\n        v_match = (0 <= top < 1) or (0 < bottom <= 1)\n\n        if v_match and h_match:\n            clipped = np.clip([left, top, right, bottom], a_min=0, a_max=1)  # Clippa i valori tra 0 e 1\n            l, t, r, b = clipped[0], clipped[1], clipped[2], clipped[3]\n            bounding_box = [str(box[0]),\n                            str(round((l + r)/2, 5)),\n                            str(round((t + b)/2, 5)),\n                            str(round(r - l, 5)),\n                            str(round(b - t, 5))]       # Formatta in YOLO\n            boxes_lists.append(bounding_box)\n    return boxes_lists","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:13.042093Z","iopub.execute_input":"2024-11-22T13:40:13.042341Z","iopub.status.idle":"2024-11-22T13:40:13.064449Z","shell.execute_reply.started":"2024-11-22T13:40:13.042316Z","shell.execute_reply":"2024-11-22T13:40:13.063653Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Data Extraction and Cleaning","metadata":{}},{"cell_type":"code","source":"# Apre un file JSON contenente annotazioni e carica i dati\nwith open(labels_json_pth, 'r') as infile:\n    data = json.load(infile)                        # Carica il file JSON in un dizionario\n    keys = list(data.keys())                        # Estrazioni delle chiavi del dizionario\n\n# Estrae la lista delle annotazioni (features) dal JSON\nfeature_list = data['features']\n\n# Definisce le colonne per il DataFrame\nCOLUMNS = ['IMAGE_ID', 'TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX', 'LONG', 'LAT']\n\ndata = []\nfor feature in tqdm_notebook(feature_list):  \n    properties = feature['properties']               # Estrae le proprietà del feature (dizionario)\n    img_id = properties['image_id']                  # Identificativo immagine\n    type_id = properties['type_id']                  # Tipo di oggetto annotato\n    bbox = properties['bounds_imcoords'].split(\",\")  # Coordinate del bounding box\n\n    geometry = feature['geometry']  \n    coords = geometry['coordinates'][0]              # Lista di coordinate (lista di liste)\n\n    # Calcola il punto centrale geografico \n    long = coords[0][0] / 2 + coords[2][0] / 2       # Media delle longitudini dei vertici opposti\n    lat = coords[0][1] / 2 + coords[1][1] / 2        # Media delle latitudini dei vertici opposti\n\n    # Crea una riga con tutte le informazioni rilevanti\n    one_row = [img_id, type_id, bbox[0], bbox[1], bbox[2], bbox[3], long, lat]\n    data.append(one_row)  \n\n# Conta il numero di istanze totali nel dataset\ninstances = len(data)\nprint(f'Ci sono {instances} istanze degli ogetti nel dataset originale')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:13.065225Z","iopub.execute_input":"2024-11-22T13:40:13.065487Z","iopub.status.idle":"2024-11-22T13:40:29.262897Z","shell.execute_reply.started":"2024-11-22T13:40:13.065463Z","shell.execute_reply":"2024-11-22T13:40:29.262026Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/601937 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d94e7b89ed74c3ab42d7f775ff3de67"}},"metadata":{}},{"name":"stdout","text":"Ci sono 601937 istanze degli ogetti nel dataset originale\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Estrazione delle colonne di interesse:","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data, columns = COLUMNS)\ndf[['XMIN', 'YMIN', 'XMAX', 'YMAX']] = df[['XMIN', 'YMIN', 'XMAX', 'YMAX']].apply(pd.to_numeric)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:29.264165Z","iopub.execute_input":"2024-11-22T13:40:29.264858Z","iopub.status.idle":"2024-11-22T13:40:31.159031Z","shell.execute_reply.started":"2024-11-22T13:40:29.264796Z","shell.execute_reply":"2024-11-22T13:40:31.158096Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   IMAGE_ID  TYPE_ID  XMIN  YMIN  XMAX  YMAX       LONG        LAT\n0  2355.tif       73  2712  1145  2746  1177 -90.531640  14.566091\n1  2355.tif       73  2720  2233  2760  2288 -90.531603  14.562313\n2  2355.tif       73  2687  1338  2740  1399 -90.531694  14.565379\n3  2355.tif       73  2691  1201  2730  1268 -90.531704  14.565838\n4  2355.tif       73  2671   838  2714   869 -90.531766  14.567149","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IMAGE_ID</th>\n      <th>TYPE_ID</th>\n      <th>XMIN</th>\n      <th>YMIN</th>\n      <th>XMAX</th>\n      <th>YMAX</th>\n      <th>LONG</th>\n      <th>LAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2712</td>\n      <td>1145</td>\n      <td>2746</td>\n      <td>1177</td>\n      <td>-90.531640</td>\n      <td>14.566091</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2720</td>\n      <td>2233</td>\n      <td>2760</td>\n      <td>2288</td>\n      <td>-90.531603</td>\n      <td>14.562313</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2687</td>\n      <td>1338</td>\n      <td>2740</td>\n      <td>1399</td>\n      <td>-90.531694</td>\n      <td>14.565379</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2691</td>\n      <td>1201</td>\n      <td>2730</td>\n      <td>1268</td>\n      <td>-90.531704</td>\n      <td>14.565838</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2671</td>\n      <td>838</td>\n      <td>2714</td>\n      <td>869</td>\n      <td>-90.531766</td>\n      <td>14.567149</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"Rimozione di 2 annotation errate per le class labels ([EDA](https://www.kaggle.com/code/ollypowell/xview-dataset-eda))","metadata":{}},{"cell_type":"code","source":"df = df[(df.TYPE_ID != 75) & (df.TYPE_ID != 82)]   # removing erroneous labels\nprint(f'{instances - len(df)} rows removed, leaving {len(df)} rows')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:31.160189Z","iopub.execute_input":"2024-11-22T13:40:31.160895Z","iopub.status.idle":"2024-11-22T13:40:31.228316Z","shell.execute_reply.started":"2024-11-22T13:40:31.160851Z","shell.execute_reply":"2024-11-22T13:40:31.227415Z"}},"outputs":[{"name":"stdout","text":"79 rows removed, leaving 601858 rows\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:31.229284Z","iopub.execute_input":"2024-11-22T13:40:31.229564Z","iopub.status.idle":"2024-11-22T13:40:31.239611Z","shell.execute_reply.started":"2024-11-22T13:40:31.229538Z","shell.execute_reply":"2024-11-22T13:40:31.238737Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   IMAGE_ID  TYPE_ID  XMIN  YMIN  XMAX  YMAX       LONG        LAT\n0  2355.tif       73  2712  1145  2746  1177 -90.531640  14.566091\n1  2355.tif       73  2720  2233  2760  2288 -90.531603  14.562313\n2  2355.tif       73  2687  1338  2740  1399 -90.531694  14.565379\n3  2355.tif       73  2691  1201  2730  1268 -90.531704  14.565838\n4  2355.tif       73  2671   838  2714   869 -90.531766  14.567149","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IMAGE_ID</th>\n      <th>TYPE_ID</th>\n      <th>XMIN</th>\n      <th>YMIN</th>\n      <th>XMAX</th>\n      <th>YMAX</th>\n      <th>LONG</th>\n      <th>LAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2712</td>\n      <td>1145</td>\n      <td>2746</td>\n      <td>1177</td>\n      <td>-90.531640</td>\n      <td>14.566091</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2720</td>\n      <td>2233</td>\n      <td>2760</td>\n      <td>2288</td>\n      <td>-90.531603</td>\n      <td>14.562313</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2687</td>\n      <td>1338</td>\n      <td>2740</td>\n      <td>1399</td>\n      <td>-90.531694</td>\n      <td>14.565379</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2691</td>\n      <td>1201</td>\n      <td>2730</td>\n      <td>1268</td>\n      <td>-90.531704</td>\n      <td>14.565838</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2671</td>\n      <td>838</td>\n      <td>2714</td>\n      <td>869</td>\n      <td>-90.531766</td>\n      <td>14.567149</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Rimozione dell'immagine 1395, poiché inesistente","metadata":{}},{"cell_type":"code","source":"old_length = len(df)\ndf = df[df.IMAGE_ID != '1395.tif']\nprint(f'{old_length - len(df)} rows removed, leaving {len(df)}')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:31.240683Z","iopub.execute_input":"2024-11-22T13:40:31.240970Z","iopub.status.idle":"2024-11-22T13:40:31.388630Z","shell.execute_reply.started":"2024-11-22T13:40:31.240945Z","shell.execute_reply":"2024-11-22T13:40:31.387765Z"}},"outputs":[{"name":"stdout","text":"131 rows removed, leaving 601727\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   IMAGE_ID  TYPE_ID  XMIN  YMIN  XMAX  YMAX       LONG        LAT\n0  2355.tif       73  2712  1145  2746  1177 -90.531640  14.566091\n1  2355.tif       73  2720  2233  2760  2288 -90.531603  14.562313\n2  2355.tif       73  2687  1338  2740  1399 -90.531694  14.565379\n3  2355.tif       73  2691  1201  2730  1268 -90.531704  14.565838\n4  2355.tif       73  2671   838  2714   869 -90.531766  14.567149","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IMAGE_ID</th>\n      <th>TYPE_ID</th>\n      <th>XMIN</th>\n      <th>YMIN</th>\n      <th>XMAX</th>\n      <th>YMAX</th>\n      <th>LONG</th>\n      <th>LAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2712</td>\n      <td>1145</td>\n      <td>2746</td>\n      <td>1177</td>\n      <td>-90.531640</td>\n      <td>14.566091</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2720</td>\n      <td>2233</td>\n      <td>2760</td>\n      <td>2288</td>\n      <td>-90.531603</td>\n      <td>14.562313</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2687</td>\n      <td>1338</td>\n      <td>2740</td>\n      <td>1399</td>\n      <td>-90.531694</td>\n      <td>14.565379</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2691</td>\n      <td>1201</td>\n      <td>2730</td>\n      <td>1268</td>\n      <td>-90.531704</td>\n      <td>14.565838</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2355.tif</td>\n      <td>73</td>\n      <td>2671</td>\n      <td>838</td>\n      <td>2714</td>\n      <td>869</td>\n      <td>-90.531766</td>\n      <td>14.567149</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Inoltre, è utile convertire gli ID dei tipi in una sequenza continua da 0 a 59 per le 60 categorie. Le label originali della competizione non erano organizzate in questo modo. Il dizionario riportato di seguito rappresenta la mappatura originale:","metadata":{}},{"cell_type":"code","source":"old_dict = {\n    11:'Fixed-wing Aircraft', 12:'Small Aircraft', 13:'Passenger/Cargo Plane', 15:'Helicopter',\n    17:'Passenger Vehicle', 18:'Small Car', 19:'Bus', 20:'Pickup Truck', 21:'Utility Truck',\n    23:'Truck', 24:'Cargo Truck', 25:'Truck Tractor w/ Box Trailer', 26:'Truck Tractor',27:'Trailer',\n    28:'Truck Tractor w/ Flatbed Trailer', 29:'Truck Tractor w/ Liquid Tank', 32:'Crane Truck',\n    33:'Railway Vehicle', 34:'Passenger Car', 35:'Cargo/Container Car', 36:'Flat Car', 37:'Tank car',\n    38:'Locomotive', 40:'Maritime Vessel', 41:'Motorboat', 42:'Sailboat', 44:'Tugboat', 45:'Barge',\n    47:'Fishing Vessel', 49:'Ferry', 50:'Yacht', 51:'Container Ship', 52:'Oil Tanker',\n    53:'Engineering Vehicle', 54:'Tower crane', 55:'Container Crane', 56:'Reach Stacker',\n    57:'Straddle Carrier', 59:'Mobile Crane', 60:'Dump Truck', 61:'Haul Truck', 62:'Scraper/Tractor',\n    63:'Front loader/Bulldozer', 64:'Excavator', 65:'Cement Mixer', 66:'Ground Grader', 71:'Hut/Tent',\n    72:'Shed', 73:'Building', 74:'Aircraft Hangar', 76:'Damaged Building', 77:'Facility', 79:'Construction Site',\n    83:'Vehicle Lot', 84:'Helipad', 86:'Storage Tank', 89:'Shipping container lot', 91:'Shipping Container',\n    93:'Pylon', 94:'Tower'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:31.391440Z","iopub.execute_input":"2024-11-22T13:40:31.391679Z","iopub.status.idle":"2024-11-22T13:40:31.397939Z","shell.execute_reply.started":"2024-11-22T13:40:31.391657Z","shell.execute_reply":"2024-11-22T13:40:31.397011Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Di seguito si aggiorna la mappatura delle label","metadata":{}},{"cell_type":"code","source":"old_keys = sorted(list(old_dict.keys()))\nnew_dict = {old_dict[x]:y for y, x in enumerate(old_keys)}\nclass_map_dict = {y:old_dict[x] for y, x in enumerate(old_keys)}\nwith open(out_json_map_pth, \"w\") as json_file:\n    json.dump(class_map_dict, json_file)\nprint(class_map_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:31.399164Z","iopub.execute_input":"2024-11-22T13:40:31.399521Z","iopub.status.idle":"2024-11-22T13:40:31.414873Z","shell.execute_reply.started":"2024-11-22T13:40:31.399485Z","shell.execute_reply":"2024-11-22T13:40:31.414110Z"}},"outputs":[{"name":"stdout","text":"{0: 'Fixed-wing Aircraft', 1: 'Small Aircraft', 2: 'Passenger/Cargo Plane', 3: 'Helicopter', 4: 'Passenger Vehicle', 5: 'Small Car', 6: 'Bus', 7: 'Pickup Truck', 8: 'Utility Truck', 9: 'Truck', 10: 'Cargo Truck', 11: 'Truck Tractor w/ Box Trailer', 12: 'Truck Tractor', 13: 'Trailer', 14: 'Truck Tractor w/ Flatbed Trailer', 15: 'Truck Tractor w/ Liquid Tank', 16: 'Crane Truck', 17: 'Railway Vehicle', 18: 'Passenger Car', 19: 'Cargo/Container Car', 20: 'Flat Car', 21: 'Tank car', 22: 'Locomotive', 23: 'Maritime Vessel', 24: 'Motorboat', 25: 'Sailboat', 26: 'Tugboat', 27: 'Barge', 28: 'Fishing Vessel', 29: 'Ferry', 30: 'Yacht', 31: 'Container Ship', 32: 'Oil Tanker', 33: 'Engineering Vehicle', 34: 'Tower crane', 35: 'Container Crane', 36: 'Reach Stacker', 37: 'Straddle Carrier', 38: 'Mobile Crane', 39: 'Dump Truck', 40: 'Haul Truck', 41: 'Scraper/Tractor', 42: 'Front loader/Bulldozer', 43: 'Excavator', 44: 'Cement Mixer', 45: 'Ground Grader', 46: 'Hut/Tent', 47: 'Shed', 48: 'Building', 49: 'Aircraft Hangar', 50: 'Damaged Building', 51: 'Facility', 52: 'Construction Site', 53: 'Vehicle Lot', 54: 'Helipad', 55: 'Storage Tank', 56: 'Shipping container lot', 57: 'Shipping Container', 58: 'Pylon', 59: 'Tower'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Ora possiamo convertire i vecchi **TYPE_ID** del dataframe nei loro nuovi valori","metadata":{}},{"cell_type":"code","source":"df['TYPE_ID'] = df['TYPE_ID'].apply(lambda x: new_dict[old_dict[x]])\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:31.415711Z","iopub.execute_input":"2024-11-22T13:40:31.415965Z","iopub.status.idle":"2024-11-22T13:40:31.633896Z","shell.execute_reply.started":"2024-11-22T13:40:31.415941Z","shell.execute_reply":"2024-11-22T13:40:31.633060Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   IMAGE_ID  TYPE_ID  XMIN  YMIN  XMAX  YMAX       LONG        LAT\n0  2355.tif       48  2712  1145  2746  1177 -90.531640  14.566091\n1  2355.tif       48  2720  2233  2760  2288 -90.531603  14.562313\n2  2355.tif       48  2687  1338  2740  1399 -90.531694  14.565379","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IMAGE_ID</th>\n      <th>TYPE_ID</th>\n      <th>XMIN</th>\n      <th>YMIN</th>\n      <th>XMAX</th>\n      <th>YMAX</th>\n      <th>LONG</th>\n      <th>LAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2355.tif</td>\n      <td>48</td>\n      <td>2712</td>\n      <td>1145</td>\n      <td>2746</td>\n      <td>1177</td>\n      <td>-90.531640</td>\n      <td>14.566091</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2355.tif</td>\n      <td>48</td>\n      <td>2720</td>\n      <td>2233</td>\n      <td>2760</td>\n      <td>2288</td>\n      <td>-90.531603</td>\n      <td>14.562313</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2355.tif</td>\n      <td>48</td>\n      <td>2687</td>\n      <td>1338</td>\n      <td>2740</td>\n      <td>1399</td>\n      <td>-90.531694</td>\n      <td>14.565379</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Per verificare che i dati vengano caricati correttamente, visualizziamo alcune immagini con eventuali annotazioni relative ai trasporti","metadata":{}},{"cell_type":"code","source":"all_classes = list(class_map_dict.keys())\ntransport_only = [x for x in all_classes if x < 48]\n\nboxes = get_boxes(df, transport_only)\nimages_for_display = random.choices(list(boxes.keys()), k=2)\ndisplay_images(images_for_display, boxes, img_fldr_pth, max_images=2, no_cols=2, text=True, class_map=class_map_dict) #adjust as desired","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:40:31.635020Z","iopub.execute_input":"2024-11-22T13:40:31.635372Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/786 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e365f2259c1646b2b1267e6bcbe9e1bb"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"## Chunk Processing\n- Suddividere i file TIFF di grandi dimensioni in chunk  \n- Salvare questi chunk come file JPG  \n- Verificare se i chunk contengono annotazioni  \n- Riformattare le annotazioni nel formato YOLO: x_center, y_center, width, height (tutto normalizzato)  \n- Scrivere tutte le annotazioni in formato YOLO in un dizionario, utilizzando il nome del file come chiave  ","metadata":{}},{"cell_type":"code","source":"boxes_dict = get_boxes(df)  # Dizionario: {filename:[['TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX'],[..],[..],..]}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_image(img_fn, \n                  dir_pth=img_fldr_pth, \n                  boxes=boxes_dict, \n                  out_dir=save_images_fldr_pth, \n                  c_height=CHUNK_HEIGHT, \n                  c_width=CHUNK_WIDTH,  \n                  jpg_q=JPEG_COMPRESSION,\n                  min_h=MIN_CHUNK_HEIGHT,\n                  min_w=MIN_CHUNK_WIDTH,\n                  writing=IMAGE_WRITING\n                 ):\n    \"\"\"\n    Suddivide un'immagine in piccoli chunk, salva i chunk in formato JPEG e converte le annotazioni in formato YOLO.\n\n    Args:\n        img_fn (str): Nome del file immagine da processare.\n        dir_pth (Path): Percorso della directory contenente le immagini di input.\n        boxes (dict): Dizionario che associa ogni immagine a una lista di annotazioni.\n        out_dir (Path): Percorso della directory dove salvare i chunk generati.\n        c_height (int): Altezza dei chunk da creare.\n        c_width (int): Larghezza dei chunk da creare.\n        jpg_q (int): Qualità di compressione JPEG (valore da 0 a 100).\n        min_h (int): Altezza minima per considerare un chunk valido.\n        min_w (int): Larghezza minima per considerare un chunk valido.\n        writing (bool): Se True, salva i chunk generati come file JPEG.\n\n    Returns:\n        tuple: Una tupla contenente:\n            - f_names (list): Lista dei nomi dei file dei chunk generati.\n            - widths (list): Lista delle larghezze dei chunk generati.\n            - heights (list): Lista delle altezze dei chunk generati.\n            - y_boxes (dict): Dizionario che associa i file dei chunk alle annotazioni in formato YOLO.\n    \"\"\"\n    \n    labels_list = boxes[img_fn]\n    img_pth = str(dir_pth / img_fn)\n    im = load_bgr_image(img_pth)\n    full_h, full_w, _ = im.shape\n    y_boxes= {}\n    f_names, widths, heights = [], [], []\n    \n    for r in range(0, full_h, c_height):\n        for c in range(0, full_w, c_width):\n            stem = img_fn.split('.')[0]\n            fn = str(f\"img_{stem}_{r}_{c}.jpg\")\n            out_pth = str(out_dir / fn)\n            width = c_width\n            height = c_height\n            if r + height > full_h:\n                height = full_h - r\n            if c + width > full_w:\n                width = full_w - c\n            big_enough = (height >= min_h) and (width >= min_w)\n            if big_enough:\n                if writing:\n                    cv2.imwrite(out_pth, im[r:r+height, c:c+height,:],  [int(cv2.IMWRITE_JPEG_QUALITY), jpg_q])\n                # Find any boxes occurring in the chunk, and convert to YOLO format\n                chunk_limits = [c, r, width, height]\n                y_boxes[fn] = match_boxes(labels_list, chunk_limits)\n                f_names.append(fn)\n                widths.append(width)\n                heights.append(height)\n    return f_names, widths, heights, y_boxes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_fns = df.IMAGE_ID.unique().tolist()\nif DEBUG:\n    img_fns = img_fns[:len(img_fns)//120]\n    df = df[df['IMAGE_ID'].isin(img_fns)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nnum_threads = mp.cpu_count() \noverall_progress = tqdm_notebook(total=len(img_fns), desc=\"Creating and saving image tiles\")\nyolo_boxes= {}\nfile_names, widths, heights = [], [], []\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n    for f_names, c_widths, c_heights, y_boxes in executor.map(process_image, img_fns):\n        file_names.extend(f_names)\n        widths.extend(c_widths)\n        heights.extend(c_heights)\n        yolo_boxes.update(y_boxes)\n        overall_progress.update(1)\noverall_progress.close()\n\nimage_data = {file_names[i]: [widths[i], heights[i]] for i in range(len(file_names))}\ntime_taken=time.time() - start_time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YOLO text files\nIterare attraverso il dizionario, creando un file di testo per ogni immagine con il formato: **classe x y larghezza altezza**, quindi salvare il file di testo completato nella stessa posizione, con lo stesso nome base dell'immagine.","metadata":{}},{"cell_type":"code","source":"all_image_files = os.listdir(save_images_fldr_pth)\nfor image_fn in tqdm_notebook(all_image_files):\n    stem = image_fn.split('.')[0]\n    fn = str (stem) + '.txt'\n    txt_pth = str(save_images_fldr_pth / fn)\n    seperator = ' '\n    with open(txt_pth, 'a') as f:\n        if image_fn in yolo_boxes:\n            for bbox in yolo_boxes[image_fn]:\n                txt = seperator.join(bbox) + '\\n'\n                f.write(txt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_files = [file for file in os.listdir(save_images_fldr_pth) if file.endswith('.txt')]\nnum_txt_files = len(txt_files)\nprint(f\"Numero di file .txt: {num_txt_files}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_with_boxes = [image_fn for image_fn in all_image_files if image_fn in yolo_boxes]\nprint(f\"Numero di immagini con dati in yolo_boxes: {len(images_with_boxes)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_paths = [save_images_fldr_pth / x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".txt\")]\ncolumn_names = ['Class_ID', 'x_center', 'y_center', 'width', 'height']\ndata = []\nfor file_path in text_paths:\n    with open(file_path, 'r') as file:\n        for line in file:\n            values = line.strip().split(' ')\n            row_data = {col: val for col, val in zip(column_names, values)}\n            row_data['File_Name'] = file_path.name\n            data.append(row_data)\n\nout_df = pd.DataFrame(data)\nout_df['Class_ID']=out_df['Class_ID'].astype(int)\nout_df['Class_Name'] = out_df['Class_ID'].map(class_map_dict).fillna('unknown')\nout_df = out_df[['File_Name', 'Class_Name', 'Class_ID', 'x_center', 'y_center', 'width', 'height']]\nout_df.to_parquet(out_data_parquet_pth, index=False)\nout_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Rimozione del 66% delle immagini senza label","metadata":{}},{"cell_type":"code","source":"def remove_empty(image_folder, yolo_boxes, image_data):\n    \"\"\"\n    Rimuove il 66% dei file di annotazione vuoti (.txt) e le immagini corrispondenti.\n    Aggiorna anche i dizionari yolo_boxes e image_data per rimuovere le voci corrispondenti.\n    \n    Args:\n        image_folder (str): Percorso della cartella contenente immagini e file .txt.\n        yolo_boxes (dict): Dizionario con annotazioni YOLO.\n        image_data (dict): Dizionario con metadati delle immagini.\n        file_names (list): Lista di nomi file immagine.\n        widths (list): Lista di larghezze delle immagini.\n        heights (list): Lista di altezze delle immagini.\n    \n    Returns:\n        tuple: Dizionari aggiornati (yolo_boxes, image_data, file_names, widths, heights).\n    \"\"\"\n    all_image_files = set(os.listdir(image_folder))  # Set per confronti più veloci\n    empty_files = [] \n\n    # Controlla i file .txt per annotazioni vuote\n    for txt_file in all_image_files:\n        if txt_file.endswith('.txt'):\n            txt_path = os.path.join(image_folder, txt_file)\n            # Controlla se il file è vuoto o contiene solo spazi\n            with open(txt_path, 'r') as file:\n                content = file.read().strip()\n            if not content:  \n                # Determina il file immagine corrispondente\n                image_file = txt_file.replace('.txt', '.jpg')  \n                empty_files.append(image_file)  # Aggiungi immagine alla lista dei file vuoti\n\n    # Seleziona il 66% dei file vuoti da rimuovere\n    num_to_remove = int(len(empty_files) * 0.66)\n    files_to_remove = random.sample(empty_files, num_to_remove)\n\n    # Rimuovi i file .txt e immagini corrispondenti\n    for image_file in files_to_remove:\n        txt_file = image_file.replace('.jpg', '.txt')  \n        txt_path = os.path.join(image_folder, txt_file)\n        image_path = os.path.join(image_folder, image_file)\n\n        if os.path.exists(txt_path):\n            os.remove(txt_path)\n        if os.path.exists(image_path):\n            os.remove(image_path)\n\n    # Aggiorna yolo_boxes eliminando i file rimossi\n    yolo_boxes = {key: value for key, value in yolo_boxes.items() if key not in files_to_remove}\n\n    # Aggiorna image_data eliminando i file rimossi\n    image_data = {key: value for key, value in image_data.items() if key not in files_to_remove}\n\n    # Filtra le liste per escludere i file da rimuovere\n    filtered_file_names = [name for name in file_names if name not in files_to_remove]\n    filtered_widths = [widths[i] for i in range(len(file_names)) if file_names[i] not in files_to_remove]\n    filtered_heights = [heights[i] for i in range(len(file_names)) if file_names[i] not in files_to_remove]\n\n    return yolo_boxes, image_data, filtered_file_names, filtered_widths, filtered_heights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo_boxes, image_data, filtered_file_names, filtered_widths, filtered_heights = remove_empty(save_images_fldr_pth, yolo_boxes, image_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verifica il numero di file .txt nel folder\ntxt_files = [f for f in os.listdir(save_images_fldr_pth) if f.endswith('.txt')]\nprint(f\"Numero di file .txt nel folder: {len(txt_files)}\")\n# Conta il numero di file immagine (per esempio file .jpg)\nimage_files = [f for f in os.listdir(save_images_fldr_pth) if f.endswith('.jpg')]\nnum_images = len(image_files)\nprint(f\"Numero di immagini nel folder: {num_images}\")\nimages_with_boxes = [image_fn for image_fn in all_image_files if image_fn in yolo_boxes]\nprint(f\"Numero di immagini con dati in yolo_boxes: {len(images_with_boxes)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import islice\n\n# Stampa i primi 5 elementi\nfor key, value in islice(image_data.items(), 5):\n    print(f\"{key}: {value}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_file_names = len(image_data) # Il numero di chiavi corrisponde al numero di file_names (per ora)\nnum_widths = len([value[0] for value in image_data.values()])  # Conta tutte le larghezze\nnum_heights = len([value[1] for value in image_data.values()])  # Conta tutte le altezze\n\nprint(f\"Il numero di immagini uniche è: {num_file_names}\")\nprint(f\"Il numero di larghezze registrate è: {num_widths}\")\nprint(f\"Il numero di altezze registrate è: {num_heights}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filenames = [x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".jpg\")]\nimage_list = random.choices(filenames, k=4)\ndisplay_yolo_images(image_list, save_images_fldr_pth, max_images=4, no_cols=2, text=True,  class_map=class_map_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#without text labels\nfilenames = [x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".jpg\")]\nimage_list = random.choices(filenames, k=4)\ndisplay_yolo_images(image_list, save_images_fldr_pth, max_images=4, no_cols=2, text=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting","metadata":{}},{"cell_type":"code","source":"total_images = len(filenames)\nindices = list(range(total_images))\nrandom.shuffle(indices)\n\ntrain_fraction = 1 - TEST_FRACTION - VAL_FRACTION\ntrain_sp = int(np.floor(train_fraction * len(indices))) # The training-validation split\nvalid_sp = int(np.floor(VAL_FRACTION * len(indices))) + train_sp # The validation-test split\ntrain_idx, val_idx, test_idx = indices[:train_sp], indices[train_sp:valid_sp], indices[valid_sp:]\n\nprint(' Training set size: \\t', len(train_idx))\nprint(' Validation set size: \\t', len(val_idx))\nprint(' Test set size: \\t', len(test_idx))\nprint(' Total dataset: \\t', total_images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Write 3 text files into the Data folder with the file paths: train.txt, val.txt, test.txt  These are lists of absolute filepaths to the images, one line each path.  They can reside anywhere just so long as the relative paths in xview_yolo.yaml points to them.","metadata":{}},{"cell_type":"code","source":"files = ['train.txt', 'val.txt', 'test.txt']\nsplits = [train_idx, val_idx, test_idx]\n\nfor fn, split in zip(files, splits):\n    txt_pth = cfg_fldr_pth / fn\n    with open(txt_pth, 'a') as f:\n        for ind in split:\n            f.write(str(future_ds_img_fldr / filenames[ind]) + '\\n')\n        print(f'{fn[:-4]} file written to {txt_pth}, with {len(split) } samples')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YAML File\nWrite a .yaml file pointing to the text file locations, and determining class names, number of categories location.\nThis is good practice, it means I don't need to move all the image files around just to change the training splits.\nAlso the .yml file gets updated automatically if anybody changes something like the number of classes.","metadata":{}},{"cell_type":"code","source":"config = {'train': str(future_ds_cfg_fldr / files[0]),\n          'val': str(future_ds_cfg_fldr / files[1]),\n          'test': str(future_ds_cfg_fldr / files[2]),\n          'nc': len(class_map_dict),\n          'names': class_map_dict\n          }\n\nwith open(yolo_yaml_pth, \"w\") as file:\n    yaml.dump(config, file, default_style=None, default_flow_style=False, sort_keys=False)\nprint(f'yaml file written to {yolo_yaml_pth}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting Check\nJust checking the first few lines of the train.txt file","metadata":{}},{"cell_type":"code","source":"for split in ['train', 'val', 'test']:\n    print(f'{split} text file')\n    print_first_n_lines(cfg_fldr_pth / f'{split}.txt', 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And the .yaml file","metadata":{}},{"cell_type":"code","source":"print_first_n_lines(yolo_yaml_pth, 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And a couple of annotations files","metadata":{}},{"cell_type":"code","source":"txt_fnames = [save_images_fldr_pth / x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".txt\")]\ntext_list = random.choices(txt_fnames, k=2)\nprint(text_list)\nfor text_f in text_list:\n    print(f'Reading {text_f}')\n    print_first_n_lines(text_f, 3)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(text_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And the csv file","metadata":{}},{"cell_type":"code","source":"out_data = pd.read_parquet(out_data_parquet_pth)\nout_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta il numero di immagini con nomi unici\nnum_immagini_uniche = out_data['File_Name'].nunique()\nprint(f\"Il numero di immagini uniche è: {num_immagini_uniche}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And the `.json`","metadata":{}},{"cell_type":"code","source":"with open(out_json_map_pth, \"r\") as json_file:\n    loaded_dict = json.load(json_file)\nprint(loaded_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YOLO to COCO\n\nPoiché non richiede molto sforzo in questo momento, riformatterò il dataframe nella geometria del formato COCO e scriverò un file `.json` in formato COCO per coloro che necessitano di tale formato. Il formato COCO è spiegato [qui](https://cocodataset.org/#format-data). A livello superiore, abbiamo principalmente bisogno di questi tre oggetti:\n\n- **images:**  \n  `{\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, }`  \n\n- **annotations:**  \n  `{\"id\": int, \"image_id\": int, \"category_id\": int, \"area\": float, \"bbox\": [x, y, width, height]}`  \n\n- **categories:**  \n  `[{\"id\": int, \"name\": str}]`","metadata":{}},{"cell_type":"markdown","source":"Copiamo il DataFrame YOLO, per estrarre le larghezze delle immagini e creare la categoria BBox","metadata":{}},{"cell_type":"code","source":"num_file_names = len(image_data) # Il numero di chiavi corrisponde al numero di file_names (per ora)\nnum_widths = len([value[0] for value in image_data.values()])  # Conta tutte le larghezze\nnum_heights = len([value[1] for value in image_data.values()])  # Conta tutte le altezze\n\nprint(f\"Il numero di immagini uniche è: {num_file_names}\")\nprint(f\"Il numero di larghezze registrate è: {num_widths}\")\nprint(f\"Il numero di altezze registrate è: {num_heights}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_data = {'width': filtered_widths, 'height': filtered_heights, 'file_name': filtered_file_names} # Ricreo il dizionario con 3 chiavi ma utilizzando le liste filtraten dopo l'eliminazione fatta prima\nim_df = pd.DataFrame(image_data)\nim_df['id'] = im_df['file_name'].str.replace(r'\\D', '', regex=True).astype(int)\nim_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_file_names = len(image_data['file_name'])  # Numero di file rimanenti\nnum_widths = len(image_data['width'])         # Numero di larghezze rimanenti\nnum_heights = len(image_data['height'])       # Numero di altezze rimanenti\n\nprint(f\"Il numero di immagini uniche è: {num_file_names}\")\nprint(f\"Il numero di larghezze registrate è: {num_widths}\")\nprint(f\"Il numero di altezze registrate è: {num_heights}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_images_in_im_df = len(im_df)\nprint(f\"Numero di immagini in im_df: {num_images_in_im_df}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def row_to_dict(row):\n    return {\n        'id': row['id'],\n        'width': row['width'],\n        'height':row['height'],\n        'file_name':row['file_name']\n    }\n\nim_list = im_df.apply(lambda row: row_to_dict(row), axis=1).tolist()\n[print(val) for val in im_list[:4]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Merge the images dataframe with the annotations to work out the absolute pixel values, plus a bit more re-organising.","metadata":{}},{"cell_type":"code","source":"annotations_df = out_data.copy()\nannotations_df['image_id'] = annotations_df['File_Name'].str.replace(r'\\D', '', regex=True).astype(int)\nannotations_df= annotations_df.rename(columns={'height': 'h', 'width': 'w'})\nan_df = annotations_df.merge(im_df, left_on='image_id', right_on='id', how='left')\nan_df['x_center']= (an_df['x_center'].astype(np.float64)*an_df['width']).round(decimals=0)\nan_df['y_center']= (an_df['y_center'].astype(np.float64)*an_df['height']).round(decimals=0)\nan_df['w']= (an_df['w'].astype(np.float64)*an_df['width']).round(decimals=0)\nan_df['h']= (an_df['h'].astype(np.float64)*an_df['height']).round(decimals=0)\nan_df['Class_ID']= an_df['Class_ID'].astype(int)\nan_df = an_df.drop(columns=['File_Name', 'file_name', 'width', 'height', 'id'])\nan_df['left'] = (an_df['x_center'] - an_df['w']/2).round(decimals=0)\nan_df['top'] =  (an_df['y_center'] - an_df['h']/2).round(decimals=0)\nan_df['bbox'] = ('[' + an_df['left'].astype(str) + ', ' \n              + an_df['top'].astype(str) + ', ' \n              + an_df['w'].astype(str) + ', '\n              + an_df['h'].astype(str) + ']')\nan_df['area'] = an_df['w'] * an_df['h']\nan_df = an_df.drop(columns=['x_center', 'y_center', 'w', 'h', 'left', 'top', 'Class_Name'])\nan_df.reset_index(inplace=True)\nan_df.rename(columns={'index': 'id'}, inplace=True)\nan_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def row_to_dict(row):\n    return {\n        'id': row['id'],\n        'image_id' : row['image_id'],\n        'category_id': row['Class_ID'],\n        'area':row['area'],\n        'bbox':row['bbox']\n    }\n\nan_list = an_df.apply(lambda row: row_to_dict(row), axis=1).tolist()\nprint(an_list[:4])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The category mapping is just about in a convenient format already","metadata":{}},{"cell_type":"code","source":"cat_list = [{key:val} for key,val in class_map_dict.items()]\nprint(cat_list[:4])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now I just need to combine the top level objects, and save to `.json`","metadata":{}},{"cell_type":"code","source":"out_json_data = {'images': im_list, 'annotations': an_list, 'categories': cat_list}\nwith open(coco_json_pth, 'w') as json_file:\n    json.dump(out_json_data, json_file, indent=4)\n    \nfor key, value in out_json_data.items():\n    print(key, value[:5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Image Sizes","metadata":{}},{"cell_type":"code","source":"def check_image_sizes(directory_path):\n    size_counts = defaultdict(int)\n\n    # Ottieni la lista dei file nella cartella\n    files = [f for f in os.listdir(directory_path) if f.endswith(('.jpg'))]\n\n    # Aggiungi una barra di progresso per iterare sui file\n    for filename in tqdm(files, desc=\"Processing images\"):\n        img_path = os.path.join(directory_path, filename)\n        with Image.open(img_path) as img:\n            size = img.size  # (width, height)\n            size_counts[size] += 1\n\n    # Crea gruppi per le dimensioni richieste\n    size_groups = {\n        'Smaller than 320x320': [],\n        '320x320': [],\n        'Larger than 320x320': [],\n    }\n\n    # Aggiungi le dimensioni agli appropriate gruppi\n    for size, count in size_counts.items():\n        width, height = size\n        if width < 320 and height < 320:\n            size_groups['Smaller than 320x320'].append((size, count))\n        elif width == 320 and height == 320:\n            size_groups['320x320'].append((size, count))\n        elif width > 320 and height > 320:\n            size_groups['Larger than 320x320'].append((size, count))\n\n    # Ordina le dimensioni per area (larghezza * altezza) in ordine decrescente\n    for group, items in size_groups.items():\n        sorted_items = sorted(items, key=lambda x: x[0][0] * x[0][1], reverse=True)  # ordina per area\n        size_groups[group] = sorted_items\n\n    # Stampa i gruppi con il numero di immagini per dimensione e il totale per intervallo\n    for group, items in size_groups.items():\n        total = sum(count for _, count in items)\n        print(f\"{group} (Totale: {total} immagini):\")\n        for size, count in items:\n            print(f\"  Dimensione {size}: {count} immagini\")\n        print()\n\n# Esegui la funzione con il percorso della cartella\ncheck_image_sizes(save_images_fldr_pth)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(coco_json_pth, 'r') as f:\n    coco_data = json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_image_size(image, img_id, coco_data):\n    # Trova le dimensioni dell'immagine nel JSON\n    image_info = next((img for img in coco_data['images'] if img['id'] == img_id), None)\n    if image_info:\n        width, height = image_info['width'], image_info['height']\n        img_width, img_height = image.size\n        # Lancia un'eccezione solo se le dimensioni non corrispondono\n        assert img_width == width and img_height == height, (\n            f\"Dimensioni errate: JSON ({width}, {height}), immagine ({img_width}, {img_height})\"\n        )\n    else:\n        raise ValueError(f\"Immagine con ID {img_id} non trovata nel JSON.\")\n\ndef check_all_images(folder_path, coco_data):\n    folder_path = Path(folder_path)\n    errors_found = False  # Flag per tenere traccia degli errori\n    check_count = 0  # Conta il numero di immagini verificate\n    \n    # Itera attraverso tutti i file nella cartella\n    for image_path in folder_path.iterdir():\n        if image_path.is_file() and image_path.suffix in ['.jpg']:  # Controlla solo file immagine\n            check_count += 1  # Incrementa il contatore delle immagini\n            # Trova l'ID corrispondente basato sul nome file\n            img_id = int(''.join(filter(str.isdigit, image_path.stem)))  # Estrae i numeri dal nome\n            try:\n                with Image.open(image_path) as img:\n                    check_image_size(img, img_id, coco_data)\n            except (AssertionError, ValueError) as e:\n                errors_found = True\n                print(f\"Errore per immagine {image_path.name}: {e}\")\n            except Exception as e:\n                errors_found = True\n                print(f\"Errore generico per immagine {image_path.name}: {e}\")\n\n    # Stampa il risultato finale\n    if not errors_found:\n        print(f\"Check completato, nessun errore trovato. Numero di immagini verificate: {check_count}\")\n    else:\n        print(f\"Check completato con errori. Numero di immagini verificate: {check_count}\")\n\n# Percorso alla directory delle immagini\ncheck_all_images(save_images_fldr_pth, coco_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Labels","metadata":{}},{"cell_type":"code","source":"def check_empty_txt_files(directory_path):\n    # Ottieni tutti i file .txt nella cartella\n    txt_files = [f for f in os.listdir(directory_path) if f.endswith(\".txt\")]\n    \n    empty_files_count = 0\n    total_lines = 0\n    non_empty_files_count = 0\n    \n    # Controlla se ogni file è vuoto\n    for txt_file in txt_files:\n        file_path = os.path.join(directory_path, txt_file)\n        if os.path.getsize(file_path) == 0:\n            empty_files_count += 1\n        else:\n            non_empty_files_count += 1\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n                total_lines += len(lines)\n    \n    # Calcola la media delle righe per i file non vuoti\n    avg_lines = total_lines / non_empty_files_count if non_empty_files_count > 0 else 0\n    \n    # Stampa il numero di file vuoti e la media delle righe nei file non vuoti\n    print(f\"Numero di file .txt vuoti: {empty_files_count}\")\n    print(f\"Numero di file .txt non vuoti: {non_empty_files_count}\")\n    print(f\"Media delle righe per file non vuoto: {avg_lines:.2f}\")\n\ncheck_empty_txt_files(save_images_fldr_pth)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Category Distribution","metadata":{}},{"cell_type":"code","source":"with open(coco_json_pth, 'r') as f:\n    coco_data = json.load(f)\n\n# Crea un dizionario per mappare category_id a categoria\ncategory_mapping = {str(index): list(category.values())[0] for index, category in enumerate(coco_data['categories'])}\n\n# Estrai i category_id dalle annotazioni\ncategory_ids = [annotation['category_id'] for annotation in coco_data['annotations']]\n\n# Conta le occorrenze di ogni category_id\ncategory_counts = Counter(category_ids)\n\n# Gruppi delle categorie\ncategory_groups = {\n    \"Aircraft\": [0, 1, 2, 3],\n    \"Passenger Vehicle\": [4, 5, 6, 53],\n    \"Truck\": [7, 8, 9, 10, 11, 12, 13, 14, 15],\n    \"Railway Vehicle\": [17, 18, 19, 20, 21, 22],\n    \"Maritime Vessel\": [23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n    \"Engineering Vehicle\": [16, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n    \"Building\": [46, 47, 48, 49, 50, 51, 52, 59],\n    \"Helipad\": [54],\n    \"Storage Tank\": [55],\n    \"Shipping Container\": [56, 57],\n    \"Pylon\": [58],\n}\n\n# Crea un dizionario per le categorie aggregate con le loro occorrenze\ngrouped_occurrences = {}\nfor group_name, category_ids in category_groups.items():\n    # Somma le occorrenze per le categorie di ogni gruppo\n    grouped_occurrences[group_name] = sum(category_counts[cat_id] for cat_id in category_ids)\n\n# Ordina le categorie raggruppate per occorrenze in ordine decrescente\ngrouped_occurrences = dict(sorted(grouped_occurrences.items(), key=lambda item: item[1], reverse=True))\n\n# Prepara i dati per il grafico\ngroups = list(grouped_occurrences.keys())\noccurrences = list(grouped_occurrences.values())\n\n\n# Crea il grafico a barre\nplt.figure(figsize=(10, 6))\nplt.barh(groups, occurrences, color='skyblue')\nplt.xlabel('Numero di Occorrenze')\nplt.ylabel('Gruppi di Categoria')\nplt.title('Distribuzione delle Occorrenze per Gruppo di Categoria')\nplt.tight_layout()\n\n# Mostra il grafico\nplt.show()\n\n# Stampare le occorrenze per ciascun group e i relativi ID\nsorted_groups = sorted(category_groups.items(), key=lambda item: sum(category_counts.get(cat_id, 0) for cat_id in item[1]))\n\nfor group, category_ids in sorted_groups:\n    # Calcola il numero totale di occorrenze per il gruppo\n    total_count = sum(category_counts.get(cat_id, 0) for cat_id in category_ids)\n    \n    print(f\"Gruppo: {group}, Occorrenze Totali: {total_count}\")\n    \n    for cat_id in category_ids:\n        category_name = category_mapping[str(cat_id)]\n        count = category_counts.get(cat_id, 0)\n        print(f\"  category_id: {cat_id}, Categoria: {category_name}, Occorrenze: {count}\")\n    \n    print(\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_coco_json_pth = out_dataset_pth / 'COCO_annotations_new.json'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica il file COCO originale\nwith open(coco_json_pth, 'r') as f:\n    coco_data = json.load(f)\n\n# Creare nuove categorie nel formato richiesto\nnew_categories = [{str(idx): group} for idx, group in enumerate(category_groups.keys())]\n\n# Mappatura inversa dei gruppi (vecchio category_id -> nuovo group_id)\ngroup_mapping = {}\nfor group_id, (group_name, category_ids) in enumerate(category_groups.items(), start=0):\n    for cat_id in category_ids:\n        group_mapping[cat_id] = group_id\n\n# Aggiorna i category_id nelle annotazioni\nfor annotation in coco_data[\"annotations\"]:\n    original_id = annotation[\"category_id\"]\n    annotation[\"category_id\"] = group_mapping.get(original_id, -1)  # Usa -1 per eventuali category_id non mappati\n\n# Verifica annotazioni non mappate\nunmapped = [ann for ann in coco_data[\"annotations\"] if ann[\"category_id\"] == -1]\nif unmapped:\n    print(f\"Attenzione: {len(unmapped)} annotazioni con category_id non mappati.\")\n\n# Aggiorna il dizionario delle categorie\ncoco_data[\"categories\"] = new_categories\n\n# Salva il file COCO modificato\nnew_coco_json_pth = Path(out_dataset_pth) / 'COCO_annotations_new.json'\nwith open(new_coco_json_pth, \"w\") as f:\n    json.dump(coco_data, f, indent=4)\n\nprint(f\"File COCO modificato salvato in {new_coco_json_pth}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica il file COCO modificato\nwith open(new_coco_json_pth, 'r') as f:\n    coco_data = json.load(f)\n\n# Crea un dizionario per mappare category_id a nome della categoria dal nuovo formato\ncategory_mapping = {\n    int(list(category.keys())[0]): list(category.values())[0]\n    for category in coco_data[\"categories\"]\n}\n\n# Estrai i category_id dalle annotazioni\ncategory_ids = [annotation[\"category_id\"] for annotation in coco_data[\"annotations\"]]\n\n# Conta le occorrenze di ogni category_id\ncategory_counts = Counter(category_ids)\n\n# Associa i nomi delle categorie alle loro occorrenze\ncategory_occurrences = {\n    cat_id: (category_mapping[cat_id], count)\n    for cat_id, count in category_counts.items()\n    if cat_id in category_mapping\n}\n\n# Ordina le categorie per occorrenze in ordine decrescente\ncategory_occurrences = dict(sorted(category_occurrences.items(), key=lambda item: item[1][1], reverse=True))\n\n# Prepara i dati per il grafico\ncategories = [f\"{cat_id} - {name}\" for cat_id, (name, _) in category_occurrences.items()]\noccurrences = [count for _, (_, count) in category_occurrences.items()]\n\n# Crea il grafico a barre\nplt.figure(figsize=(10, 8))\nplt.barh(categories, occurrences, color=\"skyblue\")\nplt.xlabel(\"Numero di Occorrenze\")\nplt.ylabel(\"Categorie\")\nplt.title(\"Distribuzione delle Occorrenze per Categoria\")\nplt.tight_layout()\n\n# Mostra il grafico\nplt.show()\n\n# Stampa le occorrenze per ogni categoria con ID\nfor cat_id, (name, count) in category_occurrences.items():\n    print(f\"Categoria ID: {cat_id}, Nome: {name}, Occorrenze: {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Region Proposal Algorithm","metadata":{}},{"cell_type":"code","source":"def generate_region_proposals(image):\n    # Converti l'immagine in NumPy array\n    img_np = np.array(image)\n\n    # Applica Selective Search\n    _, regions = selectivesearch.selective_search(img_np, scale=500, sigma=0.9, min_size=10)\n    if (len(regions) == 0):\n        print(\"Warning: Zero regioni per immagine\")\n    \n    # Filtra le regioni per eliminare duplicati e regioni piccole\n    proposals = []\n    for region in regions:\n        x, y, w, h = region['rect']\n        if w >= 10 and h >= 10:  # Esempio di filtraggio: minimo 20x20\n            proposals.append((x, y, x + w, y + h))\n\n    return proposals","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = os.path.join(save_images_fldr_pth, \"img_2032_0_960.jpg\")\n\n# Carica l'immagine\nimage = Image.open(image_path).convert('RGB')\n\n# Funzione per generare le region proposals (presuppone che sia già definita come nel tuo codice precedente)\nproposals = generate_region_proposals(image)\n\n# Stampa le region proposals\nprint(proposals)  # Controlla il contenuto delle proposte","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_region_proposal(image, proposal):\n    \"\"\"\n    Ridimensiona una region proposal a 227x227 per AlexNet.\n    \n    Args:\n        image (PIL.Image): L'immagine originale.\n        proposal (list): [x_min, y_min, x_max, y_max].\n    \n    Returns:\n        torch.Tensor: La region proposal ridimensionata.\n    \"\"\"\n    x_min, y_min, x_max, y_max = proposal\n    region = image.crop((x_min, y_min, x_max, y_max))  # Taglia la regione\n    transform = transforms.Compose([\n        transforms.Resize((227, 227)),  # Ridimensiona a 227x227\n        transforms.ToTensor(),          # Converte in Tensor\n    ])\n    return transform(region)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IoU Loss","metadata":{}},{"cell_type":"code","source":"def assign_proposals_to_gt(proposals, gt_bboxes, iou_threshold=0.5):\n    assigned_labels = []\n    assigned_boxes = []\n\n    for proposal in proposals:\n        iou_scores = [calculate_iou(proposal, gt_box) for gt_box in gt_bboxes]\n        max_iou = max(iou_scores)\n\n        if max_iou >= iou_threshold:\n            assigned_labels.append(1)  # Oggetto presente\n            assigned_boxes.append(gt_bboxes[iou_scores.index(max_iou)])\n        else:\n            assigned_labels.append(0)  # Background\n            assigned_boxes.append(proposal)\n\n    return assigned_labels, assigned_boxes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou_loss(pred_bboxes, gt_bboxes):\n    \"\"\"\n    Calcola la perdita IoU media tra i bounding box predetti e quelli di ground truth.\n    \"\"\"\n    iou_scores = torch.stack([calculate_iou(pred, gt) for pred, gt in zip(pred_bboxes, gt_bboxes)])\n    return 1 - iou_scores.mean()  # Più bassa è la IoU, maggiore è la perdita","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cross_entropy_loss(pred_class_logits, labels):\n    \"\"\"\n    Calcola la Cross-Entropy Loss per la classificazione degli oggetti.\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    return criterion(pred_class_logits, labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_loss(proposals, gt_bboxes, pred_bboxes, pred_class_logits, labels, iou_threshold=0.5):\n    # Assegna le proposte ai ground truth\n    assigned_labels, assigned_boxes = assign_proposals_to_gt(proposals, gt_bboxes, iou_threshold)\n\n    # Indici per le proposte positive e negative\n    pos_indices = [i for i, label in enumerate(assigned_labels) if label == 1]\n    neg_indices = [i for i, label in enumerate(assigned_labels) if label == 0]\n\n    # Per le proposte positive, calcola la IoU Loss\n    pos_pred_bboxes = pred_bboxes[pos_indices]\n    pos_gt_bboxes = [assigned_boxes[i] for i in pos_indices]\n    \n    # IoU Loss per le proposte positive\n    loss = iou_loss(pos_pred_bboxes, pos_gt_bboxes)\n\n    # Calcola la Cross-Entropy Loss per la classificazione\n    ce_loss = cross_entropy_loss(pred_class_logits, labels)\n    \n    # Combina la IoU Loss e la Cross-Entropy Loss\n    total_loss = loss + ce_loss\n\n    return total_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DataLoader**","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n        \n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n\n        self.img_dir = img_dir\n        \n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n        \n        self.category_map = {int(list(category.keys())[0]): list(category.values())[0] for category in coco_data['categories']}\n        \n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox_str = annotation['bbox']\n            bbox = list(map(float, bbox_str.strip('[]').split(', ')))\n\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n\n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n\n        self.coco_data = coco_data\n\n        # Trasformazioni: solo Resize e ToTensor\n        self.transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n        \n        self.non_320x320_count = 0\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n    \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n    \n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n    \n        # Carica l'immagine e convertila in RGB\n        image = Image.open(img_path).convert('RGB')\n    \n        # Assicurati che l'immagine sia un array intero per evitare problemi\n        image_np = np.array(image)\n        if image_np.dtype != np.uint8:\n            image_np = (image_np * 255).astype(np.uint8)\n            image = Image.fromarray(image_np)\n    \n        # Genera region proposals\n        proposals = generate_region_proposals(image)\n    \n        # Ridimensiona le region proposals a 227x227\n        resized_proposals = [process_region_proposal(image, proposal) for proposal in proposals]\n\n        if not resized_proposals:\n            print(f\"Immagine {img_name} non ha region proposals valide.\")\n    \n        # Applica trasformazioni\n        if self.transform:\n            image = self.transform(image)\n    \n        categories = self.image_annotations.get(img_id, [])\n        bboxes = torch.tensor(self.image_bboxes.get(img_id, []), dtype=torch.float)\n        label = torch.tensor(categories, dtype=torch.long)\n    \n        return image, label, bboxes, resized_proposals\n\n    def get_non_320x320_count(self):\n        return self.non_320x320_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    images, labels, bboxes, regions = zip(*batch)  # Separiamo i dati\n\n    # Gestiamo le immagini\n    images = torch.stack(images)\n\n    # Gestisci labels\n    labels = [item for sublist in labels for item in sublist]  # Appiattisce la lista\n    labels = torch.tensor(labels)\n\n    # Gestisci bounding boxes\n    bboxes = [torch.tensor(b) for b in bboxes]\n\n    return images, labels, bboxes, regions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=True)\nvalid_dataset = CustomDataset(val_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \ntest_dataset = CustomDataset(test_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_txt_vs_json(txt_paths, coco_data):\n    \"\"\"\n    Controlla se le immagini del JSON sono presenti in almeno uno dei file TXT.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n        coco_data (dict): Dati in formato COCO.\n    \"\"\"\n    # Estrai i nomi delle immagini dal JSON\n    image_names = [image['file_name'] for image in coco_data['images']]\n    \n    # Inizializza un set per contenere tutte le immagini presenti nei TXT\n    txt_image_names = set()\n    \n    # Leggi i nomi delle immagini da ciascun file TXT\n    for txt_path in txt_paths:\n        with open(txt_path, 'r') as f:\n            txt_image_names.update(os.path.basename(line.strip()) for line in f.readlines())\n    \n    # Trova le immagini presenti nel JSON ma non in nessuno dei TXT\n    missing_in_txts = [name for name in image_names if name not in txt_image_names]\n    \n    # Verifica e stampa i risultati\n    print(\"\\nControllo completato:\")\n    if missing_in_txts:\n        print(f\"Errore: le seguenti immagini non sono presenti in nessuno dei file TXT forniti:\\n{missing_in_txts}\")\n    else:\n        print(\"Tutte le immagini del JSON sono presenti in almeno uno dei file TXT.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta il numero di immagini nel JSON\nnum_images = len(coco_data['images'])\nprint(f\"Numero di immagini nel JSON: {num_images}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estrai i nomi delle immagini dal JSON\nimage_names = [image['file_name'] for image in coco_data['images']]\n\n# Stampa i primi 5 nomi delle immagini nel JSON\nprint(f\"Primi 5 nomi delle immagini nel JSON: {image_names[:5]}\")   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_lines_in_txt(txt_paths):\n    \"\"\"\n    Conta il numero di righe in ciascun file TXT dato il percorso e calcola la somma totale delle righe.\n    \n    Args:\n        txt_paths (list): Lista di percorsi ai file TXT.\n    \"\"\"\n    total_lines = 0  # Variabile per accumulare il numero totale di righe\n    \n    for txt_path in txt_paths:\n        try:\n            # Apri il file e conta le righe\n            with open(txt_path, 'r') as f:\n                num_lines = sum(1 for line in f)\n            total_lines += num_lines  # Aggiungi il numero di righe del file al totale\n            print(f\"Numero di righe nel file {txt_path}: {num_lines}\")\n        except FileNotFoundError:\n            print(f\"Errore: il file {txt_path} non è stato trovato.\")\n        except Exception as e:\n            print(f\"Errore nel leggere il file {txt_path}: {e}\")\n    \n    # Stampa la somma totale delle righe\n    print(f\"\\nSomma totale delle righe in tutti i file: {total_lines}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_paths = [train_txt_pth, val_txt_pth, test_txt_pth]\ncount_lines_in_txt(txt_paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_txt_vs_json(txt_paths, coco_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check per verificare un'immagine particolare tramite id e tramite file_name","metadata":{}},{"cell_type":"code","source":"# Elenco di tutte le immagini\ncoco_images = [image['file_name'] for image in coco_data['images']]\nif \"img_2032_0_960.jpg\" in coco_images:\n    print(\"Immagine trovata nel file COCO!\")\nelse:\n    print(\"Immagine NON trovata nel file COCO!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calcola l'ID estratto dal nome del file problematico\nextracted_id = int(re.sub(r'\\D', '', \"img_2032_0_960.jpg\"))\n\n# Verifica se esiste nel dizionario delle immagini COCO\nif extracted_id in train_dataset.image_info:\n    print(f\"Immagine trovata! ID: {extracted_id}, Nome file: {train_dataset.image_info[extracted_id]}\")\nelse:\n    print(f\"ID {extracted_id} non trovato nel file COCO.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Controllo del dataloader con stampa di un'immagine con i bbox, successivamente controllo della corretta generazione delle region proposals","metadata":{}},{"cell_type":"code","source":"def check_dataloader(loader):\n    for i, (images, labels, bboxes, proposals) in enumerate(loader):  # Aggiungi 'proposals' come quarto valore\n        # Verifica se 'images' è un tensore\n        if isinstance(images, torch.Tensor):\n            print(f\"Batch {i + 1}:\")\n            print(f\"  Numero di immagini nel batch: {images.size(0)}\")\n            print(f\"  Dimensione delle immagini nel batch: {images.size(1)}, {images.size(2)}, {images.size(3)}\")\n            \n            # Scegli un'immagine casuale dal batch\n            random_index = random.randint(0, images.size(0) - 1)\n            image = images[random_index].permute(1, 2, 0).numpy()  # Cambia la dimensione per il plotting\n            \n            fig, ax = plt.subplots(1, figsize=(12, 9))\n            ax.imshow(image)\n            \n            # Prendi i bounding box e le etichette per l'immagine casuale\n            # Gestisci la lunghezza di labels[random_index] in base alle dimensioni del tensore\n            if labels[random_index].dim() > 0:  # Se labels è un tensore con dimensioni\n                num_objects = labels[random_index].shape[0]\n            else:  # Se labels è un singolo valore (0-d tensor)\n                num_objects = 1\n\n            for j in range(num_objects):  # Itera sulle etichette (ognuna rappresenta un oggetto)\n                category_id = int(labels[random_index][j])  # Categoria dell'oggetto\n                x_min, y_min, width, height = bboxes[random_index][j]  # Bounding box corrispondente\n                \n                # Crea un rettangolo per il bounding box\n                rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n                ax.add_patch(rect)\n                \n                # Aggiungi il testo con l'ID della categoria\n                ax.text(x_min, y_min, str(category_id), color='white', fontsize=10, \n                        bbox=dict(facecolor='red', alpha=0.5, edgecolor='none', boxstyle='round,pad=0.3'))\n            \n            plt.axis('off')\n            plt.show()\n        else:\n            print(f\"Batch {i + 1} non contiene un tensore per 'images'!\")\n        \n        break  # Ferma dopo il primo batch, rimuovi se vuoi continuare con altri","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui il controllo per i dataloader\ncheck_dataloader(train_loader)\ncheck_dataloader(val_loader)\ncheck_dataloader(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_region_proposals(loader):\n    for i, (images, labels, bboxes, proposals) in enumerate(loader):  # Aggiungi 'proposals' come quarto valore\n        print(f\"Batch {i + 1}:\")\n        \n        # Verifica che le region proposals siano presenti e abbiano una struttura valida\n        if proposals:\n            print(f\"  Numero di immagini con region proposals nel batch {i + 1}: {len(proposals)}\")\n        else:\n            print(f\"  Warning: Nessuna regione proposta trovata per il batch {i + 1}\")\n\n        break  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_region_proposals(train_loader)\ncheck_region_proposals(val_loader)\ncheck_region_proposals(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Set Category Distribution ","metadata":{}},{"cell_type":"code","source":"# Funzione per calcolare la distribuzione delle categorie in un DataLoader\ndef count_category_occurrences(dataloader):\n    category_counts = Counter()\n\n    for images, labels, bboxes, regions in dataloader:\n        for label_batch in labels:\n            if isinstance(label_batch, torch.Tensor):\n                category_counts.update(label_batch.cpu().numpy())  # Usa .cpu().numpy() per evitare errori su GPU\n\n    return category_counts\n\n# Calcola le occorrenze per train, val, test\ntrain_category_counts = count_category_occurrences(train_loader)\nval_category_counts = count_category_occurrences(val_loader)\ntest_category_counts = count_category_occurrences(test_loader)\n\n# Funzione per calcolare percentuali delle categorie\ndef calculate_category_percentages(category_counts):\n    total_count = sum(category_counts.values())\n    category_percentages = {\n        category_id: (count, (count / total_count) * 100 if total_count > 0 else 0)\n        for category_id, count in category_counts.items()\n    }\n    return category_percentages\n\n# Calcola le percentuali per train, val e test\ntrain_category_percentages = calculate_category_percentages(train_category_counts)\nval_category_percentages = calculate_category_percentages(val_category_counts)\ntest_category_percentages = calculate_category_percentages(test_category_counts)\n\n# Funzione per stampare la distribuzione e la percentuale per ogni categoria\ndef print_category_distribution(category_percentages, category_mapping, dataset_name):\n    print(f\"\\nDistribuzione delle categorie nel {dataset_name} set:\")\n    total_occurrences = sum(count for count, _ in category_percentages.values())\n    for category_id, (count, percentage) in category_percentages.items():\n        category_name = category_mapping.get(category_id, f\"ID sconosciuto: {category_id}\")\n        print(f\"Categoria: {category_name} (ID: {category_id}), Occorrenze: {count}, Percentuale: {percentage:.2f}%\")\n    print(f\"Totale occorrenze nel {dataset_name} set: {total_occurrences}\")\n\n# Stampiamo le distribuzioni per i set train, val e test\nprint_category_distribution(train_category_percentages, category_mapping, \"Train\")\nprint_category_distribution(val_category_percentages, category_mapping, \"Validation\")\nprint_category_distribution(test_category_percentages, category_mapping, \"Test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dopo aver analizzato il numero di occorrenze nello splittaggio e notando il generale equilibrio, abbiamo deciso di non optare per uno stratified splitting.","metadata":{}},{"cell_type":"markdown","source":"# **Modelli**\nModelli scelti:\n* [R-CNN con backbone AlexNet](https://medium.com/@selfouly/r-cnn-3a9beddfd55a)\n* [SPPNet con backbone ZF-5](https://arxiv.org/pdf/1406.4729)\n* [Faster R-CNN con backbone VGG16](https://medium.com/@fractal.ai/guide-to-build-faster-rcnn-in-pytorch-42d47cb0ecd3)","metadata":{}},{"cell_type":"markdown","source":"## R-CNN (AlexNet)","metadata":{}},{"cell_type":"code","source":"class RCNNModel(nn.Module):\n    def __init__(self, base_model='alexnet', num_classes=12):  # 11 classi + sfondo\n        super(RCNNModel, self).__init__()\n        \n        # Carica AlexNet con pesi pre-addestrati usando la nuova sintassi\n        alexnet = models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)  # O usa AlexNet_Weights.DEFAULT\n        self.backbone = nn.Sequential(*list(alexnet.children())[:-2])  # Rimuovi i layer Fully Connected (FC)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Pooling adattivo\n\n        # Classificatore per multi-classe\n        self.classifier = nn.Sequential(\n            nn.Linear(alexnet.classifier[1].in_features, 512),  # Numero di feature dal penultimo layer\n            nn.ReLU(),\n            nn.Linear(512, num_classes)  # Numero di classi\n        )\n\n        # Opzionale: Bounding Box Regressor\n        self.bbox_regressor = nn.Sequential(\n            nn.Linear(alexnet.classifier[1].in_features, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4)  # 4 coordinate per il bounding box (x1, y1, x2, y2)\n        )\n\n    def forward(self, x):\n        # Estrai feature dalla backbone per ogni proposta\n        x = self.backbone(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        # Classificazione multi-classe\n        class_logits = self.classifier(x)\n\n        # Regressione del Bounding Box (opzionale)\n        bbox_preds = self.bbox_regressor(x)\n\n        return class_logits, bbox_preds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_loader, val_loader, num_classes, device='cuda'):\n        \"\"\"\n        Classe per il training e la validazione di un modello RCNN.\n        Args:\n            model (nn.Module): il modello da addestrare.\n            train_loader (DataLoader): dataloader per il set di training.\n            val_loader (DataLoader): dataloader per il set di validazione.\n            num_classes (int): numero di classi (incluso lo sfondo).\n            device (str): dispositivo per l'addestramento ('cuda' o 'cpu').\n        \"\"\"\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.num_classes = num_classes\n        \n        # Ottimizzatore\n        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n        \n        # Per tenere traccia della storia di training e validazione\n        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n    \n    def train_one_epoch(self):\n        \"\"\"Esegue un'epoca di training.\"\"\"\n        self.model.train()\n        running_loss = 0.0\n        \n        for images, labels, proposals, gt_bboxes in self.train_loader:\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n            \n            # Converti ogni elemento di proposals in un tensore\n            proposals = [torch.tensor(p).to(self.device) for p in proposals]\n            \n            # Assicurati che gt_bboxes sia un tensore e trasferiscilo sul dispositivo\n            gt_bboxes = gt_bboxes.to(self.device)  # Ground truth bounding boxes\n            \n            # Forward pass\n            outputs = self.model(images)  # Estrai le predizioni dalle immagini\n            pred_bboxes = outputs[0]  # Predizioni dei bounding box\n            pred_class_logits = outputs[1]  # Predizioni delle classi\n            \n            # Calcola la perdita\n            loss = compute_loss(proposals, gt_bboxes, pred_bboxes, pred_class_logits, labels)\n            \n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)  # Accumula la perdita totale\n        \n        # Calcola la perdita media\n        epoch_loss = running_loss / len(self.train_loader.dataset)\n        return epoch_loss\n    \n    def validate(self):\n        \"\"\"Esegue la validazione sul set di validazione.\"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels, proposals, gt_bboxes in self.val_loader:\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                proposals = proposals.to(self.device)\n                gt_bboxes = gt_bboxes.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(images)\n                pred_bboxes = outputs[0]  # Predizioni dei bounding box\n                pred_class_logits = outputs[1]  # Predizioni delle classi\n                loss = compute_loss(proposals, gt_bboxes, pred_bboxes, pred_class_logits, labels)\n                running_loss += loss.item() * images.size(0)\n                \n                # Calcolo accuratezza\n                _, preds = torch.max(pred_class_logits, 1)  # Predizioni con probabilità massima\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        \n        # Calcola la perdita media e accuratezza\n        epoch_loss = running_loss / len(self.val_loader.dataset)\n        accuracy = correct / total\n        return epoch_loss, accuracy\n    \n    def train(self, num_epochs):\n        \"\"\"Esegue il training e la validazione per un numero di epoche.\"\"\"\n        for epoch in range(num_epochs):\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n            \n            # Training\n            train_loss = self.train_one_epoch()\n            print(f\"Train Loss: {train_loss:.4f}\")\n            \n            # Validazione\n            val_loss, val_acc = self.validate()\n            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n            \n            # Salva i risultati\n            self.history['train_loss'].append(train_loss)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n    \n    def save_model(self, path):\n        \"\"\"Salva il modello addestrato.\"\"\"\n        torch.save(self.model.state_dict(), path)\n        print(f\"Modello salvato in {path}\")\n    \n    def load_model(self, path):\n        \"\"\"Carica un modello addestrato.\"\"\"\n        self.model.load_state_dict(torch.load(path))\n        self.model.to(self.device)\n        print(f\"Modello caricato da {path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(test_image_path, model, top_k=2000):\n    # Carica l'immagine\n    image = cv2.imread(test_image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Genera region proposals con Selective Search\n    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n    ss.setBaseImage(image)\n    ss.switchToSelectiveSearchFast()\n    ss_results = ss.process()[:top_k]  # Prendi le prime 2000 proposte\n    \n    # Trasforma region proposals in tensor\n    transform = transforms.Compose([\n        transforms.Resize((227, 227)),\n        transforms.ToTensor()\n    ])\n    proposals = []\n    for region in ss_results:\n        x, y, w, h = region\n        cropped = image_rgb[y:y + h, x:x + w]\n        resized = cv2.resize(cropped, (227, 227), interpolation=cv2.INTER_AREA)\n        proposals.append(transform(Image.fromarray(resized)))\n    proposals = torch.stack(proposals)  # Shape: (top_k, 3, 224, 224)\n    \n    # Predizioni del modello\n    model.eval()\n    with torch.no_grad():\n        class_logits = model(proposals)\n        predictions = torch.argmax(class_logits, dim=1).cpu().numpy()  # 0 o 1\n    \n    # Filtra region proposals positive\n    positive_regions = [ss_results[i] for i in range(len(predictions)) if predictions[i] == 1]\n    \n    return positive_regions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.is_available())  # Dovrebbe restituire True se CUDA è disponibile\ndevice = torch.device('cuda')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inizializzazione del modello\nmodel = RCNNModel(base_model='alexnet', num_classes=12)  # 12 classi = 11 + sfondo\n\n# Inizializzazione del Trainer\ntrainer = Trainer(model=model, \n                  train_loader=train_loader, \n                  val_loader=val_loader, \n                  num_classes=12,\n                  device=device)  # Usa 'cuda' per l'addestramento su GPU","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui l'addestramento per 10 epoche\ntrainer.train(num_epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model('rcnn_model.pth')  # Salva il modello addestrato","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.load_model('rcnn_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SPPNet (ZF-5)","metadata":{}},{"cell_type":"code","source":"'''\nclass SPPNet(nn.Module):\n    def __init__(self):\n        super(SPPNet, self).__init__()\n        # Definisci il modello ZF-5 (convoluzioni e pooling)\n        # Implementa i livelli SPP\n\n    def forward(self, x):\n        # Calcola il forward pass con SPP\n        return x\n\nsppnet_model = SPPNet()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Faster R-CNN (VGG16)","metadata":{}},{"cell_type":"code","source":"'''\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\n# Usa il backbone VGG16\nvgg16_backbone = models.vgg16(pretrained=True).features\n\n# Carica Faster R-CNN con backbone personalizzato\nfaster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\nfaster_rcnn_model.backbone = vgg16_backbone\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}