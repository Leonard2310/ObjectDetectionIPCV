{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2571636,"sourceType":"datasetVersion","datasetId":1561333}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Librerie","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nimport matplotlib.pyplot as plt\nimport imageio.v3 as imageio\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom pathlib import Path\nimport pandas as pd\nimport cv2\nimport os\nfrom os import sep\nimport shutil\nimport json\nimport yaml\nimport random\nimport time\nfrom tqdm.notebook import tqdm_notebook\nimport concurrent.futures\nimport multiprocessing as mp\n\nfrom PIL import Image\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\nfrom torchvision import transforms\n\nimport json\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport random\nfrom torchvision import transforms as T\nfrom torchvision.transforms import functional as TF\n\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:54:28.374008Z","iopub.execute_input":"2024-11-16T18:54:28.374480Z","iopub.status.idle":"2024-11-16T18:54:35.519979Z","shell.execute_reply.started":"2024-11-16T18:54:28.374437Z","shell.execute_reply":"2024-11-16T18:54:35.518539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"Il nostro dataset è xView, un'analisi preliminare del dataset è presente nel [documento](https://medium.com/picterra/the-xview-dataset-and-baseline-results-5ab4a1d0f47f) linkato.\n\n\nQuesto dataset rappresentava il migliore per il benchmarking di visione artificiale satellitare. La documentazione tecnica spiega che i dati sono ottenuti dai satelliti **WorldView-3**, con una distanza di campionamento al suolo uniforme di 0,3 metri. Ciò conferisce una risoluzione più elevata e omogenea rispetto alla maggior parte degli altri dataset satellitari esistenti all'epoca, molti dei quali si basano invece su fotografie aeree. Quest'ultime, infatti, presentano differenze nella distorsione causate dall'angolo di ripresa, poiché sono scattate da velivoli a bassa quota. \n\nIl dataset **xView** offre una copertura geografica ampia e diversificata, includendo anche aree meno sviluppate e urbanizzate, fornendo quindi una maggiore varietà di scenari rispetto a dataset più convenzionali. ","metadata":{}},{"cell_type":"markdown","source":"### Elenco delle categorie del dataset:  \n\n#### 0-3: Velivoli  \n- 0: Fixed-wing Aircraft  \n- 1: Small Aircraft  \n- 2: Passenger/Cargo Plane  \n- 3: Helicopter  \n\n#### 4-15: Veicoli terrestri  \n- 4: Passenger Vehicle  \n- 5: Small Car  \n- 6: Bus  \n- 7: Pickup Truck  \n- 8: Utility Truck  \n- 9: Truck  \n- 10: Cargo Truck  \n- 11: Truck Tractor w/ Box Trailer  \n- 12: Truck Tractor  \n- 13: Trailer  \n- 14: Truck Tractor w/ Flatbed Trailer  \n- 15: Truck Tractor w/ Liquid Tank  \n\n#### 16: Veicoli speciali  \n- 16: Crane Truck  \n\n#### 17-22: Veicoli ferroviari  \n- 17: Railway Vehicle  \n- 18: Passenger Car  \n- 19: Cargo/Container Car  \n- 20: Flat Car  \n- 21: Tank car  \n- 22: Locomotive  \n\n#### 23-32: Imbarcazioni  \n- 23: Maritime Vessel  \n- 24: Motorboat  \n- 25: Sailboat  \n- 26: Tugboat  \n- 27: Barge  \n- 28: Fishing Vessel  \n- 29: Ferry  \n- 30: Yacht  \n- 31: Container Ship  \n- 32: Oil Tanker  \n\n#### 33-45: Veicoli da costruzione/industriali  \n- 33: Engineering Vehicle  \n- 34: Tower crane  \n- 35: Container Crane  \n- 36: Reach Stacker  \n- 37: Straddle Carrier  \n- 38: Mobile Crane  \n- 39: Dump Truck  \n- 40: Haul Truck  \n- 41: Scraper/Tractor  \n- 42: Front loader/Bulldozer  \n- 43: Excavator  \n- 44: Cement Mixer  \n- 45: Ground Grader  \n\n#### 46-51: Edifici e strutture  \n- 46: Hut/Tent  \n- 47: Shed  \n- 48: Building  \n- 49: Aircraft Hangar  \n- 50: Damaged Building  \n- 51: Facility  \n\n#### 52-59: Altre infrastrutture  \n- 52: Construction Site  \n- 53: Vehicle Lot  \n- 54: Helipad  \n- 55: Storage Tank  \n- 56: Shipping container lot  \n- 57: Shipping Container  \n- 58: Pylon  \n- 59: Tower  ","metadata":{}},{"cell_type":"markdown","source":"Utilizziamo un processo di preprocessing per il dataset **xView** seguendo i passi presenti in questo [Notebook (Preprocessing)](https://www.kaggle.com/code/ollypowell/xview-dataset-to-yolo-and-coco-format). L'obiettivo è:\n\n1. **Pulizia e riformattazione**: Si parte dal dataset grezzo e lo si prepara per l'addestramento di modelli di intelligenza artificiale per il rilevamento di oggetti.\n2. **Suddivisione in chunck**: Poiché le immagini satellitari sono molto grandi, vengono suddivise in \"pezzi\" più piccoli (chunck) per facilitare l'elaborazione. I bounding box, che definiscono le posizioni degli oggetti nelle immagini, vengono ridimensionati in modo che corrispondano ai nuovi pezzi.\n3. **Ottimizzazione del formato**: Le immagini TIFF, che occupano molto spazio, vengono convertite in formato JPG, molto più leggero.\n4. **Personalizzazione**: È possibile scegliere la dimensione delle immagini finali e come suddividere i dati per l'addestramento.\n\nQuesto processo riduce significativamente la dimensione del dataset, rendendolo più gestibile, senza sacrificare le informazioni necessarie per l'addestramento di modelli come **YOLOv5**.\n","metadata":{}},{"cell_type":"markdown","source":"Dopo aver studiato le considerazioni fatte durante l'analisi del Dataset presenti in questo [Notebook (Exploratory Data Analysis)](https://www.kaggle.com/code/ollypowell/xview-1-dataset-eda?scriptVersionId=157247786) abbiamo scelto di effettuare delle modificare al preprocessing originale fatto nel documento sopra linkato.\nIn particolare abbiamo osservato che circa il 41% delle chunk con grandezza compresa tra 600x600 e 640x640 risultate prive di label, essendo il dataset originale una raccolta di immagini con almeno una label. Abbiamo deciso di suddividere le immagini originali in chunck più piccoli 320x320, successivamente di eliminare tutte le immagini con le label vuote e considerare per i prossimi passi solo le immagini che portano quindi informazioni utili per il training della rete. Abbiamo, inoltre, corretto alcuni errori presenti nel preprocessing originale che non permettevano di escludere i chunck inferiori ad una certa dimensione.\nLa scelta di prendere chunck più piccoli è stata presa considerando che suddividere le immagini in parti più piccole ci permette di avere immagini più piccole, massimizzare la probabilità di avere tutte le label del dataset originale e di avere una più facile visualizzazione degli ogetti da individuare anche a occhio umano.\nAbbiamo scelto di non considerare i chunck compresi nell'intervallo [300x300, 320x320) essendo un numero irrisorio rispetto ai chunck considerati e richiedendo diverse scelte progettuali su come prendere le immagini di dimensioni diversa e applicare tecniche come stretch che dovevano applicarsi anche sulle coordinate geometriche delle label.","metadata":{}},{"cell_type":"markdown","source":"## Setup\nTo run locally without changing the filepath setup, just copy the relative file structure below:\n\n- For the images: My_Project/Data/X_view/train_images\n- For the labels:  My_Project/Data/X_view/train_labels/xView_train.geojson\n- For this notebook: My_Project/Python/reformat_yolo.ipynb\n\nYou can change the 'My_Project', 'Python', 'reformat_xview.ipynb' to whatever you like.","metadata":{}},{"cell_type":"code","source":"#Data sources\nDATA_FLDR_NM = 'Data'\nIN_DATASET_NM = 'xview-dataset'\nIMAGE_FLDR_NM = 'train_images'\nIN_LABELS_FLDR_NM = 'train_labels'\nLABELS_XML_NM = 'xView_train.geojson'\n\n#Output folders and file names\nOUT_DATASET_NM = 'xview-yolo-dataset'\nCLASS_MAP_JSON_NM = 'xView_class_map.json'\nOUT_COCO_JSON_NM = 'COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nOUT_DATAFRAME_NM = 'xview_labels.parquet'\nYAML_NM = 'xview_yolo.yaml'\nCHUNK_WIDTH = 320  # width of the images being created\nCHUNK_HEIGHT = 320\nMIN_CHUNK_HEIGHT = 320 # no images will be kept if the image chunk is smaller than this\nMIN_CHUNK_WIDTH = 320\nIMAGE_WRITING = True #True to re-perform image cropping, False just to regenerated other data\nTEST_FRACTION = 0.2\nJPEG_COMPRESSION = 95 # For the saved files\nVAL_FRACTION = 0.2\nRANDOM_SEED = 2023\nDEBUG = False\n\nin_dataset_pth = Path('/kaggle/input/xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nfuture_ds_img_fldr = Path(f'/kaggle/working/{OUT_IMAGE_FLDR_NM}')\nfuture_ds_cfg_fldr = Path(f'/kaggle/working/{OUT_CFG_FLDR_NM}')\n\nlabels_json_pth = in_dataset_pth / IN_LABELS_FLDR_NM / LABELS_XML_NM\nimg_fldr_pth = in_dataset_pth / IMAGE_FLDR_NM / IMAGE_FLDR_NM\nsave_images_fldr_pth = out_dataset_pth / OUT_IMAGE_FLDR_NM \nout_data_parquet_pth = out_dataset_pth / OUT_DATAFRAME_NM\nout_json_map_pth = out_dataset_pth / CLASS_MAP_JSON_NM \nclass_map_pth = out_dataset_pth / CLASS_MAP_JSON_NM\ncfg_fldr_pth = out_dataset_pth / OUT_CFG_FLDR_NM\ncoco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nyolo_yaml_pth = cfg_fldr_pth / YAML_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'train.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\ndef make_empty_dir(directory):\n    if directory.is_dir():\n        shutil.rmtree(directory)\n    os.makedirs(directory)\n\nmake_empty_dir(cfg_fldr_pth)\nif IMAGE_WRITING:\n    make_empty_dir(save_images_fldr_pth)\n\nrandom.seed(RANDOM_SEED)\n\nprint(f'The input images are found at {cfg_fldr_pth}')\nprint(f'The input labels are found at  {labels_json_pth}')\nprint(f'Configuration files will be saved to {cfg_fldr_pth}')\nprint(f'YOLO image files will be saved to {save_images_fldr_pth}')","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:54:35.522553Z","iopub.execute_input":"2024-11-16T18:54:35.523111Z","iopub.status.idle":"2024-11-16T18:54:35.544687Z","shell.execute_reply.started":"2024-11-16T18:54:35.523069Z","shell.execute_reply":"2024-11-16T18:54:35.543362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"def get_boxes(in_df, class_lst=[]):\n    if class_lst:\n        in_df = in_df[in_df['TYPE_ID'].isin(class_lst)]\n    unique_images = in_df.IMAGE_ID.unique().tolist()\n    boxs = {}\n\n    for image in tqdm_notebook(unique_images):\n        mask = in_df['IMAGE_ID'] == image\n        masked = in_df[mask][['TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX']]\n        boxs[image] = masked.values.tolist()\n    return boxs\n\n\ndef print_first_n_lines(file_path, n):\n    try:\n        with open(file_path, 'r') as file:\n            for line_num, line in enumerate(file, 1):\n                if line_num > n:\n                    break\n                print(line.strip())\n    except FileNotFoundError:\n        print('Unable to open file')\n\n\ndef load_image(file_pth): #for display\n    image_obj = cv2.imread(file_pth)\n    image_obj = cv2.cvtColor(image_obj, cv2.COLOR_BGR2RGB)\n    return image_obj\n\n\ndef load_bgr_image(file_pth): # for processing only, no need to visualise the image\n    image_obj = cv2.imread(file_pth)\n    return image_obj\n\ndef display_images(image_lst, boxes_dictionary, image_fldr, max_images=6, no_cols=1, text=False,  class_map={}):\n    total_ims = len(image_lst)\n    display_ims = min(max_images, total_ims)\n    no_rows = display_ims//no_cols + (display_ims % no_cols > 0)\n    fig, axs = plt.subplots(no_rows, no_cols, figsize=(10, 10*no_rows/no_cols*3/4))\n    axs = axs.flatten()\n    for k, img_nm in enumerate(image_lst[:display_ims]):\n        image_path = str(image_fldr / img_nm)\n        img = load_image(image_path)\n\n        # create a bounding box with the data & draw it\n        if img_nm in boxes_dictionary:\n            for box in boxes_dictionary[img_nm]:\n                box_id, x_min, y_min, x_max, y_max = box\n                x_min, y_min, x_max, y_max = int(x_min), int(y_max), int(x_max), int(y_min)\n                # (left top), (right, bottom) == (XMIN, YMIN), (XMAX, YMAX)\n                cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0,255,0), 3)\n                if text:\n                    if class_map:\n                        box_label = class_map[box_id]\n                    else:\n                        box_label = str(box_id)\n                    cv2.putText(img, box_label, (x_min, y_max-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (36,255,12), 4)\n\n        # Show image with bboxes\n        axs[k].set_title(f\"Image {img_nm}\", fontsize = 12)\n        axs[k].imshow(img)\n        axs[k].set_axis_off()\n\n    # Display all the images\n    plt.tight_layout()\n    plt.show()\n    return\n\n#Convert YOLO to CV2 rectangle (l,t),(r,b)\ndef get_corners(x_cen, y_cen, an_width, an_height, im_width, im_height):\n    x_cen, y_cen, an_width, an_height = float(x_cen), float(y_cen), float(an_width), float(an_height)\n    left = (x_cen - an_width/2)*im_width\n    top = (y_cen - an_height/2)*im_height\n    right = (x_cen + an_width/2)*im_width\n    bottom = (y_cen + an_height/2)*im_height\n    return int(left), int(top), int(right), int(bottom)\n\n\ndef display_yolo_images(image_lst, image_fldr, max_images=6, no_cols=1, text=False,  class_map={}):\n    total_ims = len(image_lst)\n    display_ims = min(max_images, total_ims)\n    no_rows = display_ims//no_cols + (display_ims % no_cols > 0)\n    _, axs = plt.subplots(no_rows, no_cols, figsize=(10, 10*no_rows/no_cols*3/4))\n    axs = axs.flatten()\n    for k, img_nm in enumerate(image_lst[:display_ims]):\n        image_path = image_fldr / img_nm\n        text_fn = image_path.stem + '.txt'\n        boxes_path = image_fldr / text_fn\n        img = load_image(str(image_path))\n        im_h, im_w, _ = img.shape\n        with open(boxes_path) as text_file:\n            annotations = [line.rstrip().split() for line in text_file]\n\n        # create a bounding box with the data & draw it\n        for ann in annotations:\n                class_id = ann[0]\n                x_centre, y_centre, w, h = ann[1], ann[2], ann[3], ann[4]\n                x_min, y_min, x_max, y_max = get_corners(x_centre, y_centre, w, h, im_w, im_h)\n                # (left top), (right bottom) == (XMIN, YMIN), (XMAX, YMAX)\n                cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0,255,0), 3)\n                if text:\n                    if class_map:\n                        box_label = class_map[int(class_id)]\n                    else:\n                        box_label = str(class_id)\n                    cv2.putText(img, box_label, (x_min, y_max-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (36,255,12), 4)\n\n        # Show image with bboxes\n        axs[k].set_title(f\"Image {img_nm}\", fontsize = 12)\n        axs[k].imshow(img)\n        axs[k].set_axis_off()\n\n    # Display all the images\n    plt.tight_layout()\n    plt.show()\n    return\n\n\n# For a given square within a chunk of a larger image, find any boxes in it\n# Return the boxes in YOLO format relative to the chunk boundary\ndef match_boxes(box_list, chnk_lims):\n    boxes_lists = []\n    le, to = chnk_lims[0], chnk_lims[1]  # chunk_limits = [c, r, chunk_w, chunk_h]\n    w, h  = chnk_lims[2], chnk_lims[3]\n    for box in box_list:\n        o_left, o_top, o_right, o_bottom = box[1], box[2], box[3], box[4]\n        left, right = (o_left - le)/w, (o_right - le)/w  # translate and normalise\n        top, bottom = (o_top - to)/h, (o_bottom - to)/h\n\n        h_match = (0 <= left < 1) or (0 < right <= 1)\n        v_match = (0 <= top < 1) or (0 < bottom <= 1)\n\n        if v_match and h_match:\n            clipped = np.clip([left, top, right, bottom], a_min=0, a_max=1)\n            l, t, r, b = clipped[0], clipped[1], clipped[2], clipped[3]\n            bounding_box = [str(box[0]),\n                            str(round((l + r)/2, 5)),\n                            str(round((t + b)/2, 5)),\n                            str(round(r-l, 5)),\n                            str(round(b-t, 5))]\n            boxes_lists.append(bounding_box)\n    return boxes_lists","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:54:35.546420Z","iopub.execute_input":"2024-11-16T18:54:35.546868Z","iopub.status.idle":"2024-11-16T18:54:35.592069Z","shell.execute_reply.started":"2024-11-16T18:54:35.546826Z","shell.execute_reply":"2024-11-16T18:54:35.590809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Extraction and Cleaning","metadata":{}},{"cell_type":"code","source":"with open(labels_json_pth, 'r') as infile:\n    data = json.load(infile)\n    keys = list(data.keys())\n\nfeature_list = data['features']\nCOLUMNS = ['IMAGE_ID', 'TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX', 'LONG', 'LAT']\n\ndata = []\nfor feature in tqdm_notebook(feature_list):\n    properties = feature['properties'] # a dict\n    img_id = properties['image_id']  # '389.tif'\n    type_id = properties['type_id']\n    bbox = properties['bounds_imcoords'].split(\",\")  # eg '1917,38,1958,64'\n    geometry = feature ['geometry']\n    coords = geometry['coordinates'][0] #for some reason it's a list of lists\n    long = coords[0][0] / 2  + coords[2][0] / 2\n    lat = coords[0][1] / 2  + coords[1][1] / 2\n    one_row = [img_id, type_id, bbox[0], bbox[1], bbox[2], bbox[3], long, lat]\n    data.append(one_row)\n\ninstances = len(data)\nprint(f'There are {instances} object instances in the original dataset')","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:54:35.595488Z","iopub.execute_input":"2024-11-16T18:54:35.596265Z","iopub.status.idle":"2024-11-16T18:55:00.044007Z","shell.execute_reply.started":"2024-11-16T18:54:35.596204Z","shell.execute_reply":"2024-11-16T18:55:00.042897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extracting the columns of interest only","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data, columns = COLUMNS)\ndf[['XMIN', 'YMIN', 'XMAX', 'YMAX']] = df[['XMIN', 'YMIN', 'XMAX', 'YMAX']].apply(pd.to_numeric)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:00.045597Z","iopub.execute_input":"2024-11-16T18:55:00.046007Z","iopub.status.idle":"2024-11-16T18:55:04.102658Z","shell.execute_reply.started":"2024-11-16T18:55:00.045966Z","shell.execute_reply":"2024-11-16T18:55:04.101188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing two erroneous annotation class labels (See [EDA of the xView dataset](https://www.kaggle.com/code/ollypowell/xview-dataset-eda))","metadata":{}},{"cell_type":"code","source":"df = df[(df.TYPE_ID != 75) & (df.TYPE_ID != 82)]   # removing erroneous labels\nprint(f'{instances - len(df)} rows removed, leaving {len(df)} rows')","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:04.104644Z","iopub.execute_input":"2024-11-16T18:55:04.105020Z","iopub.status.idle":"2024-11-16T18:55:04.204223Z","shell.execute_reply.started":"2024-11-16T18:55:04.104976Z","shell.execute_reply":"2024-11-16T18:55:04.202641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:04.206376Z","iopub.execute_input":"2024-11-16T18:55:04.206949Z","iopub.status.idle":"2024-11-16T18:55:04.224569Z","shell.execute_reply.started":"2024-11-16T18:55:04.206894Z","shell.execute_reply":"2024-11-16T18:55:04.222948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also removing anything from image 1395, this image does not exist in the dataset","metadata":{}},{"cell_type":"code","source":"old_length = len(df)\ndf = df[df.IMAGE_ID != '1395.tif']\nprint(f'{old_length - len(df)} rows removed, leaving {len(df)}')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:04.226652Z","iopub.execute_input":"2024-11-16T18:55:04.227171Z","iopub.status.idle":"2024-11-16T18:55:04.477959Z","shell.execute_reply.started":"2024-11-16T18:55:04.227117Z","shell.execute_reply":"2024-11-16T18:55:04.476854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also it's useful to convert the type IDs into a continuous sequence from 0 to 59 for the 60 categories.  The original competition labels were not arranged this way. The dictionary below is the original mapping from the competition website.","metadata":{}},{"cell_type":"code","source":"old_dict = {\n    11:'Fixed-wing Aircraft', 12:'Small Aircraft', 13:'Passenger/Cargo Plane', 15:'Helicopter',\n    17:'Passenger Vehicle', 18:'Small Car', 19:'Bus', 20:'Pickup Truck', 21:'Utility Truck',\n    23:'Truck', 24:'Cargo Truck', 25:'Truck Tractor w/ Box Trailer', 26:'Truck Tractor',27:'Trailer',\n    28:'Truck Tractor w/ Flatbed Trailer', 29:'Truck Tractor w/ Liquid Tank', 32:'Crane Truck',\n    33:'Railway Vehicle', 34:'Passenger Car', 35:'Cargo/Container Car', 36:'Flat Car', 37:'Tank car',\n    38:'Locomotive', 40:'Maritime Vessel', 41:'Motorboat', 42:'Sailboat', 44:'Tugboat', 45:'Barge',\n    47:'Fishing Vessel', 49:'Ferry', 50:'Yacht', 51:'Container Ship', 52:'Oil Tanker',\n    53:'Engineering Vehicle', 54:'Tower crane', 55:'Container Crane', 56:'Reach Stacker',\n    57:'Straddle Carrier', 59:'Mobile Crane', 60:'Dump Truck', 61:'Haul Truck', 62:'Scraper/Tractor',\n    63:'Front loader/Bulldozer', 64:'Excavator', 65:'Cement Mixer', 66:'Ground Grader', 71:'Hut/Tent',\n    72:'Shed', 73:'Building', 74:'Aircraft Hangar', 76:'Damaged Building', 77:'Facility', 79:'Construction Site',\n    83:'Vehicle Lot', 84:'Helipad', 86:'Storage Tank', 89:'Shipping container lot', 91:'Shipping Container',\n    93:'Pylon', 94:'Tower'}","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:04.479542Z","iopub.execute_input":"2024-11-16T18:55:04.480547Z","iopub.status.idle":"2024-11-16T18:55:04.491400Z","shell.execute_reply.started":"2024-11-16T18:55:04.480464Z","shell.execute_reply":"2024-11-16T18:55:04.490070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Making a new mapping from 0 to 59","metadata":{}},{"cell_type":"code","source":"old_keys = sorted(list(old_dict.keys()))\nnew_dict = {old_dict[x]:y for y, x in enumerate(old_keys)}\nclass_map_dict = {y:old_dict[x] for y, x in enumerate(old_keys)}\nwith open(out_json_map_pth, \"w\") as json_file:\n    json.dump(class_map_dict, json_file)\nprint(class_map_dict)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:04.497112Z","iopub.execute_input":"2024-11-16T18:55:04.497525Z","iopub.status.idle":"2024-11-16T18:55:04.509533Z","shell.execute_reply.started":"2024-11-16T18:55:04.497485Z","shell.execute_reply":"2024-11-16T18:55:04.508286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can convert the dataframe's old TYPE_IDs to their new values","metadata":{}},{"cell_type":"code","source":"df['TYPE_ID'] = df['TYPE_ID'].apply(lambda x: new_dict[old_dict[x]])\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:04.511594Z","iopub.execute_input":"2024-11-16T18:55:04.512068Z","iopub.status.idle":"2024-11-16T18:55:05.054163Z","shell.execute_reply.started":"2024-11-16T18:55:04.511995Z","shell.execute_reply":"2024-11-16T18:55:05.052844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just to check the data is loading sensibly, I'm displaying a couple of images with any transport related annotations.","metadata":{}},{"cell_type":"code","source":"all_classes = list(class_map_dict.keys())\ntransport_only = [x for x in all_classes if x < 48]\n\nboxes = get_boxes(df, transport_only)\nimages_for_display = random.choices(list(boxes.keys()), k=2)\ndisplay_images(images_for_display, boxes, img_fldr_pth, max_images=2, no_cols=2, text=True, class_map=class_map_dict) #adjust as desired","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:55:05.055863Z","iopub.execute_input":"2024-11-16T18:55:05.056335Z","iopub.status.idle":"2024-11-16T18:56:06.480898Z","shell.execute_reply.started":"2024-11-16T18:55:05.056284Z","shell.execute_reply":"2024-11-16T18:56:06.479427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main Process\n- Break up the large tif files into chunks\n- Save those chunks as jpg files\n- Check if any chunks contain annotations\n- Reformat those annotations into YOLO format: x_center, y_center, width, height (all normalised)\n- Write all annotations in YOLO format to a dictionary with the filename as a key","metadata":{}},{"cell_type":"code","source":"boxes_dict = get_boxes(df) # returns a dict of the form {filename:[['TYPE_ID', 'XMIN', 'YMIN', 'XMAX', 'YMAX'],[..],[..],..]}","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:56:06.483267Z","iopub.execute_input":"2024-11-16T18:56:06.483721Z","iopub.status.idle":"2024-11-16T18:58:21.242148Z","shell.execute_reply.started":"2024-11-16T18:56:06.483679Z","shell.execute_reply":"2024-11-16T18:58:21.240562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_image(img_fn, \n                  dir_pth=img_fldr_pth, \n                  boxes=boxes_dict, \n                  out_dir=save_images_fldr_pth, \n                  c_height=CHUNK_HEIGHT, \n                  c_width=CHUNK_WIDTH,  \n                  jpg_q=JPEG_COMPRESSION,\n                  min_h=MIN_CHUNK_HEIGHT,\n                  min_w=MIN_CHUNK_WIDTH,\n                  writing=IMAGE_WRITING\n                 ):\n    \n    labels_list = boxes[img_fn]\n    img_pth = str(dir_pth / img_fn)\n    im = load_bgr_image(img_pth)\n    full_h, full_w, _ = im.shape\n    y_boxes= {}\n    f_names, widths, heights = [], [], []\n    \n    for r in range(0, full_h, c_height):\n        for c in range(0, full_w, c_width):\n            stem = img_fn.split('.')[0]\n            fn = str(f\"img_{stem}_{r}_{c}.jpg\")\n            out_pth = str(out_dir / fn)\n            width = c_width\n            height = c_height\n            if r + height > full_h:\n                height = full_h - r\n            if c + width > full_w:\n                width = full_w - c\n            big_enough = (height >= min_h) and (width >= min_w)\n            if big_enough:\n                if writing:\n                    cv2.imwrite(out_pth, im[r:r+height, c:c+height,:],  [int(cv2.IMWRITE_JPEG_QUALITY), jpg_q])\n                # Find any boxes occurring in the chunk, and convert to YOLO format\n                chunk_limits = [c, r, width, height]\n                y_boxes[fn] = match_boxes(labels_list, chunk_limits)\n                f_names.append(fn)\n                widths.append(width)\n                heights.append(height)\n    return f_names, widths, heights, y_boxes","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:58:21.244341Z","iopub.execute_input":"2024-11-16T18:58:21.244876Z","iopub.status.idle":"2024-11-16T18:58:21.260147Z","shell.execute_reply.started":"2024-11-16T18:58:21.244819Z","shell.execute_reply":"2024-11-16T18:58:21.258663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_fns = df.IMAGE_ID.unique().tolist()\nif DEBUG:\n    img_fns = img_fns[:len(img_fns)//120]\n    df = df[df['IMAGE_ID'].isin(img_fns)]","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:58:21.261575Z","iopub.execute_input":"2024-11-16T18:58:21.262056Z","iopub.status.idle":"2024-11-16T18:58:21.468005Z","shell.execute_reply.started":"2024-11-16T18:58:21.262014Z","shell.execute_reply":"2024-11-16T18:58:21.466689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\nnum_threads = mp.cpu_count() \noverall_progress = tqdm_notebook(total=len(img_fns), desc=\"Creating and saving image tiles\")\nyolo_boxes= {}\nfile_names, widths, heights = [], [], []\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n    for f_names, c_widths, c_heights, y_boxes in executor.map(process_image, img_fns):\n        file_names.extend(f_names)\n        widths.extend(c_widths)\n        heights.extend(c_heights)\n        yolo_boxes.update(y_boxes)\n        overall_progress.update(1)\noverall_progress.close()\n\nimage_data = {file_names[i]: [widths[i], heights[i]] for i in range(len(file_names))}\ntime_taken=time.time() - start_time","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:58:21.469643Z","iopub.execute_input":"2024-11-16T18:58:21.470069Z","iopub.status.idle":"2024-11-16T19:01:47.994114Z","shell.execute_reply.started":"2024-11-16T18:58:21.470028Z","shell.execute_reply":"2024-11-16T19:01:47.992702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Write the new YOLO formatted text files\nIterate through the dictionary, creating a text file for each image:  class x y width height, then save the completed text file to the same location, with the same stem name as the image.","metadata":{}},{"cell_type":"code","source":"all_image_files = os.listdir(save_images_fldr_pth)\nfor image_fn in tqdm_notebook(all_image_files):\n    stem = image_fn.split('.')[0]\n    fn = str (stem) + '.txt'\n    txt_pth = str(save_images_fldr_pth / fn)\n    seperator = ' '\n    with open(txt_pth, 'a') as f:\n        if image_fn in yolo_boxes:\n            for bbox in yolo_boxes[image_fn]:\n                txt = seperator.join(bbox) + '\\n'\n                f.write(txt)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:01:47.995986Z","iopub.execute_input":"2024-11-16T19:01:47.996401Z","iopub.status.idle":"2024-11-16T19:01:54.388395Z","shell.execute_reply.started":"2024-11-16T19:01:47.996359Z","shell.execute_reply":"2024-11-16T19:01:54.387151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt_files = [file for file in os.listdir(save_images_fldr_pth) if file.endswith('.txt')]\nnum_txt_files = len(txt_files)\nprint(f\"Numero di file .txt: {num_txt_files}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:01:54.390059Z","iopub.execute_input":"2024-11-16T19:01:54.390457Z","iopub.status.idle":"2024-11-16T19:01:54.537225Z","shell.execute_reply.started":"2024-11-16T19:01:54.390414Z","shell.execute_reply":"2024-11-16T19:01:54.535762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_with_boxes = [image_fn for image_fn in all_image_files if image_fn in yolo_boxes]\nprint(f\"Numero di immagini con dati in yolo_boxes: {len(images_with_boxes)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:01:54.539255Z","iopub.execute_input":"2024-11-16T19:01:54.539724Z","iopub.status.idle":"2024-11-16T19:01:54.579361Z","shell.execute_reply.started":"2024-11-16T19:01:54.539681Z","shell.execute_reply":"2024-11-16T19:01:54.577801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_paths = [save_images_fldr_pth / x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".txt\")]\ncolumn_names = ['Class_ID', 'x_center', 'y_center', 'width', 'height']\ndata = []\nfor file_path in text_paths:\n    with open(file_path, 'r') as file:\n        for line in file:\n            values = line.strip().split(' ')\n            row_data = {col: val for col, val in zip(column_names, values)}\n            row_data['File_Name'] = file_path.name\n            data.append(row_data)\n\nout_df = pd.DataFrame(data)\nout_df['Class_ID']=out_df['Class_ID'].astype(int)\nout_df['Class_Name'] = out_df['Class_ID'].map(class_map_dict).fillna('unknown')\nout_df = out_df[['File_Name', 'Class_Name', 'Class_ID', 'x_center', 'y_center', 'width', 'height']]\nout_df.to_parquet(out_data_parquet_pth, index=False)\nout_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:01:54.581577Z","iopub.execute_input":"2024-11-16T19:01:54.582697Z","iopub.status.idle":"2024-11-16T19:02:04.150147Z","shell.execute_reply.started":"2024-11-16T19:01:54.582581Z","shell.execute_reply":"2024-11-16T19:02:04.148861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rimozione immagini senza label","metadata":{}},{"cell_type":"code","source":"def remove_empty(image_folder, yolo_boxes, image_data):\n    \"\"\"\n    Rimuove file di annotazione vuoti (.txt) e le immagini corrispondenti.\n    Aggiorna anche i dizionari yolo_boxes e image_data per rimuovere le voci corrispondenti.\n    \n    Args:\n        image_folder (str): Percorso della cartella contenente immagini e file .txt.\n        yolo_boxes (dict): Dizionario con annotazioni YOLO.\n        image_data (dict): Dizionario con metadati delle immagini.\n    \n    Returns:\n        tuple: Dizionari aggiornati (yolo_boxes, image_data).\n    \"\"\"\n    all_image_files = set(os.listdir(image_folder))  # Set per confronti più veloci\n    files_to_remove = []  # Traccia dei file da rimuovere (nomi completi)\n\n    # Controlla i file .txt per annotazioni vuote\n    for txt_file in all_image_files:\n        if txt_file.endswith('.txt'):\n            txt_path = os.path.join(image_folder, txt_file)\n            # Controlla se il file è vuoto o contiene solo spazi\n            with open(txt_path, 'r') as file:\n                content = file.read().strip()\n            if not content:  # Vuoto o solo spazi\n                # Determina il file immagine corrispondente\n                image_file = txt_file.replace('.txt', '.jpg')  # Cambia estensione a .jpg\n                files_to_remove.append(image_file)  # Aggiungi immagine alla lista\n                os.remove(txt_path)  # Rimuovi file .txt\n                # Rimuovi anche il file immagine, se esiste\n                image_path = os.path.join(image_folder, image_file)\n                if os.path.exists(image_path):\n                    os.remove(image_path)\n\n    # Aggiorna yolo_boxes eliminando i file rimossi\n    yolo_boxes = {key: value for key, value in yolo_boxes.items() if key not in files_to_remove}\n\n    # Aggiorna image_data eliminando i file rimossi\n    image_data = {key: value for key, value in image_data.items() if key not in files_to_remove}\n\n    return yolo_boxes, image_data\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:02:04.151808Z","iopub.execute_input":"2024-11-16T19:02:04.152184Z","iopub.status.idle":"2024-11-16T19:02:04.164281Z","shell.execute_reply.started":"2024-11-16T19:02:04.152146Z","shell.execute_reply":"2024-11-16T19:02:04.162865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yolo_boxes, image_data = remove_empty(save_images_fldr_pth, yolo_boxes, image_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:02:04.166150Z","iopub.execute_input":"2024-11-16T19:02:04.166570Z","iopub.status.idle":"2024-11-16T19:03:35.811303Z","shell.execute_reply.started":"2024-11-16T19:02:04.166526Z","shell.execute_reply":"2024-11-16T19:03:35.809770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verifica il numero di file .txt nel folder\ntxt_files = [f for f in os.listdir(save_images_fldr_pth) if f.endswith('.txt')]\nprint(f\"Numero di file .txt nel folder: {len(txt_files)}\")\n# Conta il numero di file immagine (per esempio file .jpg)\nimage_files = [f for f in os.listdir(save_images_fldr_pth) if f.endswith('.jpg')]\nnum_images = len(image_files)\nprint(f\"Numero di immagini nel folder: {num_images}\")\nimages_with_boxes = [image_fn for image_fn in all_image_files if image_fn in yolo_boxes]\nprint(f\"Numero di immagini con dati in yolo_boxes: {len(images_with_boxes)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:35.813206Z","iopub.execute_input":"2024-11-16T19:03:35.813667Z","iopub.status.idle":"2024-11-16T19:03:35.968702Z","shell.execute_reply.started":"2024-11-16T19:03:35.813596Z","shell.execute_reply":"2024-11-16T19:03:35.967300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conta il numero di immagini rimaste in image_data\nprint(f\"Numero di immagini rimaste in image_data: {len(image_data)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:35.970590Z","iopub.execute_input":"2024-11-16T19:03:35.971106Z","iopub.status.idle":"2024-11-16T19:03:35.977788Z","shell.execute_reply.started":"2024-11-16T19:03:35.971053Z","shell.execute_reply":"2024-11-16T19:03:35.976228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames = [x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".jpg\")]\nimage_list = random.choices(filenames, k=4)\ndisplay_yolo_images(image_list, save_images_fldr_pth, max_images=4, no_cols=2, text=True,  class_map=class_map_dict)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:35.979364Z","iopub.execute_input":"2024-11-16T19:03:35.979873Z","iopub.status.idle":"2024-11-16T19:03:36.912331Z","shell.execute_reply.started":"2024-11-16T19:03:35.979821Z","shell.execute_reply":"2024-11-16T19:03:36.910831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#without text labels\nfilenames = [x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".jpg\")]\nimage_list = random.choices(filenames, k=4)\ndisplay_yolo_images(image_list, save_images_fldr_pth, max_images=4, no_cols=2, text=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:36.914098Z","iopub.execute_input":"2024-11-16T19:03:36.914485Z","iopub.status.idle":"2024-11-16T19:03:37.881358Z","shell.execute_reply.started":"2024-11-16T19:03:36.914445Z","shell.execute_reply":"2024-11-16T19:03:37.879735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting","metadata":{}},{"cell_type":"code","source":"total_images = len(filenames)\nindices = list(range(total_images))\nrandom.shuffle(indices)\n\ntrain_fraction = 1 - TEST_FRACTION - VAL_FRACTION\ntrain_sp = int(np.floor(train_fraction * len(indices))) # The training-validation split\nvalid_sp = int(np.floor(VAL_FRACTION * len(indices))) + train_sp # The validation-test split\ntrain_idx, val_idx, test_idx = indices[:train_sp], indices[train_sp:valid_sp], indices[valid_sp:]\n\nprint(' Training set size: \\t', len(train_idx))\nprint(' Validation set size: \\t', len(val_idx))\nprint(' Test set size: \\t', len(test_idx))\nprint(' Total dataset: \\t', total_images)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:37.883845Z","iopub.execute_input":"2024-11-16T19:03:37.884325Z","iopub.status.idle":"2024-11-16T19:03:37.937893Z","shell.execute_reply.started":"2024-11-16T19:03:37.884272Z","shell.execute_reply":"2024-11-16T19:03:37.936309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write 3 text files into the Data folder with the file paths: train.txt, val.txt, test.txt  These are lists of absolute filepaths to the images, one line each path.  They can reside anywhere just so long as the relative paths in xview_yolo.yaml points to them.","metadata":{}},{"cell_type":"code","source":"files = ['train.txt', 'val.txt', 'test.txt']\nsplits = [train_idx, val_idx, test_idx]\n\nfor fn, split in zip(files, splits):\n    txt_pth = cfg_fldr_pth / fn\n    with open(txt_pth, 'a') as f:\n        for ind in split:\n            f.write(str(future_ds_img_fldr / filenames[ind]) + '\\n')\n        print(f'{fn[:-4]} file written to {txt_pth}, with {len(split) } samples')","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:37.954109Z","iopub.execute_input":"2024-11-16T19:03:37.954548Z","iopub.status.idle":"2024-11-16T19:03:38.311800Z","shell.execute_reply.started":"2024-11-16T19:03:37.954504Z","shell.execute_reply":"2024-11-16T19:03:38.310420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## YAML File\nWrite a .yaml file pointing to the text file locations, and determining class names, number of categories location.\nThis is good practice, it means I don't need to move all the image files around just to change the training splits.\nAlso the .yml file gets updated automatically if anybody changes something like the number of classes.","metadata":{}},{"cell_type":"code","source":"config = {'train': str(future_ds_cfg_fldr / files[0]),\n          'val': str(future_ds_cfg_fldr / files[1]),\n          'test': str(future_ds_cfg_fldr / files[2]),\n          'nc': len(class_map_dict),\n          'names': class_map_dict\n          }\n\nwith open(yolo_yaml_pth, \"w\") as file:\n    yaml.dump(config, file, default_style=None, default_flow_style=False, sort_keys=False)\nprint(f'yaml file written to {yolo_yaml_pth}')","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:38.313506Z","iopub.execute_input":"2024-11-16T19:03:38.313959Z","iopub.status.idle":"2024-11-16T19:03:38.330696Z","shell.execute_reply.started":"2024-11-16T19:03:38.313918Z","shell.execute_reply":"2024-11-16T19:03:38.329295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Check\nJust checking the first few lines of the train.txt file","metadata":{}},{"cell_type":"code","source":"for split in ['train', 'val', 'test']:\n    print(f'{split} text file')\n    print_first_n_lines(cfg_fldr_pth / f'{split}.txt', 2)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:38.332714Z","iopub.execute_input":"2024-11-16T19:03:38.333208Z","iopub.status.idle":"2024-11-16T19:03:38.344300Z","shell.execute_reply.started":"2024-11-16T19:03:38.333163Z","shell.execute_reply":"2024-11-16T19:03:38.342924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the .yaml file","metadata":{}},{"cell_type":"code","source":"print_first_n_lines(yolo_yaml_pth, 10)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:38.345960Z","iopub.execute_input":"2024-11-16T19:03:38.346391Z","iopub.status.idle":"2024-11-16T19:03:38.357463Z","shell.execute_reply.started":"2024-11-16T19:03:38.346349Z","shell.execute_reply":"2024-11-16T19:03:38.356002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And a couple of annotations files","metadata":{}},{"cell_type":"code","source":"txt_fnames = [save_images_fldr_pth / x for x in os.listdir(save_images_fldr_pth) if x.endswith(\".txt\")]\ntext_list = random.choices(txt_fnames, k=2)\nprint(text_list)\nfor text_f in text_list:\n    print(f'Reading {text_f}')\n    print_first_n_lines(text_f, 3)  ","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:38.359270Z","iopub.execute_input":"2024-11-16T19:03:38.359824Z","iopub.status.idle":"2024-11-16T19:03:38.683050Z","shell.execute_reply.started":"2024-11-16T19:03:38.359781Z","shell.execute_reply":"2024-11-16T19:03:38.681606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(text_list)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:38.684890Z","iopub.execute_input":"2024-11-16T19:03:38.685261Z","iopub.status.idle":"2024-11-16T19:03:38.692102Z","shell.execute_reply.started":"2024-11-16T19:03:38.685222Z","shell.execute_reply":"2024-11-16T19:03:38.690730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the csv file","metadata":{}},{"cell_type":"code","source":"out_data = pd.read_parquet(out_data_parquet_pth)\nout_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:38.693746Z","iopub.execute_input":"2024-11-16T19:03:38.694118Z","iopub.status.idle":"2024-11-16T19:03:39.036116Z","shell.execute_reply.started":"2024-11-16T19:03:38.694080Z","shell.execute_reply":"2024-11-16T19:03:39.034710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the `.json`","metadata":{}},{"cell_type":"code","source":"with open(out_json_map_pth, \"r\") as json_file:\n    loaded_dict = json.load(json_file)\nprint(loaded_dict)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:39.038283Z","iopub.execute_input":"2024-11-16T19:03:39.038679Z","iopub.status.idle":"2024-11-16T19:03:39.051544Z","shell.execute_reply.started":"2024-11-16T19:03:39.038604Z","shell.execute_reply":"2024-11-16T19:03:39.050035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## YOLO to COCO\nSince it's not too much extra effort at this point, I'll re-format the dataframe into COCO geometry, and write a COCO `.json` file for people with that use in mind.  COCO format is explained [here](https://cocodataset.org/#format-data).  At the top level we mainly need these three objects:\n\nimages:  \n`{\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, }`   \nannotations:  \n`{\"id\": int, \"image_id\": int, \"category_id\": int, \"area\": float, \"bbox\": [x,y,width,height]}`  \ncategories:  \n`[{\"id\": int, \"name\": str}]`","metadata":{}},{"cell_type":"markdown","source":"I'll copy the YOLO DataFrame, obtain the image widths, and create the BBox category:","metadata":{}},{"cell_type":"code","source":"image_data = {'width': widths, 'height' : heights, 'file_name':file_names}\nim_df = pd.DataFrame(image_data)\nim_df['id'] = im_df['file_name'].str.replace(r'\\D', '', regex=True).astype(int)\nim_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:39.053520Z","iopub.execute_input":"2024-11-16T19:03:39.053903Z","iopub.status.idle":"2024-11-16T19:03:39.434639Z","shell.execute_reply.started":"2024-11-16T19:03:39.053866Z","shell.execute_reply":"2024-11-16T19:03:39.433395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_images_in_im_df = len(im_df)\nprint(f\"Numero di immagini in im_df: {num_images_in_im_df}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:39.436402Z","iopub.execute_input":"2024-11-16T19:03:39.436877Z","iopub.status.idle":"2024-11-16T19:03:39.443543Z","shell.execute_reply.started":"2024-11-16T19:03:39.436836Z","shell.execute_reply":"2024-11-16T19:03:39.442347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def row_to_dict(row):\n    return {\n        'id': row['id'],\n        'width': row['width'],\n        'height':row['height'],\n        'file_name':row['file_name']\n    }\n\nim_list = im_df.apply(lambda row: row_to_dict(row), axis=1).tolist()\n[print(val) for val in im_list[:4]]","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:39.445357Z","iopub.execute_input":"2024-11-16T19:03:39.445920Z","iopub.status.idle":"2024-11-16T19:03:41.496081Z","shell.execute_reply.started":"2024-11-16T19:03:39.445866Z","shell.execute_reply":"2024-11-16T19:03:41.494614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge the images dataframe with the annotations to work out the absolute pixel values, plus a bit more re-organising.","metadata":{}},{"cell_type":"code","source":"annotations_df = out_data.copy()\nannotations_df['image_id'] = annotations_df['File_Name'].str.replace(r'\\D', '', regex=True).astype(int)\nannotations_df= annotations_df.rename(columns={'height': 'h', 'width': 'w'})\nan_df = annotations_df.merge(im_df, left_on='image_id', right_on='id', how='left')\nan_df['x_center']= (an_df['x_center'].astype(np.float64)*an_df['width']).round(decimals=0)\nan_df['y_center']= (an_df['y_center'].astype(np.float64)*an_df['height']).round(decimals=0)\nan_df['w']= (an_df['w'].astype(np.float64)*an_df['width']).round(decimals=0)\nan_df['h']= (an_df['h'].astype(np.float64)*an_df['height']).round(decimals=0)\nan_df['Class_ID']= an_df['Class_ID'].astype(int)\nan_df = an_df.drop(columns=['File_Name', 'file_name', 'width', 'height', 'id'])\nan_df['left'] = (an_df['x_center'] - an_df['w']/2).round(decimals=0)\nan_df['top'] =  (an_df['y_center'] - an_df['h']/2).round(decimals=0)\nan_df['bbox'] = ('[' + an_df['left'].astype(str) + ', ' \n              + an_df['top'].astype(str) + ', ' \n              + an_df['w'].astype(str) + ', '\n              + an_df['h'].astype(str) + ']')\nan_df['area'] = an_df['w'] * an_df['h']\nan_df = an_df.drop(columns=['x_center', 'y_center', 'w', 'h', 'left', 'top', 'Class_Name'])\nan_df.reset_index(inplace=True)\nan_df.rename(columns={'index': 'id'}, inplace=True)\nan_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:41.497716Z","iopub.execute_input":"2024-11-16T19:03:41.498150Z","iopub.status.idle":"2024-11-16T19:03:48.389126Z","shell.execute_reply.started":"2024-11-16T19:03:41.498107Z","shell.execute_reply":"2024-11-16T19:03:48.387723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def row_to_dict(row):\n    return {\n        'id': row['id'],\n        'image_id' : row['image_id'],\n        'category_id': row['Class_ID'],\n        'area':row['area'],\n        'bbox':row['bbox']\n    }\n\nan_list = an_df.apply(lambda row: row_to_dict(row), axis=1).tolist()\nprint(an_list[:4])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:03:48.390785Z","iopub.execute_input":"2024-11-16T19:03:48.391200Z","iopub.status.idle":"2024-11-16T19:04:10.812903Z","shell.execute_reply.started":"2024-11-16T19:03:48.391159Z","shell.execute_reply":"2024-11-16T19:04:10.811313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The category mapping is just about in a convenient format already","metadata":{}},{"cell_type":"code","source":"cat_list = [{key:val} for key,val in class_map_dict.items()]\nprint(cat_list[:4])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:04:10.814978Z","iopub.execute_input":"2024-11-16T19:04:10.815510Z","iopub.status.idle":"2024-11-16T19:04:10.823368Z","shell.execute_reply.started":"2024-11-16T19:04:10.815447Z","shell.execute_reply":"2024-11-16T19:04:10.821857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I just need to combine the top level objects, and save to `.json`","metadata":{}},{"cell_type":"code","source":"out_json_data = {'images': im_list, 'annotations': an_list, 'categories': cat_list}\nwith open(coco_json_pth, 'w') as json_file:\n    json.dump(out_json_data, json_file, indent=4)\n    \nfor key, value in out_json_data.items():\n    print(key, value[:5])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:04:10.825342Z","iopub.execute_input":"2024-11-16T19:04:10.825822Z","iopub.status.idle":"2024-11-16T19:04:24.049278Z","shell.execute_reply.started":"2024-11-16T19:04:10.825779Z","shell.execute_reply":"2024-11-16T19:04:24.047876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Image Sizes","metadata":{}},{"cell_type":"code","source":"def check_image_sizes(directory_path):\n    size_counts = defaultdict(int)\n\n    # Ottieni la lista dei file nella cartella\n    files = [f for f in os.listdir(directory_path) if f.endswith(('.jpg'))]\n\n    # Aggiungi una barra di progresso per iterare sui file\n    for filename in tqdm(files, desc=\"Processing images\"):\n        img_path = os.path.join(directory_path, filename)\n        with Image.open(img_path) as img:\n            size = img.size  # (width, height)\n            size_counts[size] += 1\n\n    # Crea gruppi per le dimensioni richieste\n    size_groups = {\n        'Smaller than 320x320': [],\n        '320x320': [],\n        'Larger than 320x320': [],\n    }\n\n    # Aggiungi le dimensioni agli appropriate gruppi\n    for size, count in size_counts.items():\n        width, height = size\n        if width < 320 and height < 320:\n            size_groups['Smaller than 320x320'].append((size, count))\n        elif width == 320 and height == 320:\n            size_groups['320x320'].append((size, count))\n        elif width > 320 and height > 320:\n            size_groups['Larger than 320x320'].append((size, count))\n\n    # Ordina le dimensioni per area (larghezza * altezza) in ordine decrescente\n    for group, items in size_groups.items():\n        sorted_items = sorted(items, key=lambda x: x[0][0] * x[0][1], reverse=True)  # ordina per area\n        size_groups[group] = sorted_items\n\n    # Stampa i gruppi con il numero di immagini per dimensione e il totale per intervallo\n    for group, items in size_groups.items():\n        total = sum(count for _, count in items)\n        print(f\"{group} (Totale: {total} immagini):\")\n        for size, count in items:\n            print(f\"  Dimensione {size}: {count} immagini\")\n        print()\n\n# Esegui la funzione con il percorso della cartella\ncheck_image_sizes(save_images_fldr_pth)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:04:24.051191Z","iopub.execute_input":"2024-11-16T19:04:24.051642Z","iopub.status.idle":"2024-11-16T19:04:32.071152Z","shell.execute_reply.started":"2024-11-16T19:04:24.051573Z","shell.execute_reply":"2024-11-16T19:04:32.069984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(coco_json_pth, 'r') as f:\n    coco_data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:04:32.072664Z","iopub.execute_input":"2024-11-16T19:04:32.073028Z","iopub.status.idle":"2024-11-16T19:04:34.314062Z","shell.execute_reply.started":"2024-11-16T19:04:32.072990Z","shell.execute_reply":"2024-11-16T19:04:34.312735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_image_size(image, img_id, coco_data):\n    # Trova le dimensioni dell'immagine nel JSON\n    image_info = next((img for img in coco_data['images'] if img['id'] == img_id), None)\n    if image_info:\n        width, height = image_info['width'], image_info['height']\n        img_width, img_height = image.size\n        # Lancia un'eccezione solo se le dimensioni non corrispondono\n        assert img_width == width and img_height == height, (\n            f\"Dimensioni errate: JSON ({width}, {height}), immagine ({img_width}, {img_height})\"\n        )\n    else:\n        raise ValueError(f\"Immagine con ID {img_id} non trovata nel JSON.\")\n\ndef check_all_images(folder_path, coco_data):\n    folder_path = Path(folder_path)\n    errors_found = False  # Flag per tenere traccia degli errori\n    check_count = 0  # Conta il numero di immagini verificate\n    \n    # Itera attraverso tutti i file nella cartella\n    for image_path in folder_path.iterdir():\n        if image_path.is_file() and image_path.suffix in ['.jpg']:  # Controlla solo file immagine\n            check_count += 1  # Incrementa il contatore delle immagini\n            # Trova l'ID corrispondente basato sul nome file\n            img_id = int(''.join(filter(str.isdigit, image_path.stem)))  # Estrae i numeri dal nome\n            try:\n                with Image.open(image_path) as img:\n                    check_image_size(img, img_id, coco_data)\n            except (AssertionError, ValueError) as e:\n                errors_found = True\n                print(f\"Errore per immagine {image_path.name}: {e}\")\n            except Exception as e:\n                errors_found = True\n                print(f\"Errore generico per immagine {image_path.name}: {e}\")\n\n    # Stampa il risultato finale\n    if not errors_found:\n        print(f\"Check completato, nessun errore trovato. Numero di immagini verificate: {check_count}\")\n    else:\n        print(f\"Check completato con errori. Numero di immagini verificate: {check_count}\")\n\n# Percorso alla directory delle immagini\ncheck_all_images(save_images_fldr_pth, coco_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:04:34.315738Z","iopub.execute_input":"2024-11-16T19:04:34.316149Z","iopub.status.idle":"2024-11-16T19:06:51.350236Z","shell.execute_reply.started":"2024-11-16T19:04:34.316107Z","shell.execute_reply":"2024-11-16T19:06:51.348598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Labels","metadata":{}},{"cell_type":"code","source":"def check_empty_txt_files(directory_path):\n    # Ottieni tutti i file .txt nella cartella\n    txt_files = [f for f in os.listdir(directory_path) if f.endswith(\".txt\")]\n    \n    empty_files_count = 0\n    total_lines = 0\n    non_empty_files_count = 0\n    \n    # Controlla se ogni file è vuoto\n    for txt_file in txt_files:\n        file_path = os.path.join(directory_path, txt_file)\n        if os.path.getsize(file_path) == 0:\n            empty_files_count += 1\n        else:\n            non_empty_files_count += 1\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n                total_lines += len(lines)\n    \n    # Calcola la media delle righe per i file non vuoti\n    avg_lines = total_lines / non_empty_files_count if non_empty_files_count > 0 else 0\n    \n    # Stampa il numero di file vuoti e la media delle righe nei file non vuoti\n    print(f\"Numero di file .txt vuoti: {empty_files_count}\")\n    print(f\"Numero di file .txt non vuoti: {non_empty_files_count}\")\n    print(f\"Media delle righe per file non vuoto: {avg_lines:.2f}\")\n\ncheck_empty_txt_files(save_images_fldr_pth)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:06:51.352680Z","iopub.execute_input":"2024-11-16T19:06:51.353240Z","iopub.status.idle":"2024-11-16T19:06:52.865595Z","shell.execute_reply.started":"2024-11-16T19:06:51.353182Z","shell.execute_reply":"2024-11-16T19:06:52.864303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CHECK CATEGORIES","metadata":{}},{"cell_type":"code","source":"# Crea un dizionario per mappare category_id a categoria\ncategory_mapping = {str(index): list(category.values())[0] for index, category in enumerate(coco_data['categories'])}\n\n# Estrai i category_id dalle annotazioni\ncategory_ids = [annotation['category_id'] for annotation in coco_data['annotations']]\n\n# Conta le occorrenze di ogni category_id\ncategory_counts = Counter(category_ids)\n\n# Gruppi delle categorie\ncategory_groups = {\n    \"Velivoli\": [0, 1, 2, 3],\n    \"Veicoli terrestri\": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n    \"Veicoli speciali\": [16],\n    \"Veicoli ferroviari\": [17, 18, 19, 20, 21, 22],\n    \"Imbarcazioni\": [23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n    \"Veicoli da costruzione/industriali\": [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n    \"Edifici e strutture\": [46, 47, 48, 49, 50, 51],\n    \"Altre infrastrutture\": [52, 53, 54, 55, 56, 57, 58, 59]\n}\n\n# Crea un dizionario per le categorie aggregate con le loro occorrenze\ngrouped_occurrences = {}\nfor group_name, category_ids in category_groups.items():\n    # Somma le occorrenze per le categorie di ogni gruppo\n    grouped_occurrences[group_name] = sum(category_counts[cat_id] for cat_id in category_ids)\n\n# Ordina le categorie raggruppate per occorrenze in ordine decrescente\ngrouped_occurrences = dict(sorted(grouped_occurrences.items(), key=lambda item: item[1], reverse=True))\n\n# Prepara i dati per il grafico\ngroups = list(grouped_occurrences.keys())\noccurrences = list(grouped_occurrences.values())\n\n\n# Crea il grafico a barre\nplt.figure(figsize=(10, 6))\nplt.barh(groups, occurrences, color='skyblue')\nplt.xlabel('Numero di Occorrenze')\nplt.ylabel('Gruppi di Categoria')\nplt.title('Distribuzione delle Occorrenze per Gruppo di Categoria')\nplt.tight_layout()\n\n# Mostra il grafico\nplt.show()\n\n# Stampare le occorrenze per ciascun group e i relativi ID\nsorted_groups = sorted(category_groups.items(), key=lambda item: sum(category_counts.get(cat_id, 0) for cat_id in item[1]))\n\nfor group, category_ids in sorted_groups:\n    # Calcola il numero totale di occorrenze per il gruppo\n    total_count = sum(category_counts.get(cat_id, 0) for cat_id in category_ids)\n    \n    print(f\"Gruppo: {group}, Occorrenze Totali: {total_count}\")\n    \n    for cat_id in category_ids:\n        category_name = category_mapping[str(cat_id)]\n        count = category_counts.get(cat_id, 0)\n        print(f\"  category_id: {cat_id}, Categoria: {category_name}, Occorrenze: {count}\")\n    \n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:06:52.867892Z","iopub.execute_input":"2024-11-16T19:06:52.868471Z","iopub.status.idle":"2024-11-16T19:06:53.344065Z","shell.execute_reply.started":"2024-11-16T19:06:52.868412Z","shell.execute_reply":"2024-11-16T19:06:53.342564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATALOADER","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt_file, img_dir, coco_json_file, aug=False):\n        \"\"\"\n        Args:\n            txt_file (str): percorso al file di testo con i nomi delle immagini.\n            img_dir (str): directory contenente le immagini.\n            coco_json_file (str): percorso al file JSON in formato COCO contenente le annotazioni.\n            aug (bool): se True, applica le trasformazioni (augmentation).\n        \"\"\"\n        # Leggi i percorsi delle immagini dal file di testo\n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n\n        self.img_dir = img_dir\n        \n        # Carica il file COCO\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n        \n        # Crea una mappatura delle categorie (category_id -> category_name)\n        self.category_map = {str(i): category[str(i)] for i, category in enumerate(coco_data['categories'])}\n        \n        # Crea un dizionario delle annotazioni (image_id -> [category_ids])\n        self.image_annotations = {}\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = str(annotation['category_id'])  # Assicurati che category_id sia una stringa\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n            self.image_annotations[image_id].append(category_id)\n        \n        # Crea un dizionario per i percorsi delle immagini (id -> file_name)\n        self.image_info = {image['id']: image['file_name'] for image in coco_data['images']}\n        \n        self.coco_data = coco_data  # Salva il JSON completo, se necessario\n\n        # Trasformazioni senza augmentation\n        self.transform = transforms.Compose([\n            transforms.Resize((320, 320)),  # Ridimensiona a 320x320\n            transforms.ToTensor(),         # Converte in tensore\n        ])\n\n        # Trasformazioni con augmentation\n        if aug:\n            self.transform = transforms.Compose([\n                transforms.RandomHorizontalFlip(),    # Flip orizzontale casuale\n                transforms.RandomVerticalFlip(),      # Flip verticale casuale\n                transforms.RandomRotation(degrees=45),# Rotazione casuale fino a ±45 gradi\n                transforms.Resize((320, 320)),        # Ridimensiona uniformemente dopo augmentation\n                transforms.ToTensor(),\n            ])\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        # Ottieni il nome dell'immagine\n        img_name = os.path.basename(self.image_paths[index])\n\n        # Trova l'id dell'immagine nel file COCO\n        img_id = next((img_id for img_id, file_name in self.image_info.items() if file_name == img_name), None)\n        if img_id is None:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n\n        # Carica l'immagine\n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n        image = Image.open(img_path).convert('RGB')\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n\n        # Estrai le categorie associate a questa immagine\n        categories = self.image_annotations.get(img_id, [])\n        label = [self.category_map[cat_id] for cat_id in categories]\n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:06:53.345766Z","iopub.execute_input":"2024-11-16T19:06:53.346141Z","iopub.status.idle":"2024-11-16T19:06:53.365529Z","shell.execute_reply.started":"2024-11-16T19:06:53.346098Z","shell.execute_reply":"2024-11-16T19:06:53.364055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    images, labels = zip(*batch)\n    return list(images), list(labels)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:06:53.367388Z","iopub.execute_input":"2024-11-16T19:06:53.368765Z","iopub.status.idle":"2024-11-16T19:06:53.390329Z","shell.execute_reply.started":"2024-11-16T19:06:53.368715Z","shell.execute_reply":"2024-11-16T19:06:53.388911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_txt_pth, save_images_fldr_pth, coco_json_pth, aug=True)\nvalid_dataset = CustomDataset(val_txt_pth, save_images_fldr_pth, coco_json_pth, aug=False)  # Nessuna augmentazione per la validazione\ntest_dataset = CustomDataset(test_txt_pth, save_images_fldr_pth, coco_json_pth, aug=False)  # Nessuna augmentazione per il test\n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:06:53.392383Z","iopub.execute_input":"2024-11-16T19:06:53.392871Z","iopub.status.idle":"2024-11-16T19:07:02.475989Z","shell.execute_reply.started":"2024-11-16T19:06:53.392828Z","shell.execute_reply":"2024-11-16T19:07:02.474316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_txt_vs_json(txt_path, coco_data):\n    # Estrai i nomi delle immagini dal JSON\n    image_names = [image['file_name'] for image in coco_data['images']]\n    \n    # Leggi e normalizza i nomi delle immagini dal file TXT\n    with open(txt_path, 'r') as f:\n        txt_image_names = [os.path.basename(line.strip()) for line in f.readlines()]\n    \n    # Trova le immagini presenti nel TXT ma non nel JSON\n    missing_in_json = [name for name in txt_image_names if name not in image_names]\n    \n    # Trova le immagini presenti nel JSON ma non nel TXT\n    missing_in_txt = [name for name in image_names if name not in txt_image_names]\n    \n    print(f\"\\nControllo per il file TXT: {txt_path}\")\n    if missing_in_json:\n        print(\"Ci sono immagini del TXT che NON sono presenti nel JSON\")\n    else:\n        print(\"Tutte le immagini del TXT sono presenti nel JSON.\")\n    \n    if missing_in_txt:\n        print(\"Ci sono immagini del JSON che NON sono presenti nel TXT\")\n    else:\n        print(\"Tutte le immagini del JSON sono presenti nel TXT.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:07:02.478175Z","iopub.execute_input":"2024-11-16T19:07:02.478685Z","iopub.status.idle":"2024-11-16T19:07:02.489259Z","shell.execute_reply.started":"2024-11-16T19:07:02.478617Z","shell.execute_reply":"2024-11-16T19:07:02.487531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conta il numero di immagini nel JSON\nnum_images = len(coco_data['images'])\nprint(f\"Numero di immagini nel JSON: {num_images}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:07:02.491691Z","iopub.execute_input":"2024-11-16T19:07:02.492895Z","iopub.status.idle":"2024-11-16T19:07:02.505225Z","shell.execute_reply.started":"2024-11-16T19:07:02.492830Z","shell.execute_reply":"2024-11-16T19:07:02.502775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_txt_vs_json(train_txt_pth, coco_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:07:02.507406Z","iopub.execute_input":"2024-11-16T19:07:02.507908Z","iopub.status.idle":"2024-11-16T19:07:42.366934Z","shell.execute_reply.started":"2024-11-16T19:07:02.507862Z","shell.execute_reply":"2024-11-16T19:07:42.365163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_txt_vs_json(val_txt_pth, coco_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:07:42.369517Z","iopub.execute_input":"2024-11-16T19:07:42.370062Z","iopub.status.idle":"2024-11-16T19:08:22.217722Z","shell.execute_reply.started":"2024-11-16T19:07:42.370006Z","shell.execute_reply":"2024-11-16T19:08:22.216406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_txt_vs_json(test_txt_pth, coco_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:22.219511Z","iopub.execute_input":"2024-11-16T19:08:22.219939Z","iopub.status.idle":"2024-11-16T19:08:36.356093Z","shell.execute_reply.started":"2024-11-16T19:08:22.219898Z","shell.execute_reply":"2024-11-16T19:08:36.354667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_dataloader(loader):\n    missing_images_count = 0\n    for batch in loader:\n        images, labels = batch\n        \n        try:\n            # Gestione di nomi di immagini come liste\n            if isinstance(labels[0], list):  # labels è una lista di liste\n                for label_group in labels:\n                    for label in label_group:\n                        img_name = os.path.basename(label)\n            elif isinstance(labels[0], str):  # labels è una lista di stringhe\n                img_name = os.path.basename(labels[0])\n            else:\n                print(\"Formato delle etichette sconosciuto:\", type(labels[0]))\n\n            # Verifica se ci sono immagini nel batch\n            if images is None or images.shape[0] == 0:\n                missing_images_count += 1\n\n        except Exception as e:\n            print(f\"Errore nel batch: {e}\")\n    \n    print(f\"\\nTotale batch senza immagini: {missing_images_count}\")\n\n\n# Esegui il check sul DataLoader\ncheck_dataloader(train_loader)\ncheck_dataloader(val_loader)\ncheck_dataloader(test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:36.358117Z","iopub.execute_input":"2024-11-16T19:08:36.358639Z","iopub.status.idle":"2024-11-16T19:08:39.077445Z","shell.execute_reply.started":"2024-11-16T19:08:36.358567Z","shell.execute_reply":"2024-11-16T19:08:39.072167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODELLI\nModelli scelti:\n* [R-CNN con backbone AlexNet](https://medium.com/@fractal.ai/guide-to-build-faster-rcnn-in-pytorch-42d47cb0ecd3)\n* [SPPNet con backbone ZF-5](https://arxiv.org/pdf/1406.4729)\n* [Faster R-CNN con backbone VGG16](https://medium.com/@fractal.ai/guide-to-build-faster-rcnn-in-pytorch-42d47cb0ecd3)","metadata":{}},{"cell_type":"markdown","source":"## R-CNN (AlexNet)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nclass RCNNModel(nn.Module):\n    def __init__(self, base_model='alexnet', num_classes=61):  # 60 classi + sfondo\n        super(RCNNModel, self).__init__()\n        # AlexNet pre-addestrata\n        alexnet = models.alexnet(pretrained=True)\n        self.backbone = nn.Sequential(*list(alexnet.children())[:-2])  # Rimuovi i layer Fully Connected (FC)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Pooling adattivo\n        \n        # Classificatore per multi-classe\n        self.classifier = nn.Sequential(\n            nn.Linear(alexnet.classifier[1].in_features, 512),  # Numero di feature dal penultimo layer\n            nn.ReLU(),\n            nn.Linear(512, num_classes)  # Numero di classi\n        )\n\n    def forward(self, x):\n        # Estrai feature dalla backbone\n        x = self.backbone(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        # Classificazione multi-classe\n        class_logits = self.classifier(x)\n        return class_logits\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:39.078684Z","iopub.status.idle":"2024-11-16T19:08:39.079142Z","shell.execute_reply.started":"2024-11-16T19:08:39.078917Z","shell.execute_reply":"2024-11-16T19:08:39.078940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, num_classes, device='cuda'):\n        \"\"\"\n        Classe per il training e la validazione di un modello RCNN.\n        Args:\n            model (nn.Module): il modello da addestrare.\n            train_loader (DataLoader): dataloader per il set di training.\n            val_loader (DataLoader): dataloader per il set di validazione.\n            num_classes (int): numero di classi (incluso lo sfondo).\n            device (str): dispositivo per l'addestramento ('cuda' o 'cpu').\n        \"\"\"\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.num_classes = num_classes\n        \n        # Funzione di perdita per classificazione multi-classe\n        self.criterion = nn.CrossEntropyLoss()\n        \n        # Ottimizzatore\n        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n        \n        # Per tenere traccia della storia di training e validazione\n        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n    \n    def train_one_epoch(self):\n        \"\"\"Esegue un'epoca di training.\"\"\"\n        self.model.train()\n        running_loss = 0.0\n        \n        for images, labels in self.train_loader:\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n            \n            # Forward pass\n            outputs = self.model(images)\n            loss = self.criterion(outputs, labels)\n            \n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)  # Accumula la perdita totale\n        \n        # Calcola la perdita media\n        epoch_loss = running_loss / len(self.train_loader.dataset)\n        return epoch_loss\n    \n    def validate(self):\n        \"\"\"Esegue la validazione sul set di validazione.\"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in self.val_loader:\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                running_loss += loss.item() * images.size(0)\n                \n                # Calcolo accuratezza\n                _, preds = torch.max(outputs, 1)  # Predizioni con probabilità massima\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        \n        # Calcola la perdita media e accuratezza\n        epoch_loss = running_loss / len(self.val_loader.dataset)\n        accuracy = correct / total\n        return epoch_loss, accuracy\n    \n    def train(self, num_epochs):\n        \"\"\"Esegue il training e la validazione per un numero di epoche.\"\"\"\n        for epoch in range(num_epochs):\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n            \n            # Training\n            train_loss = self.train_one_epoch()\n            print(f\"Train Loss: {train_loss:.4f}\")\n            \n            # Validazione\n            val_loss, val_acc = self.validate()\n            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n            \n            # Salva i risultati\n            self.history['train_loss'].append(train_loss)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n    \n    def save_model(self, path):\n        \"\"\"Salva il modello addestrato.\"\"\"\n        torch.save(self.model.state_dict(), path)\n        print(f\"Modello salvato in {path}\")\n    \n    def load_model(self, path):\n        \"\"\"Carica un modello addestrato.\"\"\"\n        self.model.load_state_dict(torch.load(path))\n        self.model.to(self.device)\n        print(f\"Modello caricato da {path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:39.082035Z","iopub.status.idle":"2024-11-16T19:08:39.082683Z","shell.execute_reply.started":"2024-11-16T19:08:39.082333Z","shell.execute_reply":"2024-11-16T19:08:39.082364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef test_model(test_image_path, model, top_k=2000):\n    # Carica l'immagine\n    image = cv2.imread(test_image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Genera region proposals con Selective Search\n    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n    ss.setBaseImage(image)\n    ss.switchToSelectiveSearchFast()\n    ss_results = ss.process()[:top_k]  # Prendi le prime 2000 proposte\n    \n    # Trasforma region proposals in tensor\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n    proposals = []\n    for region in ss_results:\n        x, y, w, h = region\n        cropped = image_rgb[y:y + h, x:x + w]\n        resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n        proposals.append(transform(Image.fromarray(resized)))\n    proposals = torch.stack(proposals)  # Shape: (top_k, 3, 224, 224)\n    \n    # Predizioni del modello\n    model.eval()\n    with torch.no_grad():\n        class_logits = model(proposals)\n        predictions = torch.argmax(class_logits, dim=1).cpu().numpy()  # 0 o 1\n    \n    # Filtra region proposals positive\n    positive_regions = [ss_results[i] for i in range(len(predictions)) if predictions[i] == 1]\n    \n    return positive_regions\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:39.084421Z","iopub.status.idle":"2024-11-16T19:08:39.085043Z","shell.execute_reply.started":"2024-11-16T19:08:39.084724Z","shell.execute_reply":"2024-11-16T19:08:39.084756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Carica il backbone AlexNet\nalexnet = models.alexnet(pretrained=True)\n\n# Costruisci l'architettura R-CNN (personalizza in base alle tue esigenze)\nclass RCNN(nn.Module):\n    def __init__(self, backbone):\n        super(RCNN, self).__init__()\n        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n        # Aggiungi livelli completamente connessi e ROI pooling\n        # Definisci qui ulteriori strati\n        \n    def forward(self, x):\n        x = self.backbone(x)\n        # Implementa forward pass specifico per R-CNN\n        return x\n\nrcnn_model = RCNN(alexnet)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:39.087919Z","iopub.status.idle":"2024-11-16T19:08:39.088529Z","shell.execute_reply.started":"2024-11-16T19:08:39.088207Z","shell.execute_reply":"2024-11-16T19:08:39.088240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SPPNet (ZF-5)","metadata":{}},{"cell_type":"code","source":"'''\nclass SPPNet(nn.Module):\n    def __init__(self):\n        super(SPPNet, self).__init__()\n        # Definisci il modello ZF-5 (convoluzioni e pooling)\n        # Implementa i livelli SPP\n\n    def forward(self, x):\n        # Calcola il forward pass con SPP\n        return x\n\nsppnet_model = SPPNet()\n'''","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:39.090833Z","iopub.status.idle":"2024-11-16T19:08:39.091423Z","shell.execute_reply.started":"2024-11-16T19:08:39.091117Z","shell.execute_reply":"2024-11-16T19:08:39.091148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Faster R-CNN (VGG16)","metadata":{}},{"cell_type":"code","source":"'''\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\n# Usa il backbone VGG16\nvgg16_backbone = models.vgg16(pretrained=True).features\n\n# Carica Faster R-CNN con backbone personalizzato\nfaster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\nfaster_rcnn_model.backbone = vgg16_backbone\n'''","metadata":{"execution":{"iopub.status.busy":"2024-11-16T19:08:39.094039Z","iopub.status.idle":"2024-11-16T19:08:39.094650Z","shell.execute_reply.started":"2024-11-16T19:08:39.094309Z","shell.execute_reply":"2024-11-16T19:08:39.094340Z"},"trusted":true},"execution_count":null,"outputs":[]}]}