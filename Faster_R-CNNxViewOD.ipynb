{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nfrom torchvision import models\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torch import optim\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:16:18.716425Z","iopub.execute_input":"2024-12-09T16:16:18.716893Z","iopub.status.idle":"2024-12-09T16:16:18.724358Z","shell.execute_reply.started":"2024-12-09T16:16:18.716863Z","shell.execute_reply":"2024-12-09T16:16:18.723338Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nOUT_COCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n# PROPOSALS\nOUT_PROPOSALS_FLDR_NM = 'proposals'\nprop_fldr = Path(f'/kaggle/working/{OUT_PROPOSALS_FLDR_NM}')\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\n\n#DATASET\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)\nclean_output(prop_fldr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def init(self, txt_file, img_dir, coco_json_file, aug=False):\n        \"\"\"\n        Inizializza il dataset personalizzato.\n        Args:\n        - txt_file: Il file di testo contenente i percorsi delle immagini.\n        - img_dir: La cartella delle immagini.\n        - coco_json_file: Il file JSON in formato COCO contenente le annotazioni.\n        - aug: Booleano per attivare o meno l'augmentazione.\n        \"\"\"\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n \n        with open(txt_file, 'r') as f:\n            self.image_paths = [line.strip() for line in f.readlines()]\n \n        self.img_dir = img_dir\n \n        # Carica il file JSON delle annotazioni COCO\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n \n        # Crea una struttura per le annotazioni\n        self.image_annotations = {}\n        self.image_bboxes = {}\n \n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox = annotation['bbox']  # Formato COCO [x_min, y_min, width, height]\n \n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n \n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n \n        # Mappa per associare ID immagine a file_name\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n \n        # Trasformazioni di base e di augmentation\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),   \n        ])\n \n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ])\n \n        self.aug = aug\n \n    def len(self):\n        return len(self.image_paths)\n \n    def getitem(self, index):\n        # Estrai il nome dell'immagine e l'ID corrispondente\n        img_name = os.path.basename(self.image_paths[index])\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n        img_path = os.path.join(self.img_dir, img_name)\n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        # Carica l'immagine\n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        # Applica le trasformazioni\n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        # Estrai le annotazioni e i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        if not bboxes:  # Immagini senza annotazioni\n            target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros((0,), dtype=torch.int64)\n            }\n        else:\n            # Converte da formato COCO [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            scale_x = 320 / original_width\n            scale_y = 320 / original_height\n            scaled_bboxes = [\n                torch.tensor([  # [x1, y1, x2, y2]\n                    bbox[0] * scale_x,               # x_min\n                    bbox[1] * scale_y,               # y_min\n                    (bbox[0] + bbox[2]) * scale_x,   # x_max\n                    (bbox[1] + bbox[3]) * scale_y    # y_max\n                ], dtype=torch.float32)\n                for bbox in bboxes\n            ]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di collation per il DataLoader, utile per il batching di immagini e annotazioni.\n    La funzione restituirà un batch di immagini e un batch di target, formattato correttamente per Faster R-CNN.\n    \n    Args:\n    - batch: lista di tuple (image, target)\n    \n    Returns:\n    - images: batch di immagini\n    - targets: lista di dizionari contenenti le annotazioni per ogni immagine\n    \"\"\"\n    # Separa immagini e target\n    images, targets = zip(*batch)\n\n    # Converte la lista di immagini in un batch di immagini\n    images = list(images)\n    \n    # Varia le dimensioni delle immagini per garantire che siano tutte della stessa dimensione\n    # La dimensione di uscita deve essere la stessa per tutte le immagini del batch\n    images = [F.resize(img, (320, 320)) for img in images]  # Assumendo che 320 sia la dimensione target\n\n    # Restituisci il batch\n    return images, list(targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset_frcc = CustomDataset(train_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=True)\nvalid_dataset_frcc = CustomDataset(val_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \ntest_dataset_frcc = CustomDataset(test_txt_pth, save_images_fldr_pth, new_coco_json_pth, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader_frcc = DataLoader(train_dataset_frcc, batch_size=8, shuffle=True, collate_fn=collate_fn)\nval_loader_frcc = DataLoader(valid_dataset_frcc, batch_size=8, shuffle=False, collate_fn=collate_fn)\ntest_loader_frcc = DataLoader(test_dataset_frcc, batch_size=8, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader_frcc.dataset)\nval_size = len(val_loader_frcc.dataset)\ntest_size = len(test_loader_frcc.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader_frcc)\nval_batches = len(val_loader_frcc)\ntest_batches = len(test_loader_frcc)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modello Faster R-CNN (Resnet50)","metadata":{}},{"cell_type":"code","source":"def train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, save_model=True):\n    \"\"\"\n    Funzione per il training e la validazione del modello Faster R-CNN.\n    \n    Args:\n    - model: il modello Faster R-CNN.\n    - train_loader: DataLoader per il training set.\n    - val_loader: DataLoader per il validation set.\n    - optimizer: ottimizzatore per il modello.\n    - device: dispositivo su cui eseguire (es. 'cuda' o 'cpu').\n    - num_epochs: numero di epoche (default: 10).\n    - save_model: se salvare il modello dopo ogni epoca (default: True).\n    \n    Returns:\n    - losses_per_epoch: lista delle perdite per epoca durante il training.\n    \"\"\"\n    model.to(device)\n    losses_per_epoch = []\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoca {epoch + 1}/{num_epochs}\")\n        model.train()\n        total_loss = 0\n\n        # Training loop con tqdm\n        train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n        for images, targets in train_loop:\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Calcola le perdite\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n            # Ottimizzazione\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n            total_loss += losses.item()\n            train_loop.set_postfix(loss=losses.item())\n\n        losses_per_epoch.append(total_loss)\n        print(f\"Perdita Totale per l'epoca {epoch + 1}: {total_loss:.4f}\")\n\n        # Validazione\n        model.eval()\n        val_loop = tqdm(val_loader, desc=\"Validazione\", leave=False)\n        with torch.no_grad():\n            for images, targets in val_loop:\n                images = [img.to(device) for img in images]\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                predictions = model(images)\n                val_loop.set_postfix(processed=len(predictions))  # Placeholder per metriche future\n\n        # Salva il modello\n        if save_model:\n            torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n            print(f\"Modello salvato: model_epoch_{epoch + 1}.pth\")\n\n    return losses_per_epoch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, test_loader, device):\n    \"\"\"\n    Funzione per il testing del modello Faster R-CNN.\n    \n    Args:\n    - model: il modello Faster R-CNN.\n    - test_loader: DataLoader per il test set.\n    - device: dispositivo su cui eseguire (es. 'cuda' o 'cpu').\n    \n    Returns:\n    - predictions: lista delle predizioni per ogni batch.\n    \"\"\"\n    model.to(device)\n    model.eval()\n    predictions = []\n\n    print(\"\\nInizio testing...\")\n    test_loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n    with torch.no_grad():\n        for images, _ in test_loop:  # Durante il test, i target possono essere ignorati\n            images = [img.to(device) for img in images]\n            preds = model(images)\n            predictions.extend(preds)\n            test_loop.set_postfix(processed=len(predictions))\n\n    print(\"Testing completato.\")\n    return predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica il modello Faster R-CNN con ResNet50 e FPN\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n\nnum_classes = 12  # 11 classi + 1 per il background\n\n# Modifica il numero di classi in output\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Imposta il dispositivo (GPU o CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Configurazione training\nnum_epochs = 10\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training e validazione\nlosses_per_epoch = train_and_validate(\n    model=model,\n    train_loader=train_loader_frcc,\n    val_loader=val_loader_frcc,\n    optimizer=optimizer,\n    device=device,\n    num_epochs=num_epochs,\n    save_model=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing\ntest_predictions = test_model(\n    model=model,\n    test_loader=test_loader_frcc,\n    device=device\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}