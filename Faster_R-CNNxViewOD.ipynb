{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nimport shutil\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as TF\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport ast\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T10:14:13.338955Z","iopub.execute_input":"2024-12-10T10:14:13.339339Z","iopub.status.idle":"2024-12-10T10:14:13.359271Z","shell.execute_reply.started":"2024-12-10T10:14:13.339304Z","shell.execute_reply":"2024-12-10T10:14:13.358061Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nCOCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_COCO_JSON_NM = 'mod_COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n#DATASET\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:37:09.974056Z","iopub.status.idle":"2024-12-10T09:37:09.974520Z","shell.execute_reply.started":"2024-12-10T09:37:09.974320Z","shell.execute_reply":"2024-12-10T09:37:09.974342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:37:09.976585Z","iopub.status.idle":"2024-12-10T09:37:09.977172Z","shell.execute_reply.started":"2024-12-10T09:37:09.976867Z","shell.execute_reply":"2024-12-10T09:37:09.976896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:37:09.979003Z","iopub.status.idle":"2024-12-10T09:37:09.979482Z","shell.execute_reply.started":"2024-12-10T09:37:09.979287Z","shell.execute_reply":"2024-12-10T09:37:09.979309Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Background Labels","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato:\n    - Rimappa la categoria \"Aircraft\" da ID 0 a ID 11,\n    - Aggiunge una categoria \"background\" con ID 0 se non esiste,\n    - Aggiunge annotazioni di \"background\" per immagini senza bounding box.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    with open(input_path, 'r') as f:\n        data = json.load(f)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n\n    for category in raw_categories:\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n\n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n\n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n\n    # Aggiorna i category_id nelle annotazioni\n    for annotation in data.get('annotations', []):\n        if annotation['category_id'] == 0:  # Se Ã¨ Aircraft\n            annotation['category_id'] = 11\n        # Correggi il formato del campo bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n\n    # Crea un set per verificare quali immagini hanno annotazioni\n    annotated_images = {ann['image_id'] for ann in data.get('annotations', [])}\n\n    # Lista di nuove annotazioni da aggiungere per immagini senza bbox\n    new_annotations = []\n    for image in data.get('images', []):\n        if image['id'] not in annotated_images:\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, 0.0, image['width'], image['height']],\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n\n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n\n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n\n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:44:56.395939Z","iopub.execute_input":"2024-12-10T09:44:56.396352Z","iopub.status.idle":"2024-12-10T09:44:56.409729Z","shell.execute_reply.started":"2024-12-10T09:44:56.396315Z","shell.execute_reply":"2024-12-10T09:44:56.408381Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:48:41.840383Z","iopub.execute_input":"2024-12-10T09:48:41.840822Z","iopub.status.idle":"2024-12-10T09:48:58.998035Z","shell.execute_reply.started":"2024-12-10T09:48:41.840786Z","shell.execute_reply":"2024-12-10T09:48:58.996997Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def count_bboxes_per_category(json_path):\n    \"\"\"\n    Funzione che conta il numero di bounding box per ciascuna categoria in un file JSON formato COCO.\n    \n    :param json_path: Percorso al file JSON.\n    :return: Dizionario con i nomi delle categorie come chiavi e il conteggio dei bounding box come valori.\n    \"\"\"\n    # Leggi il JSON dal file\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    # Ottieni mapping delle categorie (id -> nome)\n    category_mapping = {cat['id']: cat['name'] for cat in data.get('categories', [])}\n    \n    # Conta i bounding box per ciascun category_id\n    bbox_counts = defaultdict(int)\n    for annotation in data.get('annotations', []):\n        category_id = annotation['category_id']\n        bbox_counts[category_id] += 1\n    \n    # Converti il conteggio usando i nomi delle categorie\n    bbox_counts_named = {category_mapping[cat_id]: count for cat_id, count in bbox_counts.items()}\n\n    return bbox_counts_named","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:49:14.970400Z","iopub.execute_input":"2024-12-10T09:49:14.970801Z","iopub.status.idle":"2024-12-10T09:49:14.978681Z","shell.execute_reply.started":"2024-12-10T09:49:14.970759Z","shell.execute_reply":"2024-12-10T09:49:14.977074Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"bbox_counts = count_bboxes_per_category(new_coco_json_pth)\n\n# Stampa i risultati\nfor category, count in bbox_counts.items():\n    print(f\"Categoria: {category}, Numero di bbox: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:49:27.499119Z","iopub.execute_input":"2024-12-10T09:49:27.499522Z","iopub.status.idle":"2024-12-10T09:49:31.732266Z","shell.execute_reply.started":"2024-12-10T09:49:27.499490Z","shell.execute_reply":"2024-12-10T09:49:31.730999Z"}},"outputs":[{"name":"stdout","text":"Categoria: Building, Numero di bbox: 384947\nCategoria: Passenger Vehicle, Numero di bbox: 225098\nCategoria: Railway Vehicle, Numero di bbox: 4233\nCategoria: Truck, Numero di bbox: 34379\nCategoria: Aircraft, Numero di bbox: 1708\nCategoria: Engineering Vehicle, Numero di bbox: 5473\nCategoria: Storage Tank, Numero di bbox: 2033\nCategoria: Shipping Container, Numero di bbox: 5391\nCategoria: Maritime Vessel, Numero di bbox: 6329\nCategoria: Pylon, Numero di bbox: 470\nCategoria: Helipad, Numero di bbox: 152\nCategoria: background, Numero di bbox: 13692\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"def split_stratified(json_file, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Raggruppare le annotazioni per category_id\n    category_images = defaultdict(list)\n    \n    # Aggiungi solo le annotazioni valide (dove il bbox Ã¨ valido)\n    valid_annotations = []\n    for annotation in data['annotations']:\n        # Converte la stringa del bbox in una lista\n        bbox = annotation['bbox']  \n        x_min, y_min, width, height = bbox\n        x_max = x_min + width\n        y_max = y_min + height\n        \n        if x_min <= x_max and y_min <= y_max:  # Bounding box valido\n            valid_annotations.append(annotation)\n            category_id = annotation['category_id']\n            image_id = annotation['image_id']\n            category_images[category_id].append(image_id)\n    \n    # Genera gli split per ogni category_id\n    train_images, val_images, test_images = set(), set(), set()\n    for category_id, image_ids in category_images.items():\n        # Mescola gli image_id\n        random.shuffle(image_ids)\n        \n        # Calcola i limiti per train, validation, e test\n        total = len(image_ids)\n        train_end = int(total * train_ratio)\n        val_end = int(total * (train_ratio + val_ratio))\n        \n        # Aggiungi agli split\n        train_images.update(image_ids[:train_end])\n        val_images.update(image_ids[train_end:val_end])\n        test_images.update(image_ids[val_end:])\n    \n    # Filtra le immagini e annotazioni per ciascuno split\n    def filter_data(split_images):\n        # Filtra solo le immagini e annotazioni con image_id presente in split_images\n        filtered_images = [image for image in data['images'] if image['id'] in split_images]\n        filtered_annotations = [annotation for annotation in valid_annotations if annotation['image_id'] in split_images]\n        return {'images': filtered_images, 'annotations': filtered_annotations, 'categories': data['categories']}\n    \n    # Crea i nuovi JSON per train, validation, e test\n    train_data = filter_data(train_images)\n    val_data = filter_data(val_images)\n    test_data = filter_data(test_images)\n    \n    # Salva i file JSON\n    with open('train.json', 'w') as f:\n        json.dump(train_data, f, indent=4)\n    \n    with open('val.json', 'w') as f:\n        json.dump(val_data, f, indent=4)\n    \n    with open('test.json', 'w') as f:\n        json.dump(test_data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:52:12.273742Z","iopub.execute_input":"2024-12-10T09:52:12.274205Z","iopub.status.idle":"2024-12-10T09:52:12.286185Z","shell.execute_reply.started":"2024-12-10T09:52:12.274125Z","shell.execute_reply":"2024-12-10T09:52:12.284862Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Chiamata della funzione\nsplit_stratified(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:52:14.134596Z","iopub.execute_input":"2024-12-10T09:52:14.135013Z","iopub.status.idle":"2024-12-10T09:52:49.872137Z","shell.execute_reply.started":"2024-12-10T09:52:14.134978Z","shell.execute_reply":"2024-12-10T09:52:49.870859Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"#Â DataLoader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, coco_json_file, img_dir, aug=False):\n        \"\"\"\n        Inizializza il dataset personalizzato.\n        Args:\n        - coco_json_file: Il file JSON contenente le annotazioni.\n        - img_dir: La cartella delle immagini.\n        - aug: Booleano per attivare o meno l'augmentazione.\n        - save_filtered_json: Se True, salva un file JSON filtrato.\n        - filtered_json_path: Percorso del file JSON filtrato, se `save_filtered_json` Ã¨ True.\n        \"\"\"\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n        \n        # Carica il file JSON delle annotazioni\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n        \n        # Crea una struttura per le annotazioni\n        self.image_annotations = {}\n        self.image_bboxes = {}\n        \n        # Estrai le classi (categorie) dal file JSON\n        self.classes = {}\n        for category in coco_data['categories']:\n            # Associa l'ID della categoria al nome, usando la chiave numerica come ID\n            for category_id, category_name in category.items():\n                self.classes[category_id] = category_name  # Associa l'ID categoria al nome\n        \n        # Filtra le annotazioni con bounding box validi\n        valid_annotations = []\n        for annotation in coco_data['annotations']:\n            # Converti la stringa del bbox in una lista (se Ã¨ una stringa)\n            bbox = annotation['bbox']\n            if isinstance(bbox, str):\n                bbox = json.loads(bbox)  # Converte la stringa in lista\n            x_min, y_min, width, height = bbox\n            x_max = x_min + width\n            y_max = y_min + height\n            \n            if x_min < x_max and y_min < y_max:  # Bounding box valido\n                valid_annotations.append(annotation)\n        \n        # Aggiungi la mappa di annotazioni valide\n        for annotation in valid_annotations:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox = annotation['bbox']  # Formato COCO [x_min, y_min, width, height]\n            \n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n            \n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n        \n        # Mappa per associare ID immagine a file_name\n        self.image_info = {\n            image['id']: image['file_name']\n            for image in coco_data['images']\n        }\n        \n        # Filtra le immagini che non sono presenti nel JSON\n        self.img_dir = img_dir\n        self.image_paths = [\n            os.path.join(img_dir, image['file_name'])\n            for image in coco_data['images']\n            if image['id'] in self.image_info  # Prendi solo le immagini presenti nel JSON\n        ]\n        \n        # Trasformazioni di base e di augmentation\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),   \n        ])\n        \n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) \n        \n        self.aug = aug\n    \n    def save_filtered_json(self, coco_data, valid_annotations, filtered_json_path):\n        \"\"\"\n        Salva un file JSON con annotazioni filtrate.\n        Args:\n        - coco_data: Dati COCO originali.\n        - valid_annotations: Annotazioni valide filtrate.\n        - filtered_json_path: Percorso del file JSON filtrato.\n        \"\"\"\n        filtered_data = {\n            \"images\": coco_data[\"images\"],\n            \"annotations\": valid_annotations,\n            \"categories\": coco_data[\"categories\"]\n        }\n        with open(filtered_json_path, 'w') as f:\n            json.dump(filtered_data, f, indent=4)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        # Estrai il nome dell'immagine e l'ID corrispondente\n        img_path = self.image_paths[index]\n        img_name = os.path.basename(img_path)\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n        \n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        \n        # Carica l'immagine\n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        # Applica le trasformazioni\n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Estrai le annotazioni e i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        \n        if not bboxes:  # Immagini senza annotazioni\n            target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros((0,), dtype=torch.int64)\n            }\n        else:\n            # Converte da formato COCO [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            scale_x = 320 / original_width\n            scale_y = 320 / original_height\n            \n            # Scaling dei bounding boxes\n            scaled_bboxes = []\n            for bbox in bboxes:\n                x_min, y_min, width, height = bbox\n                x_max = x_min + width\n                y_max = y_min + height\n                \n                scaled_bboxes.append(torch.tensor([  \n                    float(x_min) * scale_x,               # x_min\n                    float(y_min) * scale_y,               # y_min\n                    float(x_max) * scale_x,               # x_max\n                    float(y_max) * scale_y                # y_max\n                ], dtype=torch.float32))\n            \n            target = {\n                \"boxes\": torch.stack(scaled_bboxes),\n                \"labels\": torch.tensor(categories, dtype=torch.int64)\n            }\n        \n        return image_tensor, target","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:53:04.446302Z","iopub.execute_input":"2024-12-10T09:53:04.447602Z","iopub.status.idle":"2024-12-10T09:53:04.470311Z","shell.execute_reply.started":"2024-12-10T09:53:04.447546Z","shell.execute_reply":"2024-12-10T09:53:04.468948Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di collation per il DataLoader, utile per il batching di immagini e annotazioni.\n    La funzione restituirÃ  un batch di immagini e un batch di target, formattato correttamente per Faster R-CNN.\n    \n    Args:\n    - batch: lista di tuple (image, target)\n    \n    Returns:\n    - images: batch di immagini\n    - targets: lista di dizionari contenenti le annotazioni per ogni immagine\n    \"\"\"\n    # Separa immagini e target\n    images, targets = zip(*batch)\n\n    # Converte la lista di immagini in un batch di immagini\n    images = list(images)\n\n    # Restituisci il batch\n    return images, list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:53:07.420817Z","iopub.execute_input":"2024-12-10T09:53:07.421264Z","iopub.status.idle":"2024-12-10T09:53:07.427373Z","shell.execute_reply.started":"2024-12-10T09:53:07.421228Z","shell.execute_reply":"2024-12-10T09:53:07.426070Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_path, img_fldr,  aug=True)\nvalid_dataset = CustomDataset(val_path, img_fldr, aug=False)  \ntest_dataset = CustomDataset(test_path, img_fldr, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:53:09.534848Z","iopub.execute_input":"2024-12-10T09:53:09.535248Z","iopub.status.idle":"2024-12-10T09:53:23.722764Z","shell.execute_reply.started":"2024-12-10T09:53:09.535214Z","shell.execute_reply":"2024-12-10T09:53:23.721500Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Funzione per verificare i target con None\ndef check_none_target(data_loader):\n    none_count = 0\n    \n    for images, targets in data_loader:\n        for target in targets:\n            # Controlla se il target Ã¨ vuoto\n            if target == None:  \n                none_count += 1\n    \n    return none_count\n\n# Controlla i target nei DataLoader\ntrain_none_count = check_none_target(train_loader)\nval_none_count = check_none_target(val_loader)\ntest_none_count = check_none_target(test_loader)\n\nprint(f\"Numero di target con None nel train dataset: {train_none_count}\")\nprint(f\"Numero di target con None nel validation dataset: {val_none_count}\")\nprint(f\"Numero di target con None nel test dataset: {test_none_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T09:53:23.724650Z","iopub.execute_input":"2024-12-10T09:53:23.724996Z","iopub.status.idle":"2024-12-10T10:04:25.248821Z","shell.execute_reply.started":"2024-12-10T09:53:23.724962Z","shell.execute_reply":"2024-12-10T10:04:25.247745Z"}},"outputs":[{"name":"stdout","text":"Numero di target con None nel train dataset: 0\nNumero di target con None nel validation dataset: 0\nNumero di target con None nel test dataset: 0\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def validate_dataloader(dataloader):\n    \"\"\"\n    Valida un DataLoader verificando che ogni immagine abbia un target associato\n    e che nessun target sia `None` o vuoto.\n    \n    Args:\n    - dataloader: Il DataLoader da verificare.\n    \n    Returns:\n    - error_messages: Lista di messaggi di errore. Vuota se tutti i dati sono validi.\n    \"\"\"\n    error_messages = []\n    for batch_idx, (images, targets) in enumerate(dataloader):\n        for idx, target in enumerate(targets):\n            if target is None:\n                error_messages.append(f\"Batch {batch_idx}, Immagine {idx}: Target Ã¨ None.\")\n            elif target[\"boxes\"].numel() == 0 or target[\"labels\"].numel() == 0:\n                error_messages.append(\n                    f\"Batch {batch_idx}, Immagine {idx}: Target Ã¨ vuoto o mancano 'boxes'/'labels'.\"\n                )\n    return error_messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T10:05:02.094767Z","iopub.execute_input":"2024-12-10T10:05:02.095270Z","iopub.status.idle":"2024-12-10T10:05:02.103333Z","shell.execute_reply.started":"2024-12-10T10:05:02.095227Z","shell.execute_reply":"2024-12-10T10:05:02.101885Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Validazione del DataLoader di training\ntrain_errors = validate_dataloader(train_loader)\n\nif train_errors:\n    print(\"Errori nel DataLoader di training:\")\n    for error in train_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di training sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T10:05:03.254837Z","iopub.execute_input":"2024-12-10T10:05:03.255303Z","iopub.status.idle":"2024-12-10T10:07:39.508294Z","shell.execute_reply.started":"2024-12-10T10:05:03.255265Z","shell.execute_reply":"2024-12-10T10:07:39.507054Z"}},"outputs":[{"name":"stdout","text":"Tutti i target nel DataLoader di training sono validi.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Validazione del DataLoader di training\nval_errors = validate_dataloader(val_loader)\n\nif val_errors:\n    print(\"Errori nel DataLoader di validation:\")\n    for error in val_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di validation sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T10:08:54.240741Z","iopub.execute_input":"2024-12-10T10:08:54.241183Z","iopub.status.idle":"2024-12-10T10:10:12.095491Z","shell.execute_reply.started":"2024-12-10T10:08:54.241130Z","shell.execute_reply":"2024-12-10T10:10:12.094417Z"}},"outputs":[{"name":"stdout","text":"Tutti i target nel DataLoader di validation sono validi.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Validazione del DataLoader di training\ntest_errors = validate_dataloader(test_loader)\n\nif test_errors:\n    print(\"Errori nel DataLoader di test:\")\n    for error in test_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di test sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T10:12:53.274699Z","iopub.execute_input":"2024-12-10T10:12:53.275225Z","iopub.status.idle":"2024-12-10T10:14:13.327074Z","shell.execute_reply.started":"2024-12-10T10:12:53.275173Z","shell.execute_reply":"2024-12-10T10:14:13.325826Z"}},"outputs":[{"name":"stdout","text":"Tutti i target nel DataLoader di test sono validi.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T10:14:13.328308Z","iopub.execute_input":"2024-12-10T10:14:13.328635Z","iopub.status.idle":"2024-12-10T10:14:13.336623Z","shell.execute_reply.started":"2024-12-10T10:14:13.328602Z","shell.execute_reply":"2024-12-10T10:14:13.335273Z"}},"outputs":[{"name":"stdout","text":"Numero totale di elementi nel train_loader: 42201\nNumero totale di batch nel train_loader: 21101\nNumero totale di elementi nel val_loader: 21033\nNumero totale di batch nel val_loader: 10517\nNumero totale di elementi nel test_loader: 21173\nNumero totale di batch nel test_loader: 10587\nNumero totale di elementi in tutti i DataLoader: 84407\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"# Modello Faster R-CNN (Resnet50)","metadata":{}},{"cell_type":"code","source":"def compute_class_weights(dataset):\n    # Conta la frequenza di ogni classe nel dataset\n    class_counts = np.zeros(len(dataset.classes))  # Non consideriamo lo sfondo, quindi senza +1\n    \n    # Usa tqdm per monitorare il progresso mentre si itera sul dataset\n    for _, targets in tqdm(dataset, desc=\"Calcolo frequenze delle classi\", leave=False):\n        # Controlla se targets Ã¨ None\n        if targets is None:\n            continue\n        \n        # Assicurati che 'labels' sia un array e itera su di esso\n        if 'labels' in targets:\n            for target in targets['labels']:  \n                class_counts[target] += 1\n\n    # Calcola i pesi per le classi (senza lo sfondo)\n    total_count = sum(class_counts)  \n\n    # Calcola i pesi inversamente proporzionali alla frequenza\n    class_weights = np.divide(total_count, class_counts)\n\n    return class_weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_validate(model, train_loader, val_loader, optimizer, device, class_weights, num_epochs=10, save_model=True, num_classes=12):\n    \"\"\"\n    Funzione per il training e la validazione del modello Faster R-CNN con pesi delle classi.\n    \"\"\"\n    model.to(device)\n    losses_per_epoch = []\n    train_accuracies = []\n    val_accuracies = []\n    all_val_scores = defaultdict(list)\n    all_val_preds = defaultdict(list)\n    all_val_labels = defaultdict(list)\n\n    # Assicurati che i pesi siano un tensor PyTorch\n    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoca {epoch + 1}/{num_epochs}\")\n        model.train()\n        total_loss = 0\n\n        # Training loop con tqdm\n        train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n        all_train_preds, all_train_labels = [], []\n        for images, targets in train_loop:\n            if targets is None:\n                targets = [{'boxes': torch.zeros((1, 4), device=device), 'labels': torch.tensor([0], device=device)} for _ in range(len(images))]\n            else:\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            images = [img.to(device) for img in images]\n\n            # Calcola le perdite\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n            # Verifica se ci sono perdite per classe e ponderale\n            for key in loss_dict:\n                if 'loss' in key and loss_dict[key].dim() == 1:  # Se Ã¨ per classe\n                    losses += (loss_dict[key] * class_weights).sum()\n\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n            total_loss += losses.item()\n            train_loop.set_postfix(loss=losses.item())\n\n        losses_per_epoch.append(total_loss)\n        print(f\"Perdita Totale per l'epoca {epoch + 1}: {total_loss:.4f}\")\n\n        # Validazione\n        model.eval()\n        val_loop = tqdm(val_loader, desc=\"Validazione\", leave=False)\n        with torch.no_grad():\n            for images, targets in val_loop:\n                if targets is None:\n                    targets = [{'boxes': torch.zeros((1, 4), device=device), 'labels': torch.tensor([0], device=device)} for _ in range(len(images))]\n                else:\n                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n                images = [img.to(device) for img in images]\n                predictions = model(images)\n\n                pred_labels = [output['labels'].cpu().numpy() for output in predictions]\n                pred_scores = [output['scores'].cpu().numpy() for output in predictions]\n\n                for i, target in enumerate(targets):\n                    all_val_preds[i].extend(pred_labels[i])\n                    all_val_labels[i].extend([t.item() for t in target['labels']])\n                    all_val_scores[i].extend(pred_scores[i])\n\n        # Calcola l'accuratezza e mAP\n        val_accuracy = accuracy_score(\n            [label for labels in all_val_labels.values() for label in labels],\n            [pred for preds in all_val_preds.values() for pred in preds]\n        )\n        val_accuracies.append(val_accuracy)\n        print(f\"Accuracy (Validation): {val_accuracy:.4f}\")\n\n        # Salva il modello\n        if save_model:\n            torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n            print(f\"Modello salvato: model_epoch_{epoch + 1}.pth\")\n\n    return losses_per_epoch, train_accuracies, val_accuracies","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metrics(losses_per_epoch, train_accuracies, val_accuracies, num_epochs):\n    \"\"\"\n    Funzione per plottare le metriche di training e validazione.\n    \"\"\"\n    epochs_range = range(1, num_epochs + 1)\n\n    plt.figure(figsize=(12, 8))\n\n    # Loss plot\n    plt.subplot(2, 1, 1)\n    plt.plot(epochs_range, losses_per_epoch, label='Training Loss', color='blue')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training Loss per Epoca')\n\n    # Accuracy plot\n    plt.subplot(2, 1, 2)\n    plt.plot(epochs_range, train_accuracies, label='Accuracy (Training)', color='green')\n    plt.plot(epochs_range, val_accuracies, label='Accuracy (Validation)', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Accuracy per Epoca')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, test_loader, device):\n    \"\"\"\n    Funzione per il testing del modello Faster R-CNN.\n    \n    Args:\n    - model: il modello Faster R-CNN.\n    - test_loader: DataLoader per il test set.\n    - device: dispositivo su cui eseguire (es. 'cuda' o 'cpu').\n    \n    Returns:\n    - predictions: lista delle predizioni per ogni batch (include 'boxes', 'labels', 'scores').\n    \"\"\"\n    model.to(device)\n    model.eval()\n    predictions = []\n    \n    print(\"\\nInizio testing...\")\n    test_loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n    \n    with torch.no_grad():\n        for images, _ in test_loop:  # Durante il test, i target possono essere ignorati\n            images = [img.to(device) for img in images]\n            preds = model(images)\n            \n            # Predizioni di ciascun batch (contenente 'boxes', 'labels', 'scores')\n            # Le predizioni sono in un formato di lista di dizionari\n            for pred in preds:\n                predictions.append({\n                    'boxes': pred['boxes'].cpu().numpy(),\n                    'labels': pred['labels'].cpu().numpy(),\n                    'scores': pred['scores'].cpu().numpy()\n                })\n            \n            # Aggiungi aggiornamenti su quante predizioni sono state processate\n            test_loop.set_postfix(processed=len(predictions))\n\n    print(\"Testing completato.\")\n    return predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica il modello Faster R-CNN con ResNet50 e FPN\nmodel = fasterrcnn_resnet50_fpn(weights=None)\n\nnum_classes = 13\n\n# Modifica il numero di classi in output\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Imposta il dispositivo (GPU o CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Configurazione training\nnum_epochs = 2\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calcola i pesi delle classi\nclass_weights = compute_class_weights(train_loader.dataset)\n\nprint(class_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"losses, train_accs, val_accs = train_and_validate(model, train_loader, val_loader, optimizer, device, class_weights, num_epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(losses, train_accs, val_accs, num_epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chiamata alla funzione di test\npredictions = test_model(model=model, test_loader=test_loader, device=device)\n\n# Puoi fare qualcosa con le predizioni, come visualizzarle o calcolare metriche\nprint(f\"Numero di predizioni ottenute: {len(predictions)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}